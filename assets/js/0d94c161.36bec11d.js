"use strict";(globalThis.webpackChunkbook_website=globalThis.webpackChunkbook_website||[]).push([[4023],{769:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>a,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"module-4/vision-models","title":"Vision Models for Robotics","description":"Adapting vision foundation models for robotics: feature extraction, fine-tuning, and deployment.","source":"@site/docs/module-4/vision-models.md","sourceDirName":"module-4","slug":"/module-4/vision-models","permalink":"/physical-ai-robotics-book/docs/module-4/vision-models","draft":false,"unlisted":false,"editUrl":"https://github.com/fun33333/physical-ai-robotics-book/tree/main/book-website/docs/module-4/vision-models.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Vision Models for Robotics","sidebar_position":3,"description":"Adapting vision foundation models for robotics: feature extraction, fine-tuning, and deployment."},"sidebar":"tutorialSidebar","previous":{"title":"VLA Foundations","permalink":"/physical-ai-robotics-book/docs/module-4/vla-foundations"},"next":{"title":"Language Model Integration","permalink":"/physical-ai-robotics-book/docs/module-4/language-integration"}}');var r=i(4848),s=i(8453);const a={title:"Vision Models for Robotics",sidebar_position:3,description:"Adapting vision foundation models for robotics: feature extraction, fine-tuning, and deployment."},o="Vision Models for Robotics",l={},d=[{value:"Overview",id:"overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Vision Transformer (ViT)",id:"vision-transformer-vit",level:2},{value:"Patch-Based Image Processing",id:"patch-based-image-processing",level:3},{value:"ViT Implementation",id:"vit-implementation",level:3},{value:"CLIP: Vision-Language Alignment",id:"clip-vision-language-alignment",level:2},{value:"Contrastive Learning",id:"contrastive-learning",level:3},{value:"Using CLIP for Robotics",id:"using-clip-for-robotics",level:3},{value:"Feature Extraction for VLA",id:"feature-extraction-for-vla",level:2},{value:"Extracting Spatial Features",id:"extracting-spatial-features",level:3},{value:"Multi-Scale Features",id:"multi-scale-features",level:3},{value:"Efficient Fine-Tuning",id:"efficient-fine-tuning",level:2},{value:"LoRA: Low-Rank Adaptation",id:"lora-low-rank-adaptation",level:3},{value:"Adapter Layers",id:"adapter-layers",level:3},{value:"Real-Time Deployment",id:"real-time-deployment",level:2},{value:"Model Optimization",id:"model-optimization",level:3},{value:"Benchmark Results",id:"benchmark-results",level:3},{value:"Exercise 1: Implement ViT Feature Extractor",id:"exercise-1-implement-vit-feature-extractor",level:2},{value:"Exercise 2: Zero-Shot Object Detection",id:"exercise-2-zero-shot-object-detection",level:2},{value:"Exercise 3: Deploy Optimized Model",id:"exercise-3-deploy-optimized-model",level:2},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"vision-models-for-robotics",children:"Vision Models for Robotics"})}),"\n",(0,r.jsx)(n.p,{children:"Modern vision models trained on internet-scale data provide powerful feature representations that can be adapted for robotic perception. This chapter covers techniques for leveraging these models in robotics applications, from feature extraction to efficient deployment."}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"In this section, you will:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Understand Vision Transformer (ViT) architecture"}),"\n",(0,r.jsx)(n.li,{children:"Learn about CLIP and contrastive vision-language models"}),"\n",(0,r.jsx)(n.li,{children:"Implement feature extraction for robotics"}),"\n",(0,r.jsx)(n.li,{children:"Apply efficient fine-tuning techniques"}),"\n",(0,r.jsx)(n.li,{children:"Optimize models for real-time inference"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Understanding of convolutional neural networks"}),"\n",(0,r.jsx)(n.li,{children:"Basic knowledge of transformers"}),"\n",(0,r.jsx)(n.li,{children:"PyTorch experience"}),"\n",(0,r.jsx)(n.li,{children:"Familiarity with image processing concepts"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"vision-transformer-vit",children:"Vision Transformer (ViT)"}),"\n",(0,r.jsx)(n.h3,{id:"patch-based-image-processing",children:"Patch-Based Image Processing"}),"\n",(0,r.jsx)(n.p,{children:"ViT treats images as sequences of patches, enabling transformer processing:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Vision Transformer (ViT)                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                  \u2502\n\u2502   Input Image: 224 x 224 x 3                                    \u2502\n\u2502                                                                  \u2502\n\u2502   Step 1: Split into patches                                    \u2502\n\u2502   \u250c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2510                                             \u2502\n\u2502   \u2502 1 \u2502 2 \u2502 3 \u2502...\u2502  14x14 grid = 196 patches                  \u2502\n\u2502   \u251c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2524  Each patch: 16x16x3 = 768 values          \u2502\n\u2502   \u2502 15\u2502 16\u2502 17\u2502...\u2502                                             \u2502\n\u2502   \u251c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2524                                             \u2502\n\u2502   \u2502...\u2502...\u2502...\u2502...\u2502                                             \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2518                                             \u2502\n\u2502                                                                  \u2502\n\u2502   Step 2: Linear projection (patch embedding)                   \u2502\n\u2502   Each patch \u2192 768-dim vector                                   \u2502\n\u2502                                                                  \u2502\n\u2502   Step 3: Add position embeddings                               \u2502\n\u2502   [CLS] [patch_1 + pos_1] [patch_2 + pos_2] ... [patch_196]    \u2502\n\u2502                                                                  \u2502\n\u2502   Step 4: Transformer encoder                                   \u2502\n\u2502   12-24 layers of self-attention                                \u2502\n\u2502                                                                  \u2502\n\u2502   Output: [CLS] token \u2192 global image representation             \u2502\n\u2502           Patch tokens \u2192 spatial features                        \u2502\n\u2502                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsx)(n.h3,{id:"vit-implementation",children:"ViT Implementation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="vit_encoder.py"',children:'"""Vision Transformer encoder for robotics."""\nimport torch\nimport torch.nn as nn\nfrom einops import rearrange\n\nclass PatchEmbedding(nn.Module):\n    """Convert image to patch embeddings."""\n\n    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = (img_size // patch_size) ** 2\n\n        # Linear projection of flattened patches\n        self.proj = nn.Conv2d(\n            in_channels, embed_dim,\n            kernel_size=patch_size, stride=patch_size\n        )\n\n    def forward(self, x):\n        # x: [batch, channels, height, width]\n        x = self.proj(x)  # [batch, embed_dim, h/patch, w/patch]\n        x = rearrange(x, \'b e h w -> b (h w) e\')  # [batch, num_patches, embed_dim]\n        return x\n\n\nclass ViTEncoder(nn.Module):\n    """Vision Transformer for feature extraction."""\n\n    def __init__(\n        self,\n        img_size=224,\n        patch_size=16,\n        in_channels=3,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4.0\n    ):\n        super().__init__()\n\n        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n        num_patches = self.patch_embed.num_patches\n\n        # Learnable CLS token and position embeddings\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n\n        # Transformer encoder layers\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim,\n            nhead=num_heads,\n            dim_feedforward=int(embed_dim * mlp_ratio),\n            batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n\n        self.norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, x):\n        batch_size = x.size(0)\n\n        # Patch embedding\n        x = self.patch_embed(x)  # [batch, num_patches, embed_dim]\n\n        # Add CLS token\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n        x = torch.cat([cls_tokens, x], dim=1)  # [batch, num_patches+1, embed_dim]\n\n        # Add position embeddings\n        x = x + self.pos_embed\n\n        # Transformer encoding\n        x = self.encoder(x)\n        x = self.norm(x)\n\n        return x  # [batch, num_patches+1, embed_dim]\n\n    def get_cls_token(self, x):\n        """Get global image representation."""\n        return self.forward(x)[:, 0]  # [batch, embed_dim]\n\n    def get_patch_features(self, x):\n        """Get spatial patch features."""\n        return self.forward(x)[:, 1:]  # [batch, num_patches, embed_dim]\n\n\n# Example usage\nmodel = ViTEncoder()\nimage = torch.randn(4, 3, 224, 224)\n\nfeatures = model(image)\nprint(f"Full output: {features.shape}")  # [4, 197, 768]\n\ncls_feature = model.get_cls_token(image)\nprint(f"CLS token: {cls_feature.shape}")  # [4, 768]\n\npatch_features = model.get_patch_features(image)\nprint(f"Patch features: {patch_features.shape}")  # [4, 196, 768]\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"clip-vision-language-alignment",children:"CLIP: Vision-Language Alignment"}),"\n",(0,r.jsx)(n.h3,{id:"contrastive-learning",children:"Contrastive Learning"}),"\n",(0,r.jsx)(n.p,{children:"CLIP learns aligned vision and language representations:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    CLIP Architecture                             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                  \u2502\n\u2502   Image                          Text                            \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502   \u2502 [IMG]   \u2502                   \u2502 "A robot arm"       \u2502         \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502        \u2502                                   \u2502                     \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502   \u2502  Image  \u2502                   \u2502       Text          \u2502         \u2502\n\u2502   \u2502 Encoder \u2502                   \u2502      Encoder        \u2502         \u2502\n\u2502   \u2502  (ViT)  \u2502                   \u2502   (Transformer)     \u2502         \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502        \u2502                                   \u2502                     \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502   \u2502  Image  \u2502                   \u2502       Text          \u2502         \u2502\n\u2502   \u2502Embedding\u2502                   \u2502     Embedding       \u2502         \u2502\n\u2502   \u2502 [512]   \u2502                   \u2502       [512]         \u2502         \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502        \u2502                                   \u2502                     \u2502\n\u2502        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2502\n\u2502                        \u2502                                         \u2502\n\u2502              Contrastive Loss                                    \u2502\n\u2502        (matching pairs should be similar)                        \u2502\n\u2502                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n'})}),"\n",(0,r.jsx)(n.h3,{id:"using-clip-for-robotics",children:"Using CLIP for Robotics"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="clip_robotics.py"',children:'"""Using CLIP for robotic perception."""\nimport torch\nfrom transformers import CLIPProcessor, CLIPModel\nfrom PIL import Image\n\nclass CLIPRobotPerception:\n    """CLIP-based object understanding for robotics."""\n\n    def __init__(self, model_name="openai/clip-vit-base-patch32"):\n        self.model = CLIPModel.from_pretrained(model_name)\n        self.processor = CLIPProcessor.from_pretrained(model_name)\n        self.model.eval()\n\n    @torch.no_grad()\n    def get_image_features(self, image):\n        """Extract visual features from image."""\n        inputs = self.processor(images=image, return_tensors="pt")\n        features = self.model.get_image_features(**inputs)\n        return features / features.norm(dim=-1, keepdim=True)\n\n    @torch.no_grad()\n    def get_text_features(self, texts):\n        """Extract text features."""\n        inputs = self.processor(text=texts, return_tensors="pt", padding=True)\n        features = self.model.get_text_features(**inputs)\n        return features / features.norm(dim=-1, keepdim=True)\n\n    @torch.no_grad()\n    def classify_object(self, image, candidate_labels):\n        """Zero-shot object classification."""\n        # Get image and text features\n        image_features = self.get_image_features(image)\n        text_features = self.get_text_features(candidate_labels)\n\n        # Compute similarity\n        similarity = (image_features @ text_features.T).softmax(dim=-1)\n\n        # Return label with highest similarity\n        best_idx = similarity.argmax().item()\n        return candidate_labels[best_idx], similarity[0, best_idx].item()\n\n    @torch.no_grad()\n    def find_object(self, image, object_description):\n        """Find if described object is in image."""\n        image_features = self.get_image_features(image)\n        text_features = self.get_text_features([object_description])\n\n        similarity = (image_features @ text_features.T).item()\n        return similarity > 0.25  # Threshold for presence\n\n\n# Example usage\nperception = CLIPRobotPerception()\n\n# Load image (from camera or file)\nimage = Image.open("scene.jpg")\n\n# Zero-shot classification\nlabels = ["red cup", "blue bottle", "green box", "white plate"]\ndetected, confidence = perception.classify_object(image, labels)\nprint(f"Detected: {detected} (confidence: {confidence:.2f})")\n\n# Object presence check\nhas_cup = perception.find_object(image, "a red cup on the table")\nprint(f"Cup present: {has_cup}")\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"feature-extraction-for-vla",children:"Feature Extraction for VLA"}),"\n",(0,r.jsx)(n.h3,{id:"extracting-spatial-features",children:"Extracting Spatial Features"}),"\n",(0,r.jsx)(n.p,{children:"VLA models need spatial features, not just global representations:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="spatial_features.py"',children:'"""Extract spatial features for VLA models."""\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms\nfrom transformers import ViTModel, ViTImageProcessor\n\nclass SpatialFeatureExtractor(nn.Module):\n    """Extract spatial features from images for VLA."""\n\n    def __init__(self, model_name="google/vit-base-patch16-224", freeze=True):\n        super().__init__()\n\n        self.processor = ViTImageProcessor.from_pretrained(model_name)\n        self.model = ViTModel.from_pretrained(model_name)\n\n        if freeze:\n            for param in self.model.parameters():\n                param.requires_grad = False\n\n        self.hidden_size = self.model.config.hidden_size\n        self.num_patches = (self.model.config.image_size // self.model.config.patch_size) ** 2\n\n    def forward(self, images):\n        """\n        Extract features from images.\n\n        Args:\n            images: PIL images or tensor [batch, 3, H, W]\n\n        Returns:\n            features: [batch, num_patches, hidden_size]\n        """\n        # Process images\n        if not isinstance(images, torch.Tensor):\n            inputs = self.processor(images, return_tensors="pt")\n            pixel_values = inputs.pixel_values\n        else:\n            pixel_values = images\n\n        # Extract features\n        outputs = self.model(pixel_values, output_hidden_states=True)\n\n        # Get patch tokens (exclude CLS token)\n        patch_features = outputs.last_hidden_state[:, 1:]\n\n        return patch_features\n\n    def get_feature_map(self, images):\n        """Get 2D feature map."""\n        features = self.forward(images)\n        batch_size = features.size(0)\n        h = w = int(self.num_patches ** 0.5)\n\n        # Reshape to spatial grid\n        feature_map = features.view(batch_size, h, w, self.hidden_size)\n        feature_map = feature_map.permute(0, 3, 1, 2)  # [batch, hidden, h, w]\n\n        return feature_map\n\n\n# Example for VLA input\nextractor = SpatialFeatureExtractor()\n\n# Process robot camera image\ncamera_image = torch.randn(1, 3, 224, 224)  # Simulated camera input\nfeatures = extractor(camera_image)\n\nprint(f"Spatial features: {features.shape}")  # [1, 196, 768]\n# These 196 tokens (14x14 grid) feed into VLA transformer\n'})}),"\n",(0,r.jsx)(n.h3,{id:"multi-scale-features",children:"Multi-Scale Features"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="multiscale_features.py"',children:'"""Multi-scale feature extraction for manipulation."""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MultiScaleEncoder(nn.Module):\n    """Extract features at multiple scales."""\n\n    def __init__(self, base_encoder, scales=[1.0, 0.5, 0.25]):\n        super().__init__()\n        self.encoder = base_encoder\n        self.scales = scales\n\n    def forward(self, x):\n        """Extract multi-scale features."""\n        features = []\n        original_size = x.shape[-2:]\n\n        for scale in self.scales:\n            if scale != 1.0:\n                scaled_size = (int(original_size[0] * scale), int(original_size[1] * scale))\n                scaled_x = F.interpolate(x, size=scaled_size, mode=\'bilinear\', align_corners=False)\n            else:\n                scaled_x = x\n\n            feat = self.encoder(scaled_x)\n            features.append(feat)\n\n        return features\n\n\n# Usage for detailed manipulation understanding\nbase_encoder = SpatialFeatureExtractor()\nmulti_scale = MultiScaleEncoder(base_encoder, scales=[1.0, 0.5])\n\nimage = torch.randn(1, 3, 224, 224)\nscale_features = multi_scale(image)\n\nfor i, feat in enumerate(scale_features):\n    print(f"Scale {multi_scale.scales[i]}: {feat.shape}")\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"efficient-fine-tuning",children:"Efficient Fine-Tuning"}),"\n",(0,r.jsx)(n.h3,{id:"lora-low-rank-adaptation",children:"LoRA: Low-Rank Adaptation"}),"\n",(0,r.jsx)(n.p,{children:"Fine-tune vision models efficiently without full retraining:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="lora_finetuning.py"',children:'"""LoRA fine-tuning for vision models."""\nimport torch\nimport torch.nn as nn\nfrom peft import LoraConfig, get_peft_model\nfrom transformers import ViTForImageClassification\n\nclass LoRAVisionModel:\n    """Fine-tune ViT with LoRA for robotics tasks."""\n\n    def __init__(self, base_model_name="google/vit-base-patch16-224"):\n        # Load base model\n        self.model = ViTForImageClassification.from_pretrained(\n            base_model_name,\n            num_labels=10,  # Customize for your task\n            ignore_mismatched_sizes=True\n        )\n\n        # Configure LoRA\n        lora_config = LoraConfig(\n            r=16,  # Rank of adaptation matrices\n            lora_alpha=32,  # Scaling factor\n            target_modules=["query", "value"],  # Attention layers to adapt\n            lora_dropout=0.1,\n            bias="none"\n        )\n\n        # Apply LoRA\n        self.model = get_peft_model(self.model, lora_config)\n\n        # Print trainable parameters\n        trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n        total = sum(p.numel() for p in self.model.parameters())\n        print(f"Trainable: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)")\n\n    def train_step(self, images, labels, optimizer):\n        """Single training step."""\n        self.model.train()\n        optimizer.zero_grad()\n\n        outputs = self.model(images, labels=labels)\n        loss = outputs.loss\n\n        loss.backward()\n        optimizer.step()\n\n        return loss.item()\n\n\n# Example: Fine-tune for robot object classification\nlora_model = LoRAVisionModel()\n# Trainable: ~300K / ~86M (0.35%) - 300x fewer parameters!\n'})}),"\n",(0,r.jsx)(n.h3,{id:"adapter-layers",children:"Adapter Layers"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="adapter_layers.py"',children:'"""Adapter layers for efficient transfer learning."""\nimport torch\nimport torch.nn as nn\n\nclass Adapter(nn.Module):\n    """Bottleneck adapter for efficient fine-tuning."""\n\n    def __init__(self, hidden_size, bottleneck_size=64):\n        super().__init__()\n        self.down_proj = nn.Linear(hidden_size, bottleneck_size)\n        self.up_proj = nn.Linear(bottleneck_size, hidden_size)\n        self.act = nn.GELU()\n\n    def forward(self, x):\n        residual = x\n        x = self.down_proj(x)\n        x = self.act(x)\n        x = self.up_proj(x)\n        return x + residual\n\n\nclass AdaptedViT(nn.Module):\n    """ViT with adapter layers for robotics."""\n\n    def __init__(self, base_model, bottleneck_size=64):\n        super().__init__()\n        self.base = base_model\n\n        # Freeze base model\n        for param in self.base.parameters():\n            param.requires_grad = False\n\n        # Add adapters after each transformer block\n        hidden_size = self.base.config.hidden_size\n        self.adapters = nn.ModuleList([\n            Adapter(hidden_size, bottleneck_size)\n            for _ in range(self.base.config.num_hidden_layers)\n        ])\n\n    def forward(self, x):\n        # Embed patches\n        x = self.base.embeddings(x)\n\n        # Pass through transformer with adapters\n        for i, layer in enumerate(self.base.encoder.layer):\n            x = layer(x)[0]\n            x = self.adapters[i](x)\n\n        return x\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"real-time-deployment",children:"Real-Time Deployment"}),"\n",(0,r.jsx)(n.h3,{id:"model-optimization",children:"Model Optimization"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="optimize_inference.py"',children:'"""Optimize vision models for real-time robotics."""\nimport torch\nfrom torch.quantization import quantize_dynamic\nimport onnx\nimport onnxruntime as ort\n\ndef optimize_for_inference(model, example_input):\n    """Optimize model for deployment."""\n\n    # 1. TorchScript tracing\n    model.eval()\n    traced_model = torch.jit.trace(model, example_input)\n\n    # 2. Dynamic quantization (INT8)\n    quantized_model = quantize_dynamic(\n        model,\n        {torch.nn.Linear},\n        dtype=torch.qint8\n    )\n\n    return traced_model, quantized_model\n\n\ndef export_to_onnx(model, example_input, output_path):\n    """Export to ONNX for cross-platform deployment."""\n    model.eval()\n\n    torch.onnx.export(\n        model,\n        example_input,\n        output_path,\n        export_params=True,\n        opset_version=14,\n        input_names=[\'image\'],\n        output_names=[\'features\'],\n        dynamic_axes={\n            \'image\': {0: \'batch\'},\n            \'features\': {0: \'batch\'}\n        }\n    )\n\n    # Verify ONNX model\n    onnx_model = onnx.load(output_path)\n    onnx.checker.check_model(onnx_model)\n\n    return output_path\n\n\nclass ONNXInference:\n    """ONNX runtime inference for production."""\n\n    def __init__(self, model_path):\n        # Use GPU if available\n        providers = [\'CUDAExecutionProvider\', \'CPUExecutionProvider\']\n        self.session = ort.InferenceSession(model_path, providers=providers)\n        self.input_name = self.session.get_inputs()[0].name\n\n    def __call__(self, image):\n        """Run inference."""\n        if isinstance(image, torch.Tensor):\n            image = image.numpy()\n\n        outputs = self.session.run(None, {self.input_name: image})\n        return outputs[0]\n\n\n# Example deployment pipeline\nmodel = SpatialFeatureExtractor()\nexample = torch.randn(1, 3, 224, 224)\n\n# Export\nexport_to_onnx(model, example, "vision_encoder.onnx")\n\n# Deploy\ninference = ONNXInference("vision_encoder.onnx")\nfeatures = inference(example)\nprint(f"Inference output: {features.shape}")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"benchmark-results",children:"Benchmark Results"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Optimization"}),(0,r.jsx)(n.th,{children:"Latency (ms)"}),(0,r.jsx)(n.th,{children:"Memory (MB)"}),(0,r.jsx)(n.th,{children:"Accuracy"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"PyTorch FP32"}),(0,r.jsx)(n.td,{children:"15.2"}),(0,r.jsx)(n.td,{children:"350"}),(0,r.jsx)(n.td,{children:"100%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"TorchScript"}),(0,r.jsx)(n.td,{children:"12.8"}),(0,r.jsx)(n.td,{children:"350"}),(0,r.jsx)(n.td,{children:"100%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"INT8 Quantized"}),(0,r.jsx)(n.td,{children:"8.4"}),(0,r.jsx)(n.td,{children:"95"}),(0,r.jsx)(n.td,{children:"99.5%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"ONNX + TensorRT"}),(0,r.jsx)(n.td,{children:"4.2"}),(0,r.jsx)(n.td,{children:"280"}),(0,r.jsx)(n.td,{children:"100%"})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"exercise-1-implement-vit-feature-extractor",children:"Exercise 1: Implement ViT Feature Extractor"}),"\n",(0,r.jsxs)(n.admonition,{title:"Exercise 1: Build Feature Extractor",type:"tip",children:[(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Objective"}),": Create a ViT-based feature extractor for robot perception."]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Steps"}),":"]}),(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Implement patch embedding from scratch"}),"\n",(0,r.jsx)(n.li,{children:"Add position encodings"}),"\n",(0,r.jsx)(n.li,{children:"Build transformer encoder"}),"\n",(0,r.jsx)(n.li,{children:"Extract both CLS and patch features"}),"\n",(0,r.jsx)(n.li,{children:"Test on robot camera images"}),"\n"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Verification"}),":"]}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Your extractor should pass:\nassert features.shape == (batch_size, 196, 768)\nassert cls_token.shape == (batch_size, 768)\n"})}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Time Estimate"}),": 60 minutes"]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"exercise-2-zero-shot-object-detection",children:"Exercise 2: Zero-Shot Object Detection"}),"\n",(0,r.jsxs)(n.admonition,{title:"Exercise 2: CLIP for Robot Tasks",type:"tip",children:[(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Objective"}),": Use CLIP for zero-shot object understanding."]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Steps"}),":"]}),(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Load pre-trained CLIP model"}),"\n",(0,r.jsx)(n.li,{children:"Create object vocabulary for robot workspace"}),"\n",(0,r.jsx)(n.li,{children:"Implement object presence detection"}),"\n",(0,r.jsx)(n.li,{children:"Add object localization using attention"}),"\n",(0,r.jsx)(n.li,{children:"Test on tabletop manipulation scene"}),"\n"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected Output"}),":"]}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Identify objects in scene without training"}),"\n",(0,r.jsx)(n.li,{children:"Localize objects using attention maps"}),"\n"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Time Estimate"}),": 45 minutes"]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"exercise-3-deploy-optimized-model",children:"Exercise 3: Deploy Optimized Model"}),"\n",(0,r.jsxs)(n.admonition,{title:"Exercise 3: Real-Time Inference",type:"tip",children:[(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Objective"}),": Optimize vision model for robot deployment."]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Steps"}),":"]}),(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Export model to ONNX"}),"\n",(0,r.jsx)(n.li,{children:"Apply INT8 quantization"}),"\n",(0,r.jsx)(n.li,{children:"Benchmark inference speed"}),"\n",(0,r.jsx)(n.li,{children:"Test accuracy on held-out data"}),"\n",(0,r.jsx)(n.li,{children:"Compare latency across optimizations"}),"\n"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Target"}),": < 10ms inference on RTX 3060"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Time Estimate"}),": 45 minutes"]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"In this chapter, you learned:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Vision Transformers"}),": Patch-based processing for image understanding"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"CLIP"}),": Aligned vision-language representations for zero-shot recognition"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Feature Extraction"}),": Spatial features for VLA models"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fine-Tuning"}),": LoRA and adapters for efficient adaptation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Deployment"}),": Optimization techniques for real-time inference"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Vision encoders are the eyes of VLA systems\u2014they transform raw pixels into semantic representations that enable language grounding and action generation."}),"\n",(0,r.jsxs)(n.p,{children:["Next, explore ",(0,r.jsx)(n.a,{href:"/docs/module-4/language-integration",children:"Language Model Integration"})," to understand how language models connect to robot control."]}),"\n",(0,r.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2010.11929",children:"ViT Paper"})," - An Image is Worth 16x16 Words"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2103.00020",children:"CLIP Paper"})," - Learning Transferable Visual Models"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2304.07193",children:"DINOv2"})," - Self-Supervised Vision Features"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2303.15343",children:"SigLIP"})," - Sigmoid Loss for Vision-Language"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2106.09685",children:"LoRA Paper"})," - Low-Rank Adaptation"]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var t=i(6540);const r={},s=t.createContext(r);function a(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);