"use strict";(globalThis.webpackChunkbook_website=globalThis.webpackChunkbook_website||[]).push([[7110],{7639:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"module-3/isaac-lab","title":"Isaac Lab (Reinforcement Learning)","description":"Train robot policies with Isaac Lab: massively parallel simulation for reinforcement learning.","source":"@site/docs/module-3/isaac-lab.md","sourceDirName":"module-3","slug":"/module-3/isaac-lab","permalink":"/physical-ai-robotics-book/docs/module-3/isaac-lab","draft":false,"unlisted":false,"editUrl":"https://github.com/fun33333/physical-ai-robotics-book/tree/main/book-website/docs/module-3/isaac-lab.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"Isaac Lab (Reinforcement Learning)","sidebar_position":5,"description":"Train robot policies with Isaac Lab: massively parallel simulation for reinforcement learning."},"sidebar":"tutorialSidebar","previous":{"title":"Isaac ROS","permalink":"/physical-ai-robotics-book/docs/module-3/isaac-ros"},"next":{"title":"Perception Pipelines","permalink":"/physical-ai-robotics-book/docs/module-3/perception-pipelines"}}');var a=i(4848),s=i(8453);const t={title:"Isaac Lab (Reinforcement Learning)",sidebar_position:5,description:"Train robot policies with Isaac Lab: massively parallel simulation for reinforcement learning."},o="Isaac Lab (Reinforcement Learning)",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Reinforcement Learning Fundamentals",id:"reinforcement-learning-fundamentals",level:2},{value:"The RL Framework",id:"the-rl-framework",level:3},{value:"Key Concepts",id:"key-concepts",level:3},{value:"Why GPU-Parallel RL?",id:"why-gpu-parallel-rl",level:3},{value:"Installation",id:"installation",level:2},{value:"Setup Isaac Lab",id:"setup-isaac-lab",level:3},{value:"Project Structure",id:"project-structure",level:3},{value:"Training Your First Policy",id:"training-your-first-policy",level:2},{value:"Available Environments",id:"available-environments",level:3},{value:"Training with RL Games",id:"training-with-rl-games",level:3},{value:"Training with RSL-RL",id:"training-with-rsl-rl",level:3},{value:"Visualizing Trained Policies",id:"visualizing-trained-policies",level:3},{value:"Understanding Environments",id:"understanding-environments",level:2},{value:"Environment Structure",id:"environment-structure",level:3},{value:"Configuration System",id:"configuration-system",level:3},{value:"Reward Engineering",id:"reward-engineering",level:2},{value:"Reward Function Design",id:"reward-function-design",level:3},{value:"Reward Configuration",id:"reward-configuration",level:3},{value:"Common Reward Patterns",id:"common-reward-patterns",level:3},{value:"Custom Environments",id:"custom-environments",level:2},{value:"Creating a Reach Task",id:"creating-a-reach-task",level:3},{value:"Registering Custom Environment",id:"registering-custom-environment",level:3},{value:"Domain Randomization",id:"domain-randomization",level:2},{value:"Randomization for Sim-to-Real",id:"randomization-for-sim-to-real",level:3},{value:"Curriculum Learning",id:"curriculum-learning",level:3},{value:"Policy Deployment",id:"policy-deployment",level:2},{value:"Export Policy for Deployment",id:"export-policy-for-deployment",level:3},{value:"ROS 2 Deployment Node",id:"ros-2-deployment-node",level:3},{value:"Exercise 1: Train Cartpole",id:"exercise-1-train-cartpole",level:2},{value:"Exercise 2: Quadruped Locomotion",id:"exercise-2-quadruped-locomotion",level:2},{value:"Exercise 3: Custom Environment",id:"exercise-3-custom-environment",level:2},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(n){const e={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"isaac-lab-reinforcement-learning",children:"Isaac Lab (Reinforcement Learning)"})}),"\n",(0,a.jsx)(e.p,{children:"Isaac Lab (formerly known as Isaac Orbit) enables training robot policies through reinforcement learning at unprecedented scale. By running thousands of parallel simulations on GPU, you can train policies in hours that would take weeks with traditional approaches. This chapter shows you how to leverage Isaac Lab for learning-based robot control."}),"\n",(0,a.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(e.p,{children:"In this section, you will:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Understand reinforcement learning fundamentals for robotics"}),"\n",(0,a.jsx)(e.li,{children:"Install and configure Isaac Lab"}),"\n",(0,a.jsx)(e.li,{children:"Train policies using existing environments"}),"\n",(0,a.jsx)(e.li,{children:"Design reward functions for desired behaviors"}),"\n",(0,a.jsx)(e.li,{children:"Create custom training environments"}),"\n",(0,a.jsx)(e.li,{children:"Implement domain randomization for sim-to-real transfer"}),"\n",(0,a.jsx)(e.li,{children:"Deploy trained policies to real robots"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["Completed ",(0,a.jsx)(e.a,{href:"/docs/module-3/isaac-sim",children:"Isaac Sim"})," chapter"]}),"\n",(0,a.jsx)(e.li,{children:"NVIDIA GPU with 12GB+ VRAM (recommended for training)"}),"\n",(0,a.jsx)(e.li,{children:"Basic understanding of neural networks"}),"\n",(0,a.jsx)(e.li,{children:"Familiarity with PyTorch"}),"\n",(0,a.jsx)(e.li,{children:"Python proficiency"}),"\n"]}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"reinforcement-learning-fundamentals",children:"Reinforcement Learning Fundamentals"}),"\n",(0,a.jsx)(e.h3,{id:"the-rl-framework",children:"The RL Framework"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Reinforcement Learning Loop                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                  \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  action a\u209c   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502   \u2502         \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502                                 \u2502  \u2502\n\u2502   \u2502  Agent  \u2502              \u2502        Environment              \u2502  \u2502\n\u2502   \u2502 (Policy)\u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502   (Isaac Lab Simulation)        \u2502  \u2502\n\u2502   \u2502         \u2502  state s\u209c\u208a\u2081  \u2502                                 \u2502  \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  reward r\u209c   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                                  \u2502\n\u2502   Goal: Learn policy \u03c0(s) \u2192 a that maximizes \u03a3 \u03b3\u1d57 r\u209c           \u2502\n\u2502                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,a.jsx)(e.h3,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,a.jsxs)(e.table,{children:[(0,a.jsx)(e.thead,{children:(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.th,{children:"Concept"}),(0,a.jsx)(e.th,{children:"Description"}),(0,a.jsx)(e.th,{children:"Example in Robotics"})]})}),(0,a.jsxs)(e.tbody,{children:[(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:(0,a.jsx)(e.strong,{children:"State"})}),(0,a.jsx)(e.td,{children:"Observation of environment"}),(0,a.jsx)(e.td,{children:"Joint positions, velocities, sensor data"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:(0,a.jsx)(e.strong,{children:"Action"})}),(0,a.jsx)(e.td,{children:"Control output"}),(0,a.jsx)(e.td,{children:"Joint torques, velocity commands"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:(0,a.jsx)(e.strong,{children:"Reward"})}),(0,a.jsx)(e.td,{children:"Scalar feedback signal"}),(0,a.jsx)(e.td,{children:"Distance to goal, energy consumption"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:(0,a.jsx)(e.strong,{children:"Policy"})}),(0,a.jsx)(e.td,{children:"Mapping from state to action"}),(0,a.jsx)(e.td,{children:"Neural network controller"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:(0,a.jsx)(e.strong,{children:"Episode"})}),(0,a.jsx)(e.td,{children:"Single trial from start to termination"}),(0,a.jsx)(e.td,{children:"Robot attempt at task"})]})]})]}),"\n",(0,a.jsx)(e.h3,{id:"why-gpu-parallel-rl",children:"Why GPU-Parallel RL?"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Training Time Comparison                            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                  \u2502\n\u2502   Task: Quadruped locomotion (10M timesteps)                    \u2502\n\u2502                                                                  \u2502\n\u2502   Single Environment (CPU):                                      \u2502\n\u2502   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  ~10 hours           \u2502\n\u2502                                                                  \u2502\n\u2502   256 Parallel Envs (Isaac Lab):                                \u2502\n\u2502   \u2588\u2588                                        ~5 minutes          \u2502\n\u2502                                                                  \u2502\n\u2502   4096 Parallel Envs (Isaac Lab):                               \u2502\n\u2502   \u2588                                         ~30 seconds         \u2502\n\u2502                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"installation",children:"Installation"}),"\n",(0,a.jsx)(e.h3,{id:"setup-isaac-lab",children:"Setup Isaac Lab"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",metastring:'title="Install Isaac Lab"',children:"# 1. Ensure Isaac Sim is installed via Omniverse Launcher\n\n# 2. Clone Isaac Lab\ncd ~\ngit clone https://github.com/isaac-sim/IsaacLab.git\ncd IsaacLab\n\n# 3. Create conda environment\nconda create -n isaaclab python=3.10 -y\nconda activate isaaclab\n\n# 4. Install Isaac Lab\n./isaaclab.sh --install\n\n# 5. Verify installation\npython -c \"import omni.isaac.lab; print('Isaac Lab ready!')\"\n\n# 6. Run sample environment\npython source/standalone/tutorials/00_sim/create_empty.py\n"})}),"\n",(0,a.jsx)(e.h3,{id:"project-structure",children:"Project Structure"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:"IsaacLab/\n\u251c\u2500\u2500 source/\n\u2502   \u251c\u2500\u2500 extensions/\n\u2502   \u2502   \u251c\u2500\u2500 omni.isaac.lab/          # Core library\n\u2502   \u2502   \u251c\u2500\u2500 omni.isaac.lab_assets/   # Robot/object assets\n\u2502   \u2502   \u2514\u2500\u2500 omni.isaac.lab_tasks/    # Pre-built environments\n\u2502   \u2514\u2500\u2500 standalone/\n\u2502       \u251c\u2500\u2500 workflows/               # Training scripts\n\u2502       \u2502   \u251c\u2500\u2500 rl_games/\n\u2502       \u2502   \u251c\u2500\u2500 rsl_rl/\n\u2502       \u2502   \u2514\u2500\u2500 skrl/\n\u2502       \u2514\u2500\u2500 tutorials/               # Learning examples\n\u251c\u2500\u2500 docs/                            # Documentation\n\u2514\u2500\u2500 isaaclab.sh                      # Management script\n"})}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"training-your-first-policy",children:"Training Your First Policy"}),"\n",(0,a.jsx)(e.h3,{id:"available-environments",children:"Available Environments"}),"\n",(0,a.jsx)(e.p,{children:"Isaac Lab includes ready-to-use environments:"}),"\n",(0,a.jsxs)(e.table,{children:[(0,a.jsx)(e.thead,{children:(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.th,{children:"Environment"}),(0,a.jsx)(e.th,{children:"Task"}),(0,a.jsx)(e.th,{children:"Observation Dim"}),(0,a.jsx)(e.th,{children:"Action Dim"})]})}),(0,a.jsxs)(e.tbody,{children:[(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:(0,a.jsx)(e.code,{children:"Isaac-Cartpole-v0"})}),(0,a.jsx)(e.td,{children:"Balance pole"}),(0,a.jsx)(e.td,{children:"4"}),(0,a.jsx)(e.td,{children:"1"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:(0,a.jsx)(e.code,{children:"Isaac-Ant-v0"})}),(0,a.jsx)(e.td,{children:"Quadruped locomotion"}),(0,a.jsx)(e.td,{children:"60"}),(0,a.jsx)(e.td,{children:"8"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:(0,a.jsx)(e.code,{children:"Isaac-Humanoid-v0"})}),(0,a.jsx)(e.td,{children:"Bipedal walking"}),(0,a.jsx)(e.td,{children:"87"}),(0,a.jsx)(e.td,{children:"21"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:(0,a.jsx)(e.code,{children:"Isaac-Franka-Reach-v0"})}),(0,a.jsx)(e.td,{children:"End-effector positioning"}),(0,a.jsx)(e.td,{children:"18"}),(0,a.jsx)(e.td,{children:"7"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:(0,a.jsx)(e.code,{children:"Isaac-Franka-Lift-v0"})}),(0,a.jsx)(e.td,{children:"Object manipulation"}),(0,a.jsx)(e.td,{children:"23"}),(0,a.jsx)(e.td,{children:"7"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:(0,a.jsx)(e.code,{children:"Isaac-Anymal-C-Flat-v0"})}),(0,a.jsx)(e.td,{children:"Legged robot walking"}),(0,a.jsx)(e.td,{children:"48"}),(0,a.jsx)(e.td,{children:"12"})]})]})]}),"\n",(0,a.jsx)(e.h3,{id:"training-with-rl-games",children:"Training with RL Games"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",metastring:'title="Train Cartpole"',children:"# Navigate to Isaac Lab\ncd ~/IsaacLab\n\n# Train with default config\npython source/standalone/workflows/rl_games/train.py \\\n  --task Isaac-Cartpole-v0 \\\n  --num_envs 1024 \\\n  --max_iterations 500\n\n# Training output:\n# [INFO] Policy saved to: logs/rl_games/cartpole/...\n# [INFO] Final reward: 195.2 (solved!)\n"})}),"\n",(0,a.jsx)(e.h3,{id:"training-with-rsl-rl",children:"Training with RSL-RL"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",metastring:'title="Train quadruped locomotion"',children:"# Train Anymal walking\npython source/standalone/workflows/rsl_rl/train.py \\\n  --task Isaac-Anymal-C-Flat-v0 \\\n  --num_envs 4096 \\\n  --max_iterations 1000\n\n# Monitor with TensorBoard\ntensorboard --logdir logs/rsl_rl/anymal_c_flat\n"})}),"\n",(0,a.jsx)(e.h3,{id:"visualizing-trained-policies",children:"Visualizing Trained Policies"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",metastring:'title="Play trained policy"',children:"# Run trained policy with visualization\npython source/standalone/workflows/rl_games/play.py \\\n  --task Isaac-Cartpole-v0 \\\n  --checkpoint logs/rl_games/cartpole/nn/cartpole.pth \\\n  --num_envs 32\n"})}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"understanding-environments",children:"Understanding Environments"}),"\n",(0,a.jsx)(e.h3,{id:"environment-structure",children:"Environment Structure"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",metastring:'title="Environment anatomy"',children:'"""Isaac Lab environment components."""\nfrom omni.isaac.lab.envs import ManagerBasedRLEnv\n\nclass MyEnv(ManagerBasedRLEnv):\n    """Custom environment structure."""\n\n    # Key components:\n    # 1. Scene - robots, objects, sensors\n    # 2. Observations - what agent sees\n    # 3. Actions - how agent controls\n    # 4. Rewards - feedback signal\n    # 5. Terminations - when to reset\n    # 6. Curriculum - progressive difficulty\n'})}),"\n",(0,a.jsx)(e.h3,{id:"configuration-system",children:"Configuration System"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",metastring:'title="env_config.py"',children:'"""Environment configuration example."""\nfrom omni.isaac.lab.envs import ManagerBasedRLEnvCfg\nfrom omni.isaac.lab.scene import InteractiveSceneCfg\nfrom omni.isaac.lab.managers import (\n    ObservationGroupCfg,\n    RewardTermCfg,\n    TerminationTermCfg,\n)\n\n@configclass\nclass CartpoleEnvCfg(ManagerBasedRLEnvCfg):\n    """Cartpole environment configuration."""\n\n    # Scene configuration\n    scene: InteractiveSceneCfg = InteractiveSceneCfg(\n        num_envs=1024,\n        env_spacing=4.0,\n    )\n\n    # Observation configuration\n    observations: ObservationsCfg = ObservationsCfg()\n\n    # Action configuration\n    actions: ActionsCfg = ActionsCfg()\n\n    # Reward configuration\n    rewards: RewardsCfg = RewardsCfg()\n\n    # Termination configuration\n    terminations: TerminationsCfg = TerminationsCfg()\n\n    # Episode length\n    episode_length_s: float = 5.0\n'})}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"reward-engineering",children:"Reward Engineering"}),"\n",(0,a.jsx)(e.h3,{id:"reward-function-design",children:"Reward Function Design"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",metastring:'title="reward_functions.py"',children:'"""Reward function examples for robot learning."""\nfrom omni.isaac.lab.managers import RewardTermCfg\nimport omni.isaac.lab.utils.math as math_utils\n\ndef reward_alive(env) -> torch.Tensor:\n    """Reward for staying alive (not falling)."""\n    return torch.ones(env.num_envs, device=env.device)\n\ndef reward_forward_velocity(env, target_vel: float = 1.0) -> torch.Tensor:\n    """Reward for moving forward at target velocity."""\n    # Get base linear velocity\n    base_vel = env.scene["robot"].data.root_lin_vel_b\n    forward_vel = base_vel[:, 0]  # x-velocity\n\n    # Reward being close to target\n    vel_error = torch.abs(forward_vel - target_vel)\n    return torch.exp(-vel_error / 0.25)\n\ndef reward_action_smoothness(env) -> torch.Tensor:\n    """Penalize jerky actions."""\n    action_diff = env.actions - env.previous_actions\n    return -torch.sum(action_diff ** 2, dim=-1)\n\ndef reward_energy_efficiency(env) -> torch.Tensor:\n    """Penalize high torques."""\n    torques = env.scene["robot"].data.applied_torque\n    return -torch.sum(torch.abs(torques), dim=-1) * 0.001\n'})}),"\n",(0,a.jsx)(e.h3,{id:"reward-configuration",children:"Reward Configuration"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",metastring:'title="rewards_cfg.py"',children:'"""Reward configuration for locomotion."""\nfrom omni.isaac.lab.managers import RewardTermCfg as RewTerm\n\n@configclass\nclass RewardsCfg:\n    """Reward terms for locomotion task."""\n\n    # Positive rewards\n    alive = RewTerm(func=reward_alive, weight=1.0)\n    forward_velocity = RewTerm(\n        func=reward_forward_velocity,\n        weight=2.0,\n        params={"target_vel": 1.5}\n    )\n\n    # Penalties\n    action_smoothness = RewTerm(\n        func=reward_action_smoothness,\n        weight=0.1\n    )\n    energy = RewTerm(\n        func=reward_energy_efficiency,\n        weight=0.05\n    )\n\n    # Termination penalty\n    termination = RewTerm(\n        func=lambda env: env.reset_buf.float(),\n        weight=-100.0\n    )\n'})}),"\n",(0,a.jsx)(e.h3,{id:"common-reward-patterns",children:"Common Reward Patterns"}),"\n",(0,a.jsxs)(e.table,{children:[(0,a.jsx)(e.thead,{children:(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.th,{children:"Pattern"}),(0,a.jsx)(e.th,{children:"Use Case"}),(0,a.jsx)(e.th,{children:"Implementation"})]})}),(0,a.jsxs)(e.tbody,{children:[(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:(0,a.jsx)(e.strong,{children:"Sparse"})}),(0,a.jsx)(e.td,{children:"Goal reaching"}),(0,a.jsx)(e.td,{children:"+1 at goal, 0 otherwise"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:(0,a.jsx)(e.strong,{children:"Dense"})}),(0,a.jsx)(e.td,{children:"Continuous feedback"}),(0,a.jsx)(e.td,{children:"Distance-based shaping"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:(0,a.jsx)(e.strong,{children:"Curriculum"})}),(0,a.jsx)(e.td,{children:"Progressive difficulty"}),(0,a.jsx)(e.td,{children:"Increase threshold over time"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:(0,a.jsx)(e.strong,{children:"Imitation"})}),(0,a.jsx)(e.td,{children:"Match demonstrations"}),(0,a.jsx)(e.td,{children:"Minimize action/state difference"})]})]})]}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"custom-environments",children:"Custom Environments"}),"\n",(0,a.jsx)(e.h3,{id:"creating-a-reach-task",children:"Creating a Reach Task"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",metastring:'title="custom_reach_env.py"',children:'"""Custom reaching environment for robot arm."""\nimport torch\nfrom omni.isaac.lab.envs import ManagerBasedRLEnv, ManagerBasedRLEnvCfg\nfrom omni.isaac.lab.assets import Articulation, ArticulationCfg\nfrom omni.isaac.lab.managers import SceneEntityCfg\n\n@configclass\nclass ReachEnvCfg(ManagerBasedRLEnvCfg):\n    """Configuration for reaching task."""\n\n    # Simulation settings\n    sim: SimulationCfg = SimulationCfg(dt=1/120)\n\n    # Scene with robot\n    scene: InteractiveSceneCfg = InteractiveSceneCfg(\n        num_envs=1024,\n        env_spacing=2.5,\n    )\n\n    # Robot configuration\n    robot: ArticulationCfg = ArticulationCfg(\n        prim_path="/World/envs/env_.*/Robot",\n        spawn=sim_utils.UsdFileCfg(\n            usd_path="path/to/robot.usd"\n        ),\n        init_state=ArticulationCfg.InitialStateCfg(\n            joint_pos={".*": 0.0},\n        ),\n    )\n\n    # Observations: joint states + target position\n    observations: ObservationsCfg = ObservationsCfg(\n        policy=ObservationGroupCfg(\n            observations={\n                "joint_pos": JointPosCfg(),\n                "joint_vel": JointVelCfg(),\n                "target_pos": TargetPosCfg(),\n            }\n        )\n    )\n\n    # Actions: joint position targets\n    actions: ActionsCfg = ActionsCfg(\n        joint_effort=JointEffortActionCfg(\n            asset_name="robot",\n            joint_names=[".*"],\n        )\n    )\n\n\nclass ReachEnv(ManagerBasedRLEnv):\n    """Custom reaching environment."""\n\n    cfg: ReachEnvCfg\n\n    def __init__(self, cfg: ReachEnvCfg):\n        super().__init__(cfg)\n\n        # Target position (randomized per episode)\n        self.target_pos = torch.zeros(\n            self.num_envs, 3, device=self.device\n        )\n\n    def _reset_idx(self, env_ids: torch.Tensor):\n        """Reset environments and randomize targets."""\n        super()._reset_idx(env_ids)\n\n        # Randomize target position in workspace\n        self.target_pos[env_ids] = torch.rand(\n            len(env_ids), 3, device=self.device\n        ) * 0.4 + torch.tensor([0.3, -0.2, 0.2])\n\n    def _compute_rewards(self) -> torch.Tensor:\n        """Compute reward based on distance to target."""\n        ee_pos = self.scene["robot"].data.body_pos_w[:, -1]  # End-effector\n        distance = torch.norm(ee_pos - self.target_pos, dim=-1)\n\n        # Dense reward: closer is better\n        reward = -distance\n\n        # Bonus for reaching target\n        reached = distance < 0.05\n        reward += reached.float() * 10.0\n\n        return reward\n'})}),"\n",(0,a.jsx)(e.h3,{id:"registering-custom-environment",children:"Registering Custom Environment"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",metastring:'title="__init__.py"',children:'"""Register custom environment."""\nimport gymnasium as gym\nfrom .custom_reach_env import ReachEnv, ReachEnvCfg\n\ngym.register(\n    id="Isaac-Custom-Reach-v0",\n    entry_point="omni.isaac.lab_tasks.custom:ReachEnv",\n    kwargs={"env_cfg": ReachEnvCfg()},\n)\n'})}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"domain-randomization",children:"Domain Randomization"}),"\n",(0,a.jsx)(e.h3,{id:"randomization-for-sim-to-real",children:"Randomization for Sim-to-Real"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",metastring:'title="domain_randomization.py"',children:'"""Domain randomization configuration."""\nfrom omni.isaac.lab.managers import RandomizationTermCfg\n\n@configclass\nclass RandomizationCfg:\n    """Randomization for sim-to-real transfer."""\n\n    # Physics randomization\n    mass_randomization = RandomizationTermCfg(\n        func=randomize_rigid_body_mass,\n        mode="startup",\n        params={\n            "asset_cfg": SceneEntityCfg("robot"),\n            "mass_range": (0.8, 1.2),  # \xb120%\n        }\n    )\n\n    friction_randomization = RandomizationTermCfg(\n        func=randomize_physics_material,\n        mode="reset",\n        params={\n            "static_friction_range": (0.5, 1.5),\n            "dynamic_friction_range": (0.5, 1.5),\n        }\n    )\n\n    # Actuator randomization\n    motor_strength = RandomizationTermCfg(\n        func=randomize_actuator_gains,\n        mode="startup",\n        params={\n            "stiffness_range": (0.9, 1.1),\n            "damping_range": (0.9, 1.1),\n        }\n    )\n\n    # Observation noise\n    observation_noise = RandomizationTermCfg(\n        func=add_observation_noise,\n        mode="interval",\n        interval_range_s=(0.0, 0.0),  # Every step\n        params={\n            "noise_std": 0.01,\n        }\n    )\n\n    # External disturbances\n    push_robot = RandomizationTermCfg(\n        func=push_by_setting_velocity,\n        mode="interval",\n        interval_range_s=(5.0, 10.0),\n        params={\n            "velocity_range": (-0.5, 0.5),\n        }\n    )\n'})}),"\n",(0,a.jsx)(e.h3,{id:"curriculum-learning",children:"Curriculum Learning"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",metastring:'title="curriculum.py"',children:'"""Curriculum learning for progressive training."""\nfrom omni.isaac.lab.managers import CurriculumTermCfg\n\n@configclass\nclass CurriculumCfg:\n    """Progressive difficulty curriculum."""\n\n    terrain_difficulty = CurriculumTermCfg(\n        func=terrain_levels_curriculum,\n        params={\n            "num_levels": 5,\n            "success_threshold": 0.8,\n        }\n    )\n\n    command_velocity = CurriculumTermCfg(\n        func=velocity_curriculum,\n        params={\n            "initial_range": (0.0, 0.5),\n            "final_range": (0.0, 2.0),\n            "num_steps": 1000,\n        }\n    )\n'})}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"policy-deployment",children:"Policy Deployment"}),"\n",(0,a.jsx)(e.h3,{id:"export-policy-for-deployment",children:"Export Policy for Deployment"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",metastring:'title="export_policy.py"',children:'"""Export trained policy for deployment."""\nimport torch\nimport onnx\n\ndef export_to_onnx(checkpoint_path: str, output_path: str):\n    """Export PyTorch policy to ONNX."""\n    # Load trained policy\n    checkpoint = torch.load(checkpoint_path)\n    policy = checkpoint["policy"]\n    policy.eval()\n\n    # Create dummy input matching observation shape\n    dummy_input = torch.randn(1, policy.obs_dim)\n\n    # Export to ONNX\n    torch.onnx.export(\n        policy,\n        dummy_input,\n        output_path,\n        export_params=True,\n        opset_version=13,\n        input_names=["observation"],\n        output_names=["action"],\n        dynamic_axes={\n            "observation": {0: "batch"},\n            "action": {0: "batch"},\n        }\n    )\n\n    print(f"Exported to {output_path}")\n\n# Usage\nexport_to_onnx(\n    "logs/rsl_rl/anymal/policy.pt",\n    "deployed_policy.onnx"\n)\n'})}),"\n",(0,a.jsx)(e.h3,{id:"ros-2-deployment-node",children:"ROS 2 Deployment Node"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",metastring:'title="policy_node.py"',children:'"""ROS 2 node for deployed policy."""\nimport rclpy\nfrom rclpy.node import Node\nimport numpy as np\nimport onnxruntime as ort\nfrom sensor_msgs.msg import JointState\nfrom std_msgs.msg import Float64MultiArray\n\nclass PolicyNode(Node):\n    def __init__(self):\n        super().__init__(\'policy_node\')\n\n        # Load ONNX policy\n        self.session = ort.InferenceSession("deployed_policy.onnx")\n        self.input_name = self.session.get_inputs()[0].name\n\n        # Subscribers\n        self.joint_sub = self.create_subscription(\n            JointState, \'/joint_states\',\n            self.joint_callback, 10\n        )\n\n        # Publisher\n        self.action_pub = self.create_publisher(\n            Float64MultiArray, \'/joint_commands\', 10\n        )\n\n        # State buffer\n        self.joint_pos = np.zeros(12)\n        self.joint_vel = np.zeros(12)\n\n        # Control loop at 50 Hz\n        self.timer = self.create_timer(0.02, self.control_loop)\n\n    def joint_callback(self, msg):\n        """Update joint state from robot."""\n        self.joint_pos = np.array(msg.position)\n        self.joint_vel = np.array(msg.velocity)\n\n    def control_loop(self):\n        """Run policy and publish actions."""\n        # Build observation\n        obs = np.concatenate([\n            self.joint_pos,\n            self.joint_vel,\n            # Add command velocity, gravity vector, etc.\n        ]).astype(np.float32).reshape(1, -1)\n\n        # Run inference\n        action = self.session.run(None, {self.input_name: obs})[0]\n\n        # Publish action\n        msg = Float64MultiArray()\n        msg.data = action.flatten().tolist()\n        self.action_pub.publish(msg)\n\n\ndef main():\n    rclpy.init()\n    node = PolicyNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"exercise-1-train-cartpole",children:"Exercise 1: Train Cartpole"}),"\n",(0,a.jsxs)(e.admonition,{title:"Exercise 1: Basic Policy Training",type:"tip",children:[(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Objective"}),": Train and evaluate a cartpole balancing policy."]}),(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Steps"}),":"]}),(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Launch Isaac Lab with cartpole environment"}),"\n",(0,a.jsx)(e.li,{children:"Train for 500 iterations with 1024 environments"}),"\n",(0,a.jsx)(e.li,{children:"Monitor training with TensorBoard"}),"\n",(0,a.jsx)(e.li,{children:"Visualize the trained policy"}),"\n",(0,a.jsx)(e.li,{children:"Experiment with different hyperparameters"}),"\n"]}),(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Commands"}),":"]}),(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"# Train\npython source/standalone/workflows/rl_games/train.py \\\n  --task Isaac-Cartpole-v0 \\\n  --num_envs 1024\n\n# Monitor\ntensorboard --logdir logs/rl_games/cartpole\n\n# Play\npython source/standalone/workflows/rl_games/play.py \\\n  --task Isaac-Cartpole-v0 \\\n  --checkpoint <path_to_checkpoint>\n"})}),(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Expected Result"}),": Policy achieves 195+ reward (solved)."]}),(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Time Estimate"}),": 30 minutes"]})]}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"exercise-2-quadruped-locomotion",children:"Exercise 2: Quadruped Locomotion"}),"\n",(0,a.jsxs)(e.admonition,{title:"Exercise 2: Train Walking Robot",type:"tip",children:[(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Objective"}),": Train a quadruped robot to walk forward."]}),(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Steps"}),":"]}),(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Set up Anymal environment"}),"\n",(0,a.jsx)(e.li,{children:"Train with 4096 parallel environments"}),"\n",(0,a.jsx)(e.li,{children:"Analyze reward components"}),"\n",(0,a.jsx)(e.li,{children:"Test on different terrain types"}),"\n",(0,a.jsx)(e.li,{children:"Export policy for deployment"}),"\n"]}),(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Training Command"}),":"]}),(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"python source/standalone/workflows/rsl_rl/train.py \\\n  --task Isaac-Anymal-C-Flat-v0 \\\n  --num_envs 4096 \\\n  --max_iterations 2000\n"})}),(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Analysis Tasks"}),":"]}),(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Plot learning curves for each reward term"}),"\n",(0,a.jsx)(e.li,{children:"Compare forward velocity vs energy usage"}),"\n",(0,a.jsx)(e.li,{children:"Test robustness to external pushes"}),"\n"]}),(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Time Estimate"}),": 60 minutes"]})]}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"exercise-3-custom-environment",children:"Exercise 3: Custom Environment"}),"\n",(0,a.jsxs)(e.admonition,{title:"Exercise 3: Create Your Own Task",type:"tip",children:[(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Objective"}),": Build a custom environment for a specific task."]}),(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Steps"}),":"]}),(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Choose a task (pushing, picking, balancing)"}),"\n",(0,a.jsx)(e.li,{children:"Define observation and action spaces"}),"\n",(0,a.jsx)(e.li,{children:"Implement reward function"}),"\n",(0,a.jsx)(e.li,{children:"Configure domain randomization"}),"\n",(0,a.jsx)(e.li,{children:"Train and iterate on reward design"}),"\n"]}),(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Template"}),":"]}),(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# Start from existing environment\n# Modify rewards and terminations\n# Add task-specific observations\n"})}),(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Deliverables"}),":"]}),(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Working environment configuration"}),"\n",(0,a.jsx)(e.li,{children:"Trained policy achieving task"}),"\n",(0,a.jsx)(e.li,{children:"Documentation of reward design choices"}),"\n"]}),(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Time Estimate"}),": 2-3 hours"]})]}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(e.p,{children:"In this chapter, you learned:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"RL Fundamentals"}),": States, actions, rewards, and policies"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Isaac Lab Setup"}),": Installation and project structure"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Training"}),": Using RL Games and RSL-RL for policy learning"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Reward Engineering"}),": Designing effective reward functions"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Custom Environments"}),": Building task-specific simulations"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Sim-to-Real"}),": Domain randomization and policy deployment"]}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:"Isaac Lab's massively parallel simulation enables training robot policies that would be impractical with traditional approaches. The key to success is careful reward design and thorough domain randomization."}),"\n",(0,a.jsxs)(e.p,{children:["Next, explore ",(0,a.jsx)(e.a,{href:"/docs/module-3/perception-pipelines",children:"Perception Pipelines"})," to build end-to-end systems combining learned policies with real-time perception."]}),"\n",(0,a.jsx)(e.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"https://isaac-sim.github.io/IsaacLab/",children:"Isaac Lab Documentation"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"https://github.com/isaac-sim/IsaacLab",children:"Isaac Lab GitHub"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"https://github.com/Denys88/rl_games",children:"RL Games"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"https://github.com/leggedrobotics/rsl_rl",children:"RSL-RL"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"https://spinningup.openai.com/",children:"Spinning Up in Deep RL"})}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>t,x:()=>o});var r=i(6540);const a={},s=r.createContext(a);function t(n){const e=r.useContext(s);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:t(n.components),r.createElement(s.Provider,{value:e},n.children)}}}]);