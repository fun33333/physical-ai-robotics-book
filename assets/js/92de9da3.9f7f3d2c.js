"use strict";(globalThis.webpackChunkbook_website=globalThis.webpackChunkbook_website||[]).push([[2496],{7301:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>s,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-4/end-to-end-vla","title":"End-to-End VLA Systems","description":"Building complete VLA systems: architecture patterns, training strategies, and real-world deployment.","source":"@site/docs/module-4/end-to-end-vla.md","sourceDirName":"module-4","slug":"/module-4/end-to-end-vla","permalink":"/physical-ai-robotics-book/docs/module-4/end-to-end-vla","draft":false,"unlisted":false,"editUrl":"https://github.com/fun33333/physical-ai-robotics-book/tree/main/book-website/docs/module-4/end-to-end-vla.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"title":"End-to-End VLA Systems","sidebar_position":6,"description":"Building complete VLA systems: architecture patterns, training strategies, and real-world deployment."},"sidebar":"tutorialSidebar","previous":{"title":"Action Generation","permalink":"/physical-ai-robotics-book/docs/module-4/action-generation"},"next":{"title":"Hardware Guide for Physical AI","permalink":"/physical-ai-robotics-book/docs/hardware/"}}');var o=t(4848),r=t(8453);const s={title:"End-to-End VLA Systems",sidebar_position:6,description:"Building complete VLA systems: architecture patterns, training strategies, and real-world deployment."},a="End-to-End VLA Systems",l={},d=[{value:"Overview",id:"overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"VLA Architecture Patterns",id:"vla-architecture-patterns",level:2},{value:"System Overview",id:"system-overview",level:3},{value:"Architecture Variants",id:"architecture-variants",level:3},{value:"Complete VLA Implementation",id:"complete-vla-implementation",level:2},{value:"Full Model Architecture",id:"full-model-architecture",level:3},{value:"Training Pipeline",id:"training-pipeline",level:3},{value:"Data Collection",id:"data-collection",level:2},{value:"Demonstration Dataset",id:"demonstration-dataset",level:3},{value:"Fine-Tuning from Foundation Models",id:"fine-tuning-from-foundation-models",level:2},{value:"Loading Pre-trained Weights",id:"loading-pre-trained-weights",level:3},{value:"Deployment",id:"deployment",level:2},{value:"ROS 2 Deployment Node",id:"ros-2-deployment-node",level:3},{value:"Inference Optimization",id:"inference-optimization",level:3},{value:"Evaluation",id:"evaluation",level:2},{value:"Evaluation Metrics",id:"evaluation-metrics",level:3},{value:"Exercise 1: Build Complete VLA",id:"exercise-1-build-complete-vla",level:2},{value:"Exercise 2: Collect Demonstrations",id:"exercise-2-collect-demonstrations",level:2},{value:"Exercise 3: Deploy and Evaluate",id:"exercise-3-deploy-and-evaluate",level:2},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2}];function c(n){const e={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"end-to-end-vla-systems",children:"End-to-End VLA Systems"})}),"\n",(0,o.jsx)(e.p,{children:"With an understanding of all the components, this chapter brings everything together into complete, deployable VLA systems. You'll learn architecture patterns that combine vision, language, and action generation into coherent wholes, along with training strategies and deployment considerations for real-world robotics."}),"\n",(0,o.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(e.p,{children:"In this section, you will:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Design complete VLA system architectures"}),"\n",(0,o.jsx)(e.li,{children:"Implement end-to-end training pipelines"}),"\n",(0,o.jsx)(e.li,{children:"Collect and manage demonstration data"}),"\n",(0,o.jsx)(e.li,{children:"Fine-tune from foundation models"}),"\n",(0,o.jsx)(e.li,{children:"Deploy VLA models on robot hardware"}),"\n",(0,o.jsx)(e.li,{children:"Evaluate and iterate on system performance"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Completed all previous Module 4 chapters"}),"\n",(0,o.jsx)(e.li,{children:"PyTorch and Hugging Face Transformers experience"}),"\n",(0,o.jsx)(e.li,{children:"Access to robot simulation environment"}),"\n",(0,o.jsx)(e.li,{children:"Understanding of imitation learning concepts"}),"\n"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"vla-architecture-patterns",children:"VLA Architecture Patterns"}),"\n",(0,o.jsx)(e.h3,{id:"system-overview",children:"System Overview"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    End-to-End VLA System                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                  \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502   \u2502                      Inputs                               \u2502  \u2502\n\u2502   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502  \u2502\n\u2502   \u2502   \u2502  Camera   \u2502  \u2502 Language  \u2502  \u2502  Proprioception   \u2502   \u2502  \u2502\n\u2502   \u2502   \u2502  Images   \u2502  \u2502 Command   \u2502  \u2502  (Joint States)   \u2502   \u2502  \u2502\n\u2502   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502  \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502             \u2502              \u2502                  \u2502                 \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502   \u2502                    Encoders                              \u2502  \u2502\n\u2502   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502  \u2502\n\u2502   \u2502   \u2502   Vision  \u2502  \u2502  Language \u2502  \u2502   State Encoder   \u2502   \u2502  \u2502\n\u2502   \u2502   \u2502   (ViT)   \u2502  \u2502  (BERT)   \u2502  \u2502      (MLP)        \u2502   \u2502  \u2502\n\u2502   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502  \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502             \u2502              \u2502                  \u2502                 \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502   \u2502                  Fusion Module                           \u2502  \u2502\n\u2502   \u2502          Cross-Attention Transformer Layers             \u2502  \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                            \u2502                                    \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502   \u2502                  Action Decoder                          \u2502  \u2502\n\u2502   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502  \u2502\n\u2502   \u2502   \u2502  Diffusion / Transformer / Flow Matching Policy  \u2502  \u2502  \u2502\n\u2502   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502  \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                            \u2502                                    \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502   \u2502                     Outputs                              \u2502  \u2502\n\u2502   \u2502      Action Trajectory: [a\u2081, a\u2082, ..., a\u209c]               \u2502  \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,o.jsx)(e.h3,{id:"architecture-variants",children:"Architecture Variants"}),"\n",(0,o.jsxs)(e.table,{children:[(0,o.jsx)(e.thead,{children:(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.th,{children:"Architecture"}),(0,o.jsx)(e.th,{children:"Vision"}),(0,o.jsx)(e.th,{children:"Language"}),(0,o.jsx)(e.th,{children:"Fusion"}),(0,o.jsx)(e.th,{children:"Action"}),(0,o.jsx)(e.th,{children:"Example"})]})}),(0,o.jsxs)(e.tbody,{children:[(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:(0,o.jsx)(e.strong,{children:"RT-1"})}),(0,o.jsx)(e.td,{children:"EfficientNet"}),(0,o.jsx)(e.td,{children:"USE"}),(0,o.jsx)(e.td,{children:"FiLM"}),(0,o.jsx)(e.td,{children:"Discrete tokens"}),(0,o.jsx)(e.td,{children:"Google"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:(0,o.jsx)(e.strong,{children:"RT-2"})}),(0,o.jsx)(e.td,{children:"ViT (PaLI)"}),(0,o.jsx)(e.td,{children:"LLM"}),(0,o.jsx)(e.td,{children:"Joint embedding"}),(0,o.jsx)(e.td,{children:"Discrete tokens"}),(0,o.jsx)(e.td,{children:"Google"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:(0,o.jsx)(e.strong,{children:"OpenVLA"})}),(0,o.jsx)(e.td,{children:"SigLIP"}),(0,o.jsx)(e.td,{children:"Llama 2"}),(0,o.jsx)(e.td,{children:"Projector"}),(0,o.jsx)(e.td,{children:"Discrete tokens"}),(0,o.jsx)(e.td,{children:"Berkeley"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:(0,o.jsx)(e.strong,{children:"Octo"})}),(0,o.jsx)(e.td,{children:"ViT"}),(0,o.jsx)(e.td,{children:"T5"}),(0,o.jsx)(e.td,{children:"Cross-attention"}),(0,o.jsx)(e.td,{children:"Diffusion"}),(0,o.jsx)(e.td,{children:"Berkeley"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:(0,o.jsx)(e.strong,{children:"ACT"})}),(0,o.jsx)(e.td,{children:"ResNet"}),(0,o.jsx)(e.td,{children:"-"}),(0,o.jsx)(e.td,{children:"Transformer"}),(0,o.jsx)(e.td,{children:"Continuous"}),(0,o.jsx)(e.td,{children:"Stanford"})]})]})]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"complete-vla-implementation",children:"Complete VLA Implementation"}),"\n",(0,o.jsx)(e.h3,{id:"full-model-architecture",children:"Full Model Architecture"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",metastring:'title="vla_model.py"',children:'"""Complete Vision-Language-Action model implementation."""\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoModel, AutoTokenizer\nfrom typing import Dict, Optional, Tuple\n\nclass VLAModel(nn.Module):\n    """End-to-end VLA model for robot control."""\n\n    def __init__(\n        self,\n        vision_encoder: str = "google/siglip-base-patch16-224",\n        language_encoder: str = "bert-base-uncased",\n        hidden_dim: int = 512,\n        action_dim: int = 7,\n        action_horizon: int = 16,\n        num_transformer_layers: int = 6,\n        num_heads: int = 8,\n        freeze_encoders: bool = True\n    ):\n        super().__init__()\n\n        self.action_dim = action_dim\n        self.action_horizon = action_horizon\n\n        # Vision encoder\n        self.vision_encoder = AutoModel.from_pretrained(vision_encoder)\n        self.vision_dim = self.vision_encoder.config.hidden_size\n\n        # Language encoder\n        self.language_encoder = AutoModel.from_pretrained(language_encoder)\n        self.tokenizer = AutoTokenizer.from_pretrained(language_encoder)\n        self.language_dim = self.language_encoder.config.hidden_size\n\n        # Optionally freeze encoders\n        if freeze_encoders:\n            for param in self.vision_encoder.parameters():\n                param.requires_grad = False\n            for param in self.language_encoder.parameters():\n                param.requires_grad = False\n\n        # Projection layers\n        self.vision_proj = nn.Linear(self.vision_dim, hidden_dim)\n        self.language_proj = nn.Linear(self.language_dim, hidden_dim)\n        self.proprio_proj = nn.Linear(action_dim, hidden_dim)\n\n        # Fusion transformer\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=hidden_dim,\n            nhead=num_heads,\n            dim_feedforward=hidden_dim * 4,\n            batch_first=True\n        )\n        self.fusion_transformer = nn.TransformerEncoder(\n            encoder_layer,\n            num_layers=num_transformer_layers\n        )\n\n        # Action decoder (using diffusion)\n        from .action_generation import ConditionalUNet1D, DiffusionPolicy\n        self.action_unet = ConditionalUNet1D(\n            action_dim=action_dim,\n            action_horizon=action_horizon,\n            obs_dim=hidden_dim\n        )\n        self.diffusion = DiffusionPolicy(\n            model=self.action_unet,\n            action_dim=action_dim,\n            action_horizon=action_horizon\n        )\n\n        # Learnable tokens\n        self.cls_token = nn.Parameter(torch.randn(1, 1, hidden_dim))\n        self.action_token = nn.Parameter(torch.randn(1, 1, hidden_dim))\n\n    def encode_observation(\n        self,\n        image: torch.Tensor,\n        instruction: str,\n        proprio: torch.Tensor\n    ) -> torch.Tensor:\n        """\n        Encode multimodal observation.\n\n        Args:\n            image: [batch, 3, H, W] camera image\n            instruction: text instruction (will be tokenized)\n            proprio: [batch, action_dim] proprioceptive state\n\n        Returns:\n            fused_features: [batch, hidden_dim]\n        """\n        batch_size = image.size(0)\n        device = image.device\n\n        # Encode vision\n        with torch.no_grad() if not self.vision_encoder.training else torch.enable_grad():\n            vision_out = self.vision_encoder(pixel_values=image)\n            vision_features = vision_out.last_hidden_state  # [B, num_patches, vision_dim]\n\n        # Encode language\n        tokens = self.tokenizer(\n            instruction,\n            return_tensors="pt",\n            padding=True,\n            truncation=True,\n            max_length=77\n        ).to(device)\n\n        with torch.no_grad() if not self.language_encoder.training else torch.enable_grad():\n            language_out = self.language_encoder(**tokens)\n            language_features = language_out.last_hidden_state  # [B, seq_len, lang_dim]\n\n        # Project to common dimension\n        v = self.vision_proj(vision_features)  # [B, P, H]\n        l = self.language_proj(language_features)  # [B, S, H]\n        p = self.proprio_proj(proprio).unsqueeze(1)  # [B, 1, H]\n\n        # Add special tokens\n        cls = self.cls_token.expand(batch_size, -1, -1)\n        action_tok = self.action_token.expand(batch_size, -1, -1)\n\n        # Concatenate all tokens\n        combined = torch.cat([cls, v, l, p, action_tok], dim=1)\n\n        # Fuse with transformer\n        fused = self.fusion_transformer(combined)\n\n        # Extract action conditioning (last token)\n        action_condition = fused[:, -1]  # [B, H]\n\n        return action_condition\n\n    def forward(\n        self,\n        image: torch.Tensor,\n        instruction: str,\n        proprio: torch.Tensor,\n        action: torch.Tensor = None\n    ) -> Dict[str, torch.Tensor]:\n        """\n        Forward pass for training or inference.\n\n        Args:\n            image: [batch, 3, H, W]\n            instruction: text string\n            proprio: [batch, action_dim]\n            action: [batch, horizon, action_dim] for training\n\n        Returns:\n            dict with loss (training) or action (inference)\n        """\n        # Encode observation\n        obs_encoding = self.encode_observation(image, instruction, proprio)\n\n        if action is not None:\n            # Training: compute diffusion loss\n            loss = self.diffusion.training_loss(action, obs_encoding)\n            return {"loss": loss}\n        else:\n            # Inference: sample actions\n            actions = self.diffusion.sample(obs_encoding)\n            return {"actions": actions}\n\n    @torch.no_grad()\n    def predict_action(\n        self,\n        image: torch.Tensor,\n        instruction: str,\n        proprio: torch.Tensor\n    ) -> torch.Tensor:\n        """Predict action trajectory for deployment."""\n        self.eval()\n        output = self.forward(image, instruction, proprio)\n        return output["actions"]\n\n\nclass VLATokenizer:\n    """Unified tokenization for VLA inputs."""\n\n    def __init__(\n        self,\n        text_tokenizer: str = "bert-base-uncased",\n        action_vocab_size: int = 256,\n        action_dim: int = 7\n    ):\n        self.text_tokenizer = AutoTokenizer.from_pretrained(text_tokenizer)\n        self.action_vocab_size = action_vocab_size\n        self.action_dim = action_dim\n\n        # Special tokens for action\n        self.action_start_token = action_vocab_size\n        self.action_end_token = action_vocab_size + 1\n\n    def tokenize_action(self, action: torch.Tensor) -> torch.Tensor:\n        """Convert continuous action to tokens."""\n        # Normalize to [0, 1]\n        action_normalized = (action + 1) / 2  # Assume action in [-1, 1]\n        action_normalized = torch.clamp(action_normalized, 0, 1)\n\n        # Discretize\n        tokens = (action_normalized * (self.action_vocab_size - 1)).long()\n        return tokens\n\n    def detokenize_action(self, tokens: torch.Tensor) -> torch.Tensor:\n        """Convert tokens back to continuous action."""\n        action_normalized = tokens.float() / (self.action_vocab_size - 1)\n        action = action_normalized * 2 - 1  # Map back to [-1, 1]\n        return action\n'})}),"\n",(0,o.jsx)(e.h3,{id:"training-pipeline",children:"Training Pipeline"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",metastring:'title="train_vla.py"',children:'"""Training pipeline for VLA models."""\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport wandb\nfrom tqdm import tqdm\n\nclass VLATrainer:\n    """Trainer for VLA models."""\n\n    def __init__(\n        self,\n        model: VLAModel,\n        train_dataset,\n        val_dataset,\n        batch_size: int = 32,\n        learning_rate: float = 1e-4,\n        num_epochs: int = 100,\n        gradient_accumulation: int = 1,\n        device: str = "cuda"\n    ):\n        self.model = model.to(device)\n        self.device = device\n        self.num_epochs = num_epochs\n        self.gradient_accumulation = gradient_accumulation\n\n        # Data loaders\n        self.train_loader = DataLoader(\n            train_dataset,\n            batch_size=batch_size,\n            shuffle=True,\n            num_workers=4,\n            pin_memory=True\n        )\n        self.val_loader = DataLoader(\n            val_dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=4\n        )\n\n        # Optimizer\n        self.optimizer = AdamW(\n            model.parameters(),\n            lr=learning_rate,\n            weight_decay=0.01\n        )\n\n        # Scheduler\n        self.scheduler = CosineAnnealingLR(\n            self.optimizer,\n            T_max=num_epochs * len(self.train_loader)\n        )\n\n    def train_epoch(self, epoch: int) -> float:\n        """Train for one epoch."""\n        self.model.train()\n        total_loss = 0\n        num_batches = 0\n\n        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch}")\n        self.optimizer.zero_grad()\n\n        for i, batch in enumerate(pbar):\n            # Move to device\n            images = batch["image"].to(self.device)\n            instructions = batch["instruction"]\n            proprio = batch["proprio"].to(self.device)\n            actions = batch["action"].to(self.device)\n\n            # Forward pass\n            output = self.model(images, instructions, proprio, actions)\n            loss = output["loss"] / self.gradient_accumulation\n\n            # Backward pass\n            loss.backward()\n\n            if (i + 1) % self.gradient_accumulation == 0:\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                self.optimizer.step()\n                self.scheduler.step()\n                self.optimizer.zero_grad()\n\n            total_loss += loss.item() * self.gradient_accumulation\n            num_batches += 1\n\n            pbar.set_postfix({"loss": loss.item() * self.gradient_accumulation})\n\n        return total_loss / num_batches\n\n    @torch.no_grad()\n    def validate(self) -> dict:\n        """Run validation."""\n        self.model.eval()\n        total_loss = 0\n        total_mse = 0\n        num_batches = 0\n\n        for batch in self.val_loader:\n            images = batch["image"].to(self.device)\n            instructions = batch["instruction"]\n            proprio = batch["proprio"].to(self.device)\n            actions = batch["action"].to(self.device)\n\n            # Compute loss\n            output = self.model(images, instructions, proprio, actions)\n            total_loss += output["loss"].item()\n\n            # Compute action prediction error\n            pred_actions = self.model.predict_action(images, instructions, proprio)\n            mse = torch.nn.functional.mse_loss(pred_actions, actions)\n            total_mse += mse.item()\n\n            num_batches += 1\n\n        return {\n            "val_loss": total_loss / num_batches,\n            "val_mse": total_mse / num_batches\n        }\n\n    def train(self):\n        """Full training loop."""\n        wandb.init(project="vla-training")\n\n        best_val_loss = float("inf")\n\n        for epoch in range(self.num_epochs):\n            train_loss = self.train_epoch(epoch)\n            val_metrics = self.validate()\n\n            wandb.log({\n                "train_loss": train_loss,\n                **val_metrics,\n                "epoch": epoch,\n                "lr": self.scheduler.get_last_lr()[0]\n            })\n\n            print(f"Epoch {epoch}: train_loss={train_loss:.4f}, "\n                  f"val_loss={val_metrics[\'val_loss\']:.4f}")\n\n            # Save best model\n            if val_metrics["val_loss"] < best_val_loss:\n                best_val_loss = val_metrics["val_loss"]\n                torch.save(\n                    self.model.state_dict(),\n                    "best_vla_model.pt"\n                )\n\n        wandb.finish()\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"data-collection",children:"Data Collection"}),"\n",(0,o.jsx)(e.h3,{id:"demonstration-dataset",children:"Demonstration Dataset"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",metastring:'title="demonstration_dataset.py"',children:'"""Dataset for robot demonstrations."""\nimport torch\nfrom torch.utils.data import Dataset\nimport numpy as np\nfrom pathlib import Path\nimport h5py\nfrom PIL import Image\nfrom torchvision import transforms\n\nclass DemonstrationDataset(Dataset):\n    """Dataset of robot demonstrations for VLA training."""\n\n    def __init__(\n        self,\n        data_dir: str,\n        action_horizon: int = 16,\n        image_size: int = 224,\n        augment: bool = True\n    ):\n        self.data_dir = Path(data_dir)\n        self.action_horizon = action_horizon\n\n        # Image transforms\n        if augment:\n            self.transform = transforms.Compose([\n                transforms.Resize((image_size, image_size)),\n                transforms.ColorJitter(0.1, 0.1, 0.1),\n                transforms.RandomAffine(degrees=5, translate=(0.05, 0.05)),\n                transforms.ToTensor(),\n                transforms.Normalize(\n                    mean=[0.485, 0.456, 0.406],\n                    std=[0.229, 0.224, 0.225]\n                )\n            ])\n        else:\n            self.transform = transforms.Compose([\n                transforms.Resize((image_size, image_size)),\n                transforms.ToTensor(),\n                transforms.Normalize(\n                    mean=[0.485, 0.456, 0.406],\n                    std=[0.229, 0.224, 0.225]\n                )\n            ])\n\n        # Load episode index\n        self.episodes = self._load_episodes()\n        self.samples = self._build_sample_index()\n\n    def _load_episodes(self) -> list:\n        """Load all episode files."""\n        episodes = []\n        for h5_file in sorted(self.data_dir.glob("*.h5")):\n            with h5py.File(h5_file, "r") as f:\n                episodes.append({\n                    "path": h5_file,\n                    "length": f["actions"].shape[0],\n                    "instruction": f.attrs["instruction"]\n                })\n        return episodes\n\n    def _build_sample_index(self) -> list:\n        """Build index of valid samples."""\n        samples = []\n        for ep_idx, ep in enumerate(self.episodes):\n            # Each timestep that has enough future actions\n            for t in range(ep["length"] - self.action_horizon):\n                samples.append((ep_idx, t))\n        return samples\n\n    def __len__(self) -> int:\n        return len(self.samples)\n\n    def __getitem__(self, idx: int) -> dict:\n        ep_idx, t = self.samples[idx]\n        episode = self.episodes[ep_idx]\n\n        with h5py.File(episode["path"], "r") as f:\n            # Load image\n            image = Image.fromarray(f["images"][t])\n            image = self.transform(image)\n\n            # Load proprioception\n            proprio = torch.tensor(f["proprio"][t], dtype=torch.float32)\n\n            # Load action trajectory\n            action = torch.tensor(\n                f["actions"][t:t + self.action_horizon],\n                dtype=torch.float32\n            )\n\n        return {\n            "image": image,\n            "instruction": episode["instruction"],\n            "proprio": proprio,\n            "action": action\n        }\n\n\nclass DataCollector:\n    """Collect demonstrations for VLA training."""\n\n    def __init__(\n        self,\n        robot_interface,\n        save_dir: str,\n        camera_topic: str = "/camera/color/image_raw"\n    ):\n        self.robot = robot_interface\n        self.save_dir = Path(save_dir)\n        self.save_dir.mkdir(parents=True, exist_ok=True)\n        self.camera_topic = camera_topic\n\n        self.episode_count = 0\n\n    def collect_episode(self, instruction: str) -> bool:\n        """Collect single demonstration episode."""\n        print(f"Collecting episode for: {instruction}")\n        print("Press Enter to start, \'q\' to quit...")\n\n        if input().lower() == \'q\':\n            return False\n\n        # Storage for episode data\n        images = []\n        proprios = []\n        actions = []\n\n        print("Recording... Press Enter when done.")\n\n        try:\n            while True:\n                # Get current observation\n                image = self.robot.get_camera_image()\n                proprio = self.robot.get_joint_positions()\n\n                # Get action (from teleoperation or controller)\n                action = self.robot.get_commanded_action()\n\n                # Store\n                images.append(image)\n                proprios.append(proprio)\n                actions.append(action)\n\n                # Check for completion\n                import sys\n                import select\n                if select.select([sys.stdin], [], [], 0.01)[0]:\n                    if sys.stdin.readline().strip() == \'\':\n                        break\n\n        except KeyboardInterrupt:\n            pass\n\n        # Save episode\n        if len(images) > 10:  # Minimum episode length\n            self._save_episode(images, proprios, actions, instruction)\n            self.episode_count += 1\n            print(f"Saved episode {self.episode_count} ({len(images)} frames)")\n            return True\n        else:\n            print("Episode too short, discarding.")\n            return True\n\n    def _save_episode(\n        self,\n        images: list,\n        proprios: list,\n        actions: list,\n        instruction: str\n    ):\n        """Save episode to HDF5 file."""\n        filename = self.save_dir / f"episode_{self.episode_count:05d}.h5"\n\n        with h5py.File(filename, "w") as f:\n            f.create_dataset("images", data=np.array(images))\n            f.create_dataset("proprio", data=np.array(proprios))\n            f.create_dataset("actions", data=np.array(actions))\n            f.attrs["instruction"] = instruction\n            f.attrs["length"] = len(images)\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"fine-tuning-from-foundation-models",children:"Fine-Tuning from Foundation Models"}),"\n",(0,o.jsx)(e.h3,{id:"loading-pre-trained-weights",children:"Loading Pre-trained Weights"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",metastring:'title="finetune_vla.py"',children:'"""Fine-tuning VLA from foundation models."""\nimport torch\nfrom transformers import AutoModel\n\nclass VLAFineTuner:\n    """Fine-tune VLA from pre-trained foundation models."""\n\n    def __init__(\n        self,\n        base_model: VLAModel,\n        pretrained_vla: str = None,\n        freeze_vision: bool = True,\n        freeze_language: bool = True,\n        lora_rank: int = 16\n    ):\n        self.model = base_model\n\n        # Load pre-trained VLA weights if available\n        if pretrained_vla:\n            self._load_pretrained(pretrained_vla)\n\n        # Configure what to train\n        self._configure_training(freeze_vision, freeze_language, lora_rank)\n\n    def _load_pretrained(self, checkpoint_path: str):\n        """Load pre-trained VLA weights."""\n        state_dict = torch.load(checkpoint_path, map_location="cpu")\n\n        # Handle partial loading\n        model_dict = self.model.state_dict()\n        pretrained_dict = {\n            k: v for k, v in state_dict.items()\n            if k in model_dict and v.shape == model_dict[k].shape\n        }\n\n        model_dict.update(pretrained_dict)\n        self.model.load_state_dict(model_dict)\n\n        print(f"Loaded {len(pretrained_dict)}/{len(model_dict)} parameters")\n\n    def _configure_training(\n        self,\n        freeze_vision: bool,\n        freeze_language: bool,\n        lora_rank: int\n    ):\n        """Configure which parameters to train."""\n        # Freeze encoders\n        if freeze_vision:\n            for param in self.model.vision_encoder.parameters():\n                param.requires_grad = False\n\n        if freeze_language:\n            for param in self.model.language_encoder.parameters():\n                param.requires_grad = False\n\n        # Add LoRA to projection layers\n        if lora_rank > 0:\n            self._add_lora(lora_rank)\n\n        # Print trainable parameters\n        trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n        total = sum(p.numel() for p in self.model.parameters())\n        print(f"Trainable: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)")\n\n    def _add_lora(self, rank: int):\n        """Add LoRA adapters to linear layers."""\n        from peft import LoraConfig, get_peft_model\n\n        lora_config = LoraConfig(\n            r=rank,\n            lora_alpha=32,\n            target_modules=["vision_proj", "language_proj", "proprio_proj"],\n            lora_dropout=0.1\n        )\n\n        # Apply LoRA (simplified - would need proper implementation)\n        for name, module in self.model.named_modules():\n            if isinstance(module, torch.nn.Linear) and any(\n                t in name for t in lora_config.target_modules\n            ):\n                # Add LoRA parameters\n                in_features = module.in_features\n                out_features = module.out_features\n\n                module.lora_A = torch.nn.Parameter(\n                    torch.randn(rank, in_features) * 0.01\n                )\n                module.lora_B = torch.nn.Parameter(\n                    torch.zeros(out_features, rank)\n                )\n\n\ndef create_finetuning_optimizer(model, learning_rate: float = 1e-4):\n    """Create optimizer with different LR for different components."""\n    param_groups = [\n        # Frozen encoder params (if unfrozen for fine-tuning)\n        {\n            "params": [p for n, p in model.named_parameters()\n                      if "encoder" in n and p.requires_grad],\n            "lr": learning_rate * 0.1\n        },\n        # Projection layers\n        {\n            "params": [p for n, p in model.named_parameters()\n                      if "proj" in n and p.requires_grad],\n            "lr": learning_rate\n        },\n        # Action decoder\n        {\n            "params": [p for n, p in model.named_parameters()\n                      if "action" in n or "diffusion" in n and p.requires_grad],\n            "lr": learning_rate * 2\n        },\n        # LoRA parameters\n        {\n            "params": [p for n, p in model.named_parameters()\n                      if "lora" in n and p.requires_grad],\n            "lr": learning_rate * 5\n        }\n    ]\n\n    return torch.optim.AdamW(param_groups, weight_decay=0.01)\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"deployment",children:"Deployment"}),"\n",(0,o.jsx)(e.h3,{id:"ros-2-deployment-node",children:"ROS 2 Deployment Node"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",metastring:'title="vla_ros_node.py"',children:'"""ROS 2 node for VLA model deployment."""\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, JointState\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom cv_bridge import CvBridge\nimport torch\nimport numpy as np\nfrom threading import Lock\n\nclass VLAControlNode(Node):\n    """ROS 2 node for VLA-based robot control."""\n\n    def __init__(self):\n        super().__init__(\'vla_controller\')\n\n        # Parameters\n        self.declare_parameter(\'model_path\', \'best_vla_model.pt\')\n        self.declare_parameter(\'control_rate\', 10.0)\n        self.declare_parameter(\'action_horizon\', 16)\n        self.declare_parameter(\'replan_interval\', 4)\n\n        # Load model\n        self.device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n        self.model = self._load_model()\n        self.model.eval()\n\n        # CV bridge\n        self.bridge = CvBridge()\n\n        # State\n        self.current_image = None\n        self.current_joints = None\n        self.current_instruction = "do nothing"\n        self.action_buffer = None\n        self.action_index = 0\n        self.state_lock = Lock()\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/color/image_raw\',\n            self.image_callback, 10\n        )\n        self.joint_sub = self.create_subscription(\n            JointState, \'/joint_states\',\n            self.joint_callback, 10\n        )\n        self.instruction_sub = self.create_subscription(\n            String, \'/vla/instruction\',\n            self.instruction_callback, 10\n        )\n\n        # Publisher\n        self.action_pub = self.create_publisher(\n            JointState, \'/joint_commands\', 10\n        )\n\n        # Control timer\n        control_rate = self.get_parameter(\'control_rate\').value\n        self.control_timer = self.create_timer(\n            1.0 / control_rate, self.control_loop\n        )\n\n        # Replan timer\n        replan_interval = self.get_parameter(\'replan_interval\').value\n        self.replan_timer = self.create_timer(\n            replan_interval / control_rate, self.replan\n        )\n\n        self.get_logger().info(\'VLA controller initialized\')\n\n    def _load_model(self) -> VLAModel:\n        """Load VLA model."""\n        model_path = self.get_parameter(\'model_path\').value\n        model = VLAModel()  # Configure as needed\n        model.load_state_dict(torch.load(model_path, map_location=self.device))\n        model.to(self.device)\n        return model\n\n    def image_callback(self, msg):\n        """Update current image."""\n        with self.state_lock:\n            self.current_image = self.bridge.imgmsg_to_cv2(msg, \'rgb8\')\n\n    def joint_callback(self, msg):\n        """Update current joint state."""\n        with self.state_lock:\n            self.current_joints = np.array(msg.position)\n\n    def instruction_callback(self, msg):\n        """Update current instruction."""\n        with self.state_lock:\n            self.current_instruction = msg.data\n            self.action_buffer = None  # Force replan\n            self.action_index = 0\n        self.get_logger().info(f\'New instruction: {msg.data}\')\n\n    def replan(self):\n        """Generate new action trajectory."""\n        with self.state_lock:\n            if self.current_image is None or self.current_joints is None:\n                return\n\n            image = self.current_image.copy()\n            joints = self.current_joints.copy()\n            instruction = self.current_instruction\n\n        # Preprocess\n        image_tensor = self._preprocess_image(image)\n        proprio_tensor = torch.tensor(joints, dtype=torch.float32).unsqueeze(0)\n\n        # Predict\n        with torch.no_grad():\n            actions = self.model.predict_action(\n                image_tensor.to(self.device),\n                instruction,\n                proprio_tensor.to(self.device)\n            )\n\n        with self.state_lock:\n            self.action_buffer = actions.cpu().numpy()[0]  # [horizon, action_dim]\n            self.action_index = 0\n\n        self.get_logger().debug(\'Replanned action trajectory\')\n\n    def _preprocess_image(self, image: np.ndarray) -> torch.Tensor:\n        """Preprocess image for model."""\n        from torchvision import transforms\n\n        transform = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n\n        return transform(image).unsqueeze(0)\n\n    def control_loop(self):\n        """Execute action from buffer."""\n        with self.state_lock:\n            if self.action_buffer is None:\n                return\n\n            if self.action_index >= len(self.action_buffer):\n                return\n\n            action = self.action_buffer[self.action_index]\n            self.action_index += 1\n\n        # Publish action\n        msg = JointState()\n        msg.header.stamp = self.get_clock().now().to_msg()\n        msg.position = action.tolist()\n        self.action_pub.publish(msg)\n\n\ndef main():\n    rclpy.init()\n    node = VLAControlNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(e.h3,{id:"inference-optimization",children:"Inference Optimization"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",metastring:'title="optimized_inference.py"',children:'"""Optimize VLA model for real-time inference."""\nimport torch\nimport torch.nn as nn\nimport onnxruntime as ort\n\nclass OptimizedVLAInference:\n    """Optimized VLA inference for deployment."""\n\n    def __init__(\n        self,\n        model: VLAModel,\n        use_fp16: bool = True,\n        compile_model: bool = True\n    ):\n        self.device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n        # Move to device\n        model = model.to(self.device)\n        model.eval()\n\n        # Convert to FP16\n        if use_fp16 and self.device.type == \'cuda\':\n            model = model.half()\n\n        # Compile model (PyTorch 2.0+)\n        if compile_model:\n            model = torch.compile(model, mode=\'reduce-overhead\')\n\n        self.model = model\n\n    @torch.no_grad()\n    def predict(\n        self,\n        image: torch.Tensor,\n        instruction: str,\n        proprio: torch.Tensor\n    ) -> torch.Tensor:\n        """Optimized prediction."""\n        # Ensure correct dtype\n        if self.model.vision_proj.weight.dtype == torch.float16:\n            image = image.half()\n            proprio = proprio.half()\n\n        # Move to device\n        image = image.to(self.device)\n        proprio = proprio.to(self.device)\n\n        # Predict\n        return self.model.predict_action(image, instruction, proprio)\n\n    def benchmark(self, num_iterations: int = 100) -> dict:\n        """Benchmark inference speed."""\n        import time\n\n        # Dummy inputs\n        image = torch.randn(1, 3, 224, 224)\n        instruction = "pick up the red block"\n        proprio = torch.randn(1, 7)\n\n        # Warmup\n        for _ in range(10):\n            self.predict(image, instruction, proprio)\n\n        # Synchronize CUDA\n        if self.device.type == \'cuda\':\n            torch.cuda.synchronize()\n\n        # Benchmark\n        start = time.perf_counter()\n        for _ in range(num_iterations):\n            self.predict(image, instruction, proprio)\n\n        if self.device.type == \'cuda\':\n            torch.cuda.synchronize()\n\n        elapsed = time.perf_counter() - start\n        avg_time = elapsed / num_iterations * 1000  # ms\n\n        return {\n            "avg_latency_ms": avg_time,\n            "throughput_fps": 1000 / avg_time\n        }\n\n\ndef export_to_onnx(model: VLAModel, output_path: str):\n    """Export VLA model to ONNX."""\n    model.eval()\n\n    # Dummy inputs\n    image = torch.randn(1, 3, 224, 224)\n    proprio = torch.randn(1, 7)\n\n    # Export encoder only (action decoder uses diffusion)\n    class EncoderWrapper(nn.Module):\n        def __init__(self, model):\n            super().__init__()\n            self.model = model\n\n        def forward(self, image, proprio):\n            return self.model.encode_observation(image, "example", proprio)\n\n    wrapper = EncoderWrapper(model)\n\n    torch.onnx.export(\n        wrapper,\n        (image, proprio),\n        output_path,\n        export_params=True,\n        opset_version=14,\n        input_names=[\'image\', \'proprio\'],\n        output_names=[\'encoding\'],\n        dynamic_axes={\n            \'image\': {0: \'batch\'},\n            \'proprio\': {0: \'batch\'},\n            \'encoding\': {0: \'batch\'}\n        }\n    )\n\n    print(f"Exported encoder to {output_path}")\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"evaluation",children:"Evaluation"}),"\n",(0,o.jsx)(e.h3,{id:"evaluation-metrics",children:"Evaluation Metrics"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",metastring:'title="evaluation.py"',children:'"""Evaluation metrics for VLA systems."""\nimport numpy as np\nfrom typing import Dict, List\n\nclass VLAEvaluator:\n    """Evaluate VLA model performance."""\n\n    def __init__(self, env, model):\n        self.env = env\n        self.model = model\n\n    def evaluate_task(\n        self,\n        instruction: str,\n        num_trials: int = 10,\n        max_steps: int = 200\n    ) -> Dict[str, float]:\n        """Evaluate on specific task."""\n        successes = []\n        lengths = []\n        rewards = []\n\n        for trial in range(num_trials):\n            obs = self.env.reset()\n            total_reward = 0\n            success = False\n\n            for step in range(max_steps):\n                # Get action from model\n                action = self.model.predict_action(\n                    obs[\'image\'],\n                    instruction,\n                    obs[\'proprio\']\n                )\n\n                # Execute first action from trajectory\n                obs, reward, done, info = self.env.step(action[0])\n                total_reward += reward\n\n                if info.get(\'success\', False):\n                    success = True\n                    break\n\n                if done:\n                    break\n\n            successes.append(success)\n            lengths.append(step + 1)\n            rewards.append(total_reward)\n\n        return {\n            "success_rate": np.mean(successes),\n            "avg_length": np.mean(lengths),\n            "avg_reward": np.mean(rewards),\n            "std_reward": np.std(rewards)\n        }\n\n    def evaluate_suite(\n        self,\n        tasks: List[str],\n        num_trials_per_task: int = 10\n    ) -> Dict[str, Dict[str, float]]:\n        """Evaluate on suite of tasks."""\n        results = {}\n        for task in tasks:\n            print(f"Evaluating: {task}")\n            results[task] = self.evaluate_task(task, num_trials_per_task)\n        return results\n\n    def compute_generalization_gap(\n        self,\n        train_tasks: List[str],\n        test_tasks: List[str]\n    ) -> float:\n        """Compute gap between train and test performance."""\n        train_results = self.evaluate_suite(train_tasks)\n        test_results = self.evaluate_suite(test_tasks)\n\n        train_success = np.mean([r["success_rate"] for r in train_results.values()])\n        test_success = np.mean([r["success_rate"] for r in test_results.values()])\n\n        return train_success - test_success\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"exercise-1-build-complete-vla",children:"Exercise 1: Build Complete VLA"}),"\n",(0,o.jsxs)(e.admonition,{title:"Exercise 1: End-to-End VLA System",type:"tip",children:[(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Objective"}),": Implement a complete VLA system from scratch."]}),(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Steps"}),":"]}),(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Implement vision encoder with pre-trained ViT"}),"\n",(0,o.jsx)(e.li,{children:"Add language encoder with BERT"}),"\n",(0,o.jsx)(e.li,{children:"Build fusion transformer"}),"\n",(0,o.jsx)(e.li,{children:"Integrate diffusion action decoder"}),"\n",(0,o.jsx)(e.li,{children:"Train on simulated demonstrations"}),"\n"]}),(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Verification"}),":"]}),(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'model = VLAModel()\noutput = model(image, "pick up red cube", proprio)\nassert "actions" in output or "loss" in output\n'})}),(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Time Estimate"}),": 2-3 hours"]})]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"exercise-2-collect-demonstrations",children:"Exercise 2: Collect Demonstrations"}),"\n",(0,o.jsxs)(e.admonition,{title:"Exercise 2: Data Collection Pipeline",type:"tip",children:[(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Objective"}),": Build demonstration collection system."]}),(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Steps"}),":"]}),(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Set up teleoperation interface"}),"\n",(0,o.jsx)(e.li,{children:"Implement episode recording"}),"\n",(0,o.jsx)(e.li,{children:"Create HDF5 storage format"}),"\n",(0,o.jsx)(e.li,{children:"Collect 50 demonstrations for one task"}),"\n",(0,o.jsx)(e.li,{children:"Build PyTorch Dataset class"}),"\n"]}),(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Data Requirements"}),":"]}),(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"30+ frames per episode"}),"\n",(0,o.jsx)(e.li,{children:"Images, proprio, actions stored"}),"\n",(0,o.jsx)(e.li,{children:"Instruction annotated per episode"}),"\n"]}),(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Time Estimate"}),": 90 minutes"]})]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"exercise-3-deploy-and-evaluate",children:"Exercise 3: Deploy and Evaluate"}),"\n",(0,o.jsxs)(e.admonition,{title:"Exercise 3: Real-World Deployment",type:"tip",children:[(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Objective"}),": Deploy VLA model on robot hardware."]}),(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Steps"}),":"]}),(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Export model with optimizations"}),"\n",(0,o.jsx)(e.li,{children:"Create ROS 2 control node"}),"\n",(0,o.jsx)(e.li,{children:"Implement action execution loop"}),"\n",(0,o.jsx)(e.li,{children:"Test on 3 different tasks"}),"\n",(0,o.jsx)(e.li,{children:"Measure success rate and latency"}),"\n"]}),(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Targets"}),":"]}),(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Inference < 100ms"}),"\n",(0,o.jsx)(e.li,{children:"Success rate > 70% on trained tasks"}),"\n",(0,o.jsx)(e.li,{children:"Smooth action execution"}),"\n"]}),(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Time Estimate"}),": 2 hours"]})]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(e.p,{children:"In this chapter, you learned:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Architecture Patterns"}),": How RT-1, RT-2, OpenVLA, and Octo structure VLA models"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Complete Implementation"}),": End-to-end VLA with vision, language, and action components"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Training Pipelines"}),": Efficient training with demonstration data"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Data Collection"}),": Building demonstration datasets for imitation learning"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Fine-Tuning"}),": Adapting foundation models for robot tasks"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Deployment"}),": ROS 2 integration and inference optimization"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Evaluation"}),": Metrics and benchmarks for VLA performance"]}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:"VLA systems represent the frontier of robot learning - combining the semantic understanding of foundation models with the precision needed for physical manipulation. Success requires careful attention to each component, from data collection through deployment."}),"\n",(0,o.jsx)(e.p,{children:"This completes Module 4 on Vision-Language-Action models. You now have the foundation to build, train, and deploy modern VLA systems for robot control."}),"\n",(0,o.jsx)(e.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://arxiv.org/abs/2212.06817",children:"RT-1 Paper"})," - Robotics Transformer"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://arxiv.org/abs/2307.15818",children:"RT-2 Paper"})," - Vision-Language-Action Models"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://openvla.github.io/",children:"OpenVLA"})," - Open-source VLA"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://octo-models.github.io/",children:"Octo"})," - Generalist Robot Policy"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://robopen.github.io/",children:"RoboAgent"})," - Large-Scale Robot Learning"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://rail-berkeley.github.io/bridgedata/",children:"Bridge Data"})," - Robot Manipulation Dataset"]}),"\n"]})]})}function m(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(c,{...n})}):c(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>s,x:()=>a});var i=t(6540);const o={},r=i.createContext(o);function s(n){const e=i.useContext(r);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:s(n.components),i.createElement(r.Provider,{value:e},n.children)}}}]);