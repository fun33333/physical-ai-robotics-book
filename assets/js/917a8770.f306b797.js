"use strict";(globalThis.webpackChunkbook_website=globalThis.webpackChunkbook_website||[]).push([[4079],{4692:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>p,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"module-3/perception-pipelines","title":"Perception Pipelines","description":"Building end-to-end perception systems: sensor fusion, object tracking, and scene understanding.","source":"@site/docs/module-3/perception-pipelines.md","sourceDirName":"module-3","slug":"/module-3/perception-pipelines","permalink":"/physical-ai-robotics-book/docs/module-3/perception-pipelines","draft":false,"unlisted":false,"editUrl":"https://github.com/fun33333/physical-ai-robotics-book/tree/main/book-website/docs/module-3/perception-pipelines.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"title":"Perception Pipelines","sidebar_position":6,"description":"Building end-to-end perception systems: sensor fusion, object tracking, and scene understanding."},"sidebar":"tutorialSidebar","previous":{"title":"Isaac Lab (Reinforcement Learning)","permalink":"/physical-ai-robotics-book/docs/module-3/isaac-lab"},"next":{"title":"Module 4: Vision-Language-Action (VLA)","permalink":"/physical-ai-robotics-book/docs/module-4/"}}');var r=i(4848),s=i(8453);const a={title:"Perception Pipelines",sidebar_position:6,description:"Building end-to-end perception systems: sensor fusion, object tracking, and scene understanding."},o="Perception Pipelines",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Pipeline Architecture",id:"pipeline-architecture",level:2},{value:"Design Principles",id:"design-principles",level:3},{value:"Key Principles",id:"key-principles",level:3},{value:"Sensor Synchronization",id:"sensor-synchronization",level:2},{value:"Time Synchronization",id:"time-synchronization",level:3},{value:"TF Transform Management",id:"tf-transform-management",level:3},{value:"Multi-Sensor Fusion",id:"multi-sensor-fusion",level:2},{value:"Detection Fusion",id:"detection-fusion",level:3},{value:"Camera-LiDAR Projection",id:"camera-lidar-projection",level:3},{value:"Object Tracking",id:"object-tracking",level:2},{value:"Multi-Object Tracker",id:"multi-object-tracker",level:3},{value:"Scene Understanding",id:"scene-understanding",level:2},{value:"Scene Graph Representation",id:"scene-graph-representation",level:3},{value:"Complete Pipeline",id:"complete-pipeline",level:2},{value:"ROS 2 Pipeline Node",id:"ros-2-pipeline-node",level:3},{value:"Launch File",id:"launch-file",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Profiling Tools",id:"profiling-tools",level:3},{value:"Optimization Strategies",id:"optimization-strategies",level:3},{value:"Exercise 1: Build Detection Pipeline",id:"exercise-1-build-detection-pipeline",level:2},{value:"Exercise 2: Object Tracking",id:"exercise-2-object-tracking",level:2},{value:"Exercise 3: Scene Graph System",id:"exercise-3-scene-graph-system",level:2},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"perception-pipelines",children:"Perception Pipelines"})}),"\n",(0,r.jsx)(n.p,{children:"Individual perception components must be combined into coherent pipelines that provide robots with understanding of their environment. This chapter teaches you to design and implement complete perception systems that integrate detection, tracking, and scene understanding into real-time workflows."}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"In this section, you will:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Design modular perception pipeline architectures"}),"\n",(0,r.jsx)(n.li,{children:"Implement multi-sensor fusion techniques"}),"\n",(0,r.jsx)(n.li,{children:"Build object tracking systems across frames"}),"\n",(0,r.jsx)(n.li,{children:"Create scene graphs for semantic understanding"}),"\n",(0,r.jsx)(n.li,{children:"Optimize pipeline performance for real-time operation"}),"\n",(0,r.jsx)(n.li,{children:"Debug and visualize perception systems"}),"\n",(0,r.jsx)(n.li,{children:"Handle perception failures gracefully"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Completed ",(0,r.jsx)(n.a,{href:"/docs/module-3/isaac-ros",children:"Isaac ROS"})," chapter"]}),"\n",(0,r.jsx)(n.li,{children:"Understanding of ROS 2 topics, services, and actions"}),"\n",(0,r.jsx)(n.li,{children:"Familiarity with computer vision concepts"}),"\n",(0,r.jsx)(n.li,{children:"Basic linear algebra (transforms, matrices)"}),"\n",(0,r.jsx)(n.li,{children:"Python and C++ programming experience"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"pipeline-architecture",children:"Pipeline Architecture"}),"\n",(0,r.jsx)(n.h3,{id:"design-principles",children:"Design Principles"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Perception Pipeline Architecture                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                  \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502                   Sensor Layer                           \u2502   \u2502\n\u2502   \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502   \u2502\n\u2502   \u2502  \u2502RGB Cam \u2502  \u2502Depth   \u2502  \u2502 LiDAR  \u2502  \u2502  IMU   \u2502        \u2502   \u2502\n\u2502   \u2502  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518        \u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502           \u2502           \u2502           \u2502           \u2502                  \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502               Synchronization & Preprocessing            \u2502   \u2502\n\u2502   \u2502    Time sync \u2022 Undistort \u2022 Point cloud generation       \u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                               \u2502                                  \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502                   Perception Modules                     \u2502   \u2502\n\u2502   \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502   \u2502\n\u2502   \u2502  \u2502 Detection   \u2502 \u2502 Segmentation \u2502 \u2502   SLAM/Odom     \u2502  \u2502   \u2502\n\u2502   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502             \u2502               \u2502                  \u2502                 \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502                    Fusion Layer                          \u2502   \u2502\n\u2502   \u2502    Object tracking \u2022 Pose estimation \u2022 Map integration   \u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                               \u2502                                  \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502                   Scene Understanding                    \u2502   \u2502\n\u2502   \u2502    Scene graph \u2022 Semantic map \u2022 Spatial reasoning        \u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsx)(n.h3,{id:"key-principles",children:"Key Principles"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Principle"}),(0,r.jsx)(n.th,{children:"Description"}),(0,r.jsx)(n.th,{children:"Implementation"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Modularity"})}),(0,r.jsx)(n.td,{children:"Swap components without rewriting"}),(0,r.jsx)(n.td,{children:"ROS 2 interfaces"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Time Sync"})}),(0,r.jsx)(n.td,{children:"All sensors aligned temporally"}),(0,r.jsx)(n.td,{children:"message_filters"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Graceful Degradation"})}),(0,r.jsx)(n.td,{children:"Continue with partial data"}),(0,r.jsx)(n.td,{children:"Fallback paths"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Real-time"})}),(0,r.jsx)(n.td,{children:"Meet latency requirements"}),(0,r.jsx)(n.td,{children:"Profile and optimize"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Observability"})}),(0,r.jsx)(n.td,{children:"Monitor system health"}),(0,r.jsx)(n.td,{children:"Diagnostics, logging"})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"sensor-synchronization",children:"Sensor Synchronization"}),"\n",(0,r.jsx)(n.h3,{id:"time-synchronization",children:"Time Synchronization"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="sync_node.py"',children:"\"\"\"Multi-sensor synchronization using message_filters.\"\"\"\nimport rclpy\nfrom rclpy.node import Node\nfrom message_filters import Subscriber, ApproximateTimeSynchronizer\nfrom sensor_msgs.msg import Image, CameraInfo, PointCloud2\nfrom nav_msgs.msg import Odometry\n\nclass SensorSyncNode(Node):\n    def __init__(self):\n        super().__init__('sensor_sync')\n\n        # Create subscribers\n        self.rgb_sub = Subscriber(self, Image, '/camera/color/image_raw')\n        self.depth_sub = Subscriber(self, Image, '/camera/depth/image_raw')\n        self.info_sub = Subscriber(self, CameraInfo, '/camera/color/camera_info')\n        self.lidar_sub = Subscriber(self, PointCloud2, '/lidar/points')\n\n        # Time synchronizer with 50ms tolerance\n        self.sync = ApproximateTimeSynchronizer(\n            [self.rgb_sub, self.depth_sub, self.info_sub, self.lidar_sub],\n            queue_size=10,\n            slop=0.05  # 50ms tolerance\n        )\n        self.sync.registerCallback(self.synced_callback)\n\n        # Publisher for synchronized bundle\n        self.synced_pub = self.create_publisher(\n            SensorBundle, '/perception/sensor_bundle', 10\n        )\n\n        self.get_logger().info('Sensor sync ready')\n\n    def synced_callback(self, rgb_msg, depth_msg, info_msg, lidar_msg):\n        \"\"\"Process synchronized sensor data.\"\"\"\n        # Create synchronized bundle\n        bundle = SensorBundle()\n        bundle.header.stamp = self.get_clock().now().to_msg()\n        bundle.rgb = rgb_msg\n        bundle.depth = depth_msg\n        bundle.camera_info = info_msg\n        bundle.point_cloud = lidar_msg\n\n        self.synced_pub.publish(bundle)\n\n\ndef main():\n    rclpy.init()\n    node = SensorSyncNode()\n    rclpy.spin(node)\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h3,{id:"tf-transform-management",children:"TF Transform Management"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="transform_manager.py"',children:'"""Transform management for perception pipeline."""\nimport rclpy\nfrom rclpy.node import Node\nfrom tf2_ros import Buffer, TransformListener, TransformBroadcaster\nfrom geometry_msgs.msg import TransformStamped\nimport numpy as np\n\nclass TransformManager(Node):\n    def __init__(self):\n        super().__init__(\'transform_manager\')\n\n        # TF2 setup\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n        self.tf_broadcaster = TransformBroadcaster(self)\n\n        # Frame configuration\n        self.declare_parameter(\'base_frame\', \'base_link\')\n        self.declare_parameter(\'camera_frame\', \'camera_link\')\n        self.declare_parameter(\'lidar_frame\', \'lidar_link\')\n\n    def get_transform(self, target_frame: str, source_frame: str):\n        """Get transform between frames."""\n        try:\n            transform = self.tf_buffer.lookup_transform(\n                target_frame,\n                source_frame,\n                rclpy.time.Time(),\n                timeout=rclpy.duration.Duration(seconds=1.0)\n            )\n            return transform\n        except Exception as e:\n            self.get_logger().warn(f\'Transform lookup failed: {e}\')\n            return None\n\n    def transform_point_cloud(self, cloud, target_frame):\n        """Transform point cloud to target frame."""\n        transform = self.get_transform(target_frame, cloud.header.frame_id)\n        if transform is None:\n            return None\n\n        # Apply transformation using tf2_sensor_msgs\n        from tf2_sensor_msgs import do_transform_cloud\n        return do_transform_cloud(cloud, transform)\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"multi-sensor-fusion",children:"Multi-Sensor Fusion"}),"\n",(0,r.jsx)(n.h3,{id:"detection-fusion",children:"Detection Fusion"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="detection_fusion.py"',children:'"""Fuse detections from multiple sensors."""\nimport numpy as np\nfrom dataclasses import dataclass\nfrom typing import List\nfrom scipy.optimize import linear_sum_assignment\n\n@dataclass\nclass Detection:\n    """Unified detection format."""\n    bbox_2d: np.ndarray  # [x, y, w, h]\n    bbox_3d: np.ndarray  # [x, y, z, l, w, h, yaw]\n    confidence: float\n    class_id: int\n    source: str  # \'camera\', \'lidar\', \'radar\'\n\nclass DetectionFuser:\n    """Fuse detections from camera and LiDAR."""\n\n    def __init__(self, iou_threshold: float = 0.3):\n        self.iou_threshold = iou_threshold\n\n    def fuse(self, camera_dets: List[Detection],\n             lidar_dets: List[Detection]) -> List[Detection]:\n        """Fuse camera and LiDAR detections."""\n        if not camera_dets:\n            return lidar_dets\n        if not lidar_dets:\n            return camera_dets\n\n        # Build cost matrix based on 3D IoU\n        cost_matrix = np.zeros((len(camera_dets), len(lidar_dets)))\n        for i, cam_det in enumerate(camera_dets):\n            for j, lid_det in enumerate(lidar_dets):\n                iou = self._compute_3d_iou(cam_det.bbox_3d, lid_det.bbox_3d)\n                cost_matrix[i, j] = 1.0 - iou\n\n        # Hungarian algorithm for optimal assignment\n        row_indices, col_indices = linear_sum_assignment(cost_matrix)\n\n        fused_detections = []\n        matched_camera = set()\n        matched_lidar = set()\n\n        for row, col in zip(row_indices, col_indices):\n            if cost_matrix[row, col] < (1.0 - self.iou_threshold):\n                # Fuse matched detections\n                fused = self._merge_detections(\n                    camera_dets[row], lidar_dets[col]\n                )\n                fused_detections.append(fused)\n                matched_camera.add(row)\n                matched_lidar.add(col)\n\n        # Add unmatched detections\n        for i, det in enumerate(camera_dets):\n            if i not in matched_camera:\n                fused_detections.append(det)\n\n        for j, det in enumerate(lidar_dets):\n            if j not in matched_lidar:\n                fused_detections.append(det)\n\n        return fused_detections\n\n    def _compute_3d_iou(self, box1: np.ndarray, box2: np.ndarray) -> float:\n        """Compute 3D IoU between two bounding boxes."""\n        # Simplified axis-aligned IoU\n        x1_min, x1_max = box1[0] - box1[3]/2, box1[0] + box1[3]/2\n        y1_min, y1_max = box1[1] - box1[4]/2, box1[1] + box1[4]/2\n        z1_min, z1_max = box1[2] - box1[5]/2, box1[2] + box1[5]/2\n\n        x2_min, x2_max = box2[0] - box2[3]/2, box2[0] + box2[3]/2\n        y2_min, y2_max = box2[1] - box2[4]/2, box2[1] + box2[4]/2\n        z2_min, z2_max = box2[2] - box2[5]/2, box2[2] + box2[5]/2\n\n        # Intersection\n        xi_min = max(x1_min, x2_min)\n        yi_min = max(y1_min, y2_min)\n        zi_min = max(z1_min, z2_min)\n        xi_max = min(x1_max, x2_max)\n        yi_max = min(y1_max, y2_max)\n        zi_max = min(z1_max, z2_max)\n\n        if xi_min >= xi_max or yi_min >= yi_max or zi_min >= zi_max:\n            return 0.0\n\n        inter_vol = (xi_max - xi_min) * (yi_max - yi_min) * (zi_max - zi_min)\n        vol1 = box1[3] * box1[4] * box1[5]\n        vol2 = box2[3] * box2[4] * box2[5]\n\n        return inter_vol / (vol1 + vol2 - inter_vol)\n\n    def _merge_detections(self, cam_det: Detection,\n                          lid_det: Detection) -> Detection:\n        """Merge camera and LiDAR detections."""\n        # Use LiDAR for 3D position (more accurate)\n        # Use camera for classification (better features)\n        return Detection(\n            bbox_2d=cam_det.bbox_2d,\n            bbox_3d=lid_det.bbox_3d,\n            confidence=(cam_det.confidence + lid_det.confidence) / 2,\n            class_id=cam_det.class_id,\n            source=\'fused\'\n        )\n'})}),"\n",(0,r.jsx)(n.h3,{id:"camera-lidar-projection",children:"Camera-LiDAR Projection"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="projection.py"',children:'"""Project between camera and LiDAR coordinate frames."""\nimport numpy as np\n\nclass CameraLidarProjector:\n    """Project points between camera and LiDAR frames."""\n\n    def __init__(self, camera_matrix: np.ndarray,\n                 extrinsic: np.ndarray):\n        """\n        Args:\n            camera_matrix: 3x3 intrinsic camera matrix\n            extrinsic: 4x4 transform from LiDAR to camera frame\n        """\n        self.K = camera_matrix\n        self.T_cam_lidar = extrinsic\n\n    def lidar_to_image(self, points_lidar: np.ndarray) -> np.ndarray:\n        """Project LiDAR points onto image plane."""\n        # Add homogeneous coordinate\n        n_points = points_lidar.shape[0]\n        points_h = np.hstack([points_lidar, np.ones((n_points, 1))])\n\n        # Transform to camera frame\n        points_cam = (self.T_cam_lidar @ points_h.T).T[:, :3]\n\n        # Filter points behind camera\n        valid = points_cam[:, 2] > 0\n        points_cam = points_cam[valid]\n\n        # Project to image plane\n        points_2d = (self.K @ points_cam.T).T\n        points_2d = points_2d[:, :2] / points_2d[:, 2:3]\n\n        return points_2d, valid\n\n    def image_to_ray(self, pixel: np.ndarray) -> np.ndarray:\n        """Get ray direction from pixel coordinate."""\n        # Unproject pixel to normalized camera coords\n        K_inv = np.linalg.inv(self.K)\n        pixel_h = np.array([pixel[0], pixel[1], 1.0])\n        ray_cam = K_inv @ pixel_h\n\n        # Transform to LiDAR frame\n        T_lidar_cam = np.linalg.inv(self.T_cam_lidar)\n        ray_lidar = T_lidar_cam[:3, :3] @ ray_cam\n\n        return ray_lidar / np.linalg.norm(ray_lidar)\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"object-tracking",children:"Object Tracking"}),"\n",(0,r.jsx)(n.h3,{id:"multi-object-tracker",children:"Multi-Object Tracker"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="tracker.py"',children:'"""Multi-object tracking with Kalman filter."""\nimport numpy as np\nfrom filterpy.kalman import KalmanFilter\nfrom scipy.optimize import linear_sum_assignment\nfrom dataclasses import dataclass, field\nfrom typing import List, Optional\nimport uuid\n\n@dataclass\nclass Track:\n    """Tracked object state."""\n    id: str = field(default_factory=lambda: str(uuid.uuid4())[:8])\n    state: np.ndarray = None  # [x, y, z, vx, vy, vz]\n    covariance: np.ndarray = None\n    class_id: int = -1\n    confidence: float = 0.0\n    age: int = 0\n    hits: int = 0\n    misses: int = 0\n    kf: KalmanFilter = None\n\n\nclass MultiObjectTracker:\n    """Track multiple objects across frames."""\n\n    def __init__(self,\n                 max_age: int = 5,\n                 min_hits: int = 3,\n                 iou_threshold: float = 0.3):\n        self.max_age = max_age\n        self.min_hits = min_hits\n        self.iou_threshold = iou_threshold\n        self.tracks: List[Track] = []\n\n    def update(self, detections: List[Detection]) -> List[Track]:\n        """Update tracks with new detections."""\n        # Predict existing tracks\n        for track in self.tracks:\n            track.kf.predict()\n            track.state = track.kf.x.flatten()\n            track.covariance = track.kf.P\n            track.age += 1\n\n        # Match detections to tracks\n        if len(self.tracks) > 0 and len(detections) > 0:\n            matched, unmatched_dets, unmatched_trks = self._associate(\n                detections, self.tracks\n            )\n        else:\n            matched = []\n            unmatched_dets = list(range(len(detections)))\n            unmatched_trks = list(range(len(self.tracks)))\n\n        # Update matched tracks\n        for det_idx, trk_idx in matched:\n            track = self.tracks[trk_idx]\n            det = detections[det_idx]\n\n            # Kalman update\n            measurement = det.bbox_3d[:3]  # x, y, z\n            track.kf.update(measurement)\n            track.state = track.kf.x.flatten()\n            track.covariance = track.kf.P\n            track.hits += 1\n            track.misses = 0\n            track.confidence = det.confidence\n            track.class_id = det.class_id\n\n        # Create new tracks for unmatched detections\n        for det_idx in unmatched_dets:\n            det = detections[det_idx]\n            track = self._create_track(det)\n            self.tracks.append(track)\n\n        # Mark missed tracks\n        for trk_idx in unmatched_trks:\n            self.tracks[trk_idx].misses += 1\n\n        # Remove dead tracks\n        self.tracks = [\n            t for t in self.tracks\n            if t.misses <= self.max_age\n        ]\n\n        # Return confirmed tracks\n        return [\n            t for t in self.tracks\n            if t.hits >= self.min_hits\n        ]\n\n    def _create_track(self, det: Detection) -> Track:\n        """Initialize new track from detection."""\n        # 6D Kalman filter: [x, y, z, vx, vy, vz]\n        kf = KalmanFilter(dim_x=6, dim_z=3)\n\n        # State transition matrix (constant velocity)\n        dt = 0.1  # Assume 10 Hz\n        kf.F = np.array([\n            [1, 0, 0, dt, 0, 0],\n            [0, 1, 0, 0, dt, 0],\n            [0, 0, 1, 0, 0, dt],\n            [0, 0, 0, 1, 0, 0],\n            [0, 0, 0, 0, 1, 0],\n            [0, 0, 0, 0, 0, 1],\n        ])\n\n        # Measurement matrix\n        kf.H = np.array([\n            [1, 0, 0, 0, 0, 0],\n            [0, 1, 0, 0, 0, 0],\n            [0, 0, 1, 0, 0, 0],\n        ])\n\n        # Initial state from detection\n        kf.x = np.array([\n            det.bbox_3d[0], det.bbox_3d[1], det.bbox_3d[2],\n            0, 0, 0\n        ]).reshape(-1, 1)\n\n        # Covariance matrices\n        kf.P *= 10\n        kf.R = np.diag([0.1, 0.1, 0.1])\n        kf.Q = np.diag([0.01, 0.01, 0.01, 0.1, 0.1, 0.1])\n\n        return Track(\n            state=kf.x.flatten(),\n            covariance=kf.P,\n            class_id=det.class_id,\n            confidence=det.confidence,\n            hits=1,\n            kf=kf\n        )\n\n    def _associate(self, detections, tracks):\n        """Associate detections with existing tracks."""\n        cost_matrix = np.zeros((len(detections), len(tracks)))\n\n        for d, det in enumerate(detections):\n            for t, track in enumerate(tracks):\n                # Use Mahalanobis distance\n                innovation = det.bbox_3d[:3] - track.state[:3]\n                S = track.covariance[:3, :3] + np.eye(3) * 0.1\n                dist = np.sqrt(innovation @ np.linalg.inv(S) @ innovation)\n                cost_matrix[d, t] = dist\n\n        # Hungarian algorithm\n        det_indices, trk_indices = linear_sum_assignment(cost_matrix)\n\n        matched = []\n        unmatched_dets = list(range(len(detections)))\n        unmatched_trks = list(range(len(tracks)))\n\n        for d, t in zip(det_indices, trk_indices):\n            if cost_matrix[d, t] < 10.0:  # Gating threshold\n                matched.append((d, t))\n                unmatched_dets.remove(d)\n                unmatched_trks.remove(t)\n\n        return matched, unmatched_dets, unmatched_trks\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"scene-understanding",children:"Scene Understanding"}),"\n",(0,r.jsx)(n.h3,{id:"scene-graph-representation",children:"Scene Graph Representation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="scene_graph.py"',children:'"""Scene graph for semantic understanding."""\nfrom dataclasses import dataclass, field\nfrom typing import List, Dict, Optional\nimport numpy as np\n\n@dataclass\nclass SceneNode:\n    """Node in scene graph."""\n    id: str\n    class_name: str\n    position: np.ndarray\n    orientation: np.ndarray\n    dimensions: np.ndarray\n    confidence: float\n    attributes: Dict[str, any] = field(default_factory=dict)\n    parent_id: Optional[str] = None\n\n\n@dataclass\nclass SceneRelation:\n    """Spatial relation between nodes."""\n    subject_id: str\n    object_id: str\n    relation_type: str  # \'on\', \'near\', \'inside\', \'left_of\', etc.\n    confidence: float\n\n\nclass SceneGraph:\n    """Scene graph for semantic scene understanding."""\n\n    def __init__(self):\n        self.nodes: Dict[str, SceneNode] = {}\n        self.relations: List[SceneRelation] = []\n\n    def add_node(self, node: SceneNode):\n        """Add node to scene graph."""\n        self.nodes[node.id] = node\n\n    def remove_node(self, node_id: str):\n        """Remove node and its relations."""\n        if node_id in self.nodes:\n            del self.nodes[node_id]\n            self.relations = [\n                r for r in self.relations\n                if r.subject_id != node_id and r.object_id != node_id\n            ]\n\n    def add_relation(self, relation: SceneRelation):\n        """Add spatial relation."""\n        self.relations.append(relation)\n\n    def compute_relations(self):\n        """Compute spatial relations between nodes."""\n        self.relations = []\n        nodes = list(self.nodes.values())\n\n        for i, node1 in enumerate(nodes):\n            for node2 in nodes[i+1:]:\n                relations = self._compute_pairwise_relations(node1, node2)\n                self.relations.extend(relations)\n\n    def _compute_pairwise_relations(self, node1: SceneNode,\n                                     node2: SceneNode) -> List[SceneRelation]:\n        """Compute relations between two nodes."""\n        relations = []\n        distance = np.linalg.norm(node1.position - node2.position)\n\n        # Near relation\n        if distance < 1.0:  # Within 1 meter\n            relations.append(SceneRelation(\n                subject_id=node1.id,\n                object_id=node2.id,\n                relation_type=\'near\',\n                confidence=1.0 - distance\n            ))\n\n        # On relation (vertical alignment)\n        if abs(node1.position[0] - node2.position[0]) < 0.3 and \\\n           abs(node1.position[1] - node2.position[1]) < 0.3:\n            if node1.position[2] > node2.position[2]:\n                relations.append(SceneRelation(\n                    subject_id=node1.id,\n                    object_id=node2.id,\n                    relation_type=\'on\',\n                    confidence=0.9\n                ))\n\n        # Left/Right relations\n        relative = node2.position - node1.position\n        if abs(relative[1]) > 0.3:\n            relation_type = \'right_of\' if relative[1] > 0 else \'left_of\'\n            relations.append(SceneRelation(\n                subject_id=node2.id,\n                object_id=node1.id,\n                relation_type=relation_type,\n                confidence=min(abs(relative[1]), 1.0)\n            ))\n\n        return relations\n\n    def query(self, query: str) -> List[SceneNode]:\n        """Query scene graph (simplified)."""\n        # Example: "cup on table"\n        tokens = query.lower().split()\n\n        results = []\n        for node in self.nodes.values():\n            if node.class_name.lower() in tokens:\n                results.append(node)\n\n        return results\n\n    def to_dict(self) -> dict:\n        """Export scene graph as dictionary."""\n        return {\n            \'nodes\': [\n                {\n                    \'id\': n.id,\n                    \'class\': n.class_name,\n                    \'position\': n.position.tolist(),\n                    \'confidence\': n.confidence\n                }\n                for n in self.nodes.values()\n            ],\n            \'relations\': [\n                {\n                    \'subject\': r.subject_id,\n                    \'object\': r.object_id,\n                    \'relation\': r.relation_type,\n                    \'confidence\': r.confidence\n                }\n                for r in self.relations\n            ]\n        }\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"complete-pipeline",children:"Complete Pipeline"}),"\n",(0,r.jsx)(n.h3,{id:"ros-2-pipeline-node",children:"ROS 2 Pipeline Node"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="perception_pipeline.py"',children:'"""Complete perception pipeline ROS 2 node."""\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.qos import QoSProfile, ReliabilityPolicy\nfrom sensor_msgs.msg import Image, PointCloud2\nfrom vision_msgs.msg import Detection3DArray, Detection3D\nfrom visualization_msgs.msg import MarkerArray, Marker\nfrom geometry_msgs.msg import Point\n\nimport numpy as np\nfrom cv_bridge import CvBridge\n\nclass PerceptionPipeline(Node):\n    """Complete perception pipeline node."""\n\n    def __init__(self):\n        super().__init__(\'perception_pipeline\')\n\n        # Components\n        self.bridge = CvBridge()\n        self.tracker = MultiObjectTracker()\n        self.scene_graph = SceneGraph()\n        self.fuser = DetectionFuser()\n\n        # Parameters\n        self.declare_parameter(\'detection_threshold\', 0.5)\n        self.declare_parameter(\'tracking_enabled\', True)\n\n        # Subscribers\n        qos = QoSProfile(\n            depth=10,\n            reliability=ReliabilityPolicy.BEST_EFFORT\n        )\n\n        self.camera_det_sub = self.create_subscription(\n            Detection3DArray, \'/camera/detections\',\n            self.camera_detection_callback, qos\n        )\n        self.lidar_det_sub = self.create_subscription(\n            Detection3DArray, \'/lidar/detections\',\n            self.lidar_detection_callback, qos\n        )\n\n        # Publishers\n        self.tracks_pub = self.create_publisher(\n            Detection3DArray, \'/perception/tracks\', 10\n        )\n        self.scene_pub = self.create_publisher(\n            MarkerArray, \'/perception/scene_graph\', 10\n        )\n\n        # Buffers\n        self.camera_detections = []\n        self.lidar_detections = []\n\n        # Timer for pipeline execution\n        self.timer = self.create_timer(0.1, self.run_pipeline)\n\n        self.get_logger().info(\'Perception pipeline initialized\')\n\n    def camera_detection_callback(self, msg):\n        """Buffer camera detections."""\n        self.camera_detections = self._convert_detections(msg)\n\n    def lidar_detection_callback(self, msg):\n        """Buffer LiDAR detections."""\n        self.lidar_detections = self._convert_detections(msg)\n\n    def _convert_detections(self, msg) -> List[Detection]:\n        """Convert ROS message to Detection objects."""\n        detections = []\n        for det in msg.detections:\n            bbox_3d = np.array([\n                det.bbox.center.position.x,\n                det.bbox.center.position.y,\n                det.bbox.center.position.z,\n                det.bbox.size.x,\n                det.bbox.size.y,\n                det.bbox.size.z,\n                0.0  # yaw\n            ])\n            detections.append(Detection(\n                bbox_2d=np.zeros(4),\n                bbox_3d=bbox_3d,\n                confidence=det.results[0].hypothesis.score if det.results else 0.5,\n                class_id=int(det.results[0].hypothesis.class_id) if det.results else -1,\n                source=\'ros\'\n            ))\n        return detections\n\n    def run_pipeline(self):\n        """Execute perception pipeline."""\n        # 1. Fuse detections\n        fused = self.fuser.fuse(\n            self.camera_detections,\n            self.lidar_detections\n        )\n\n        # 2. Track objects\n        if self.get_parameter(\'tracking_enabled\').value:\n            tracks = self.tracker.update(fused)\n        else:\n            tracks = fused\n\n        # 3. Update scene graph\n        self.update_scene_graph(tracks)\n\n        # 4. Publish results\n        self.publish_tracks(tracks)\n        self.publish_scene_visualization()\n\n        # Clear buffers\n        self.camera_detections = []\n        self.lidar_detections = []\n\n    def update_scene_graph(self, tracks):\n        """Update scene graph with tracked objects."""\n        # Remove stale nodes\n        current_ids = {t.id for t in tracks}\n        stale_ids = [\n            nid for nid in self.scene_graph.nodes\n            if nid not in current_ids\n        ]\n        for nid in stale_ids:\n            self.scene_graph.remove_node(nid)\n\n        # Update/add nodes\n        for track in tracks:\n            node = SceneNode(\n                id=track.id,\n                class_name=self._get_class_name(track.class_id),\n                position=track.state[:3],\n                orientation=np.array([0, 0, 0, 1]),\n                dimensions=np.array([0.5, 0.5, 0.5]),\n                confidence=track.confidence\n            )\n            self.scene_graph.add_node(node)\n\n        # Recompute relations\n        self.scene_graph.compute_relations()\n\n    def _get_class_name(self, class_id: int) -> str:\n        """Map class ID to name."""\n        class_names = {\n            0: \'person\', 1: \'car\', 2: \'bicycle\',\n            3: \'chair\', 4: \'table\', 5: \'cup\'\n        }\n        return class_names.get(class_id, \'unknown\')\n\n    def publish_tracks(self, tracks):\n        """Publish tracked objects."""\n        msg = Detection3DArray()\n        msg.header.stamp = self.get_clock().now().to_msg()\n        msg.header.frame_id = \'base_link\'\n\n        for track in tracks:\n            det = Detection3D()\n            det.bbox.center.position.x = float(track.state[0])\n            det.bbox.center.position.y = float(track.state[1])\n            det.bbox.center.position.z = float(track.state[2])\n            det.tracking_id = track.id\n            msg.detections.append(det)\n\n        self.tracks_pub.publish(msg)\n\n    def publish_scene_visualization(self):\n        """Publish scene graph visualization."""\n        markers = MarkerArray()\n\n        # Node markers\n        for i, node in enumerate(self.scene_graph.nodes.values()):\n            marker = Marker()\n            marker.header.frame_id = \'base_link\'\n            marker.id = i\n            marker.type = Marker.CUBE\n            marker.action = Marker.ADD\n            marker.pose.position.x = float(node.position[0])\n            marker.pose.position.y = float(node.position[1])\n            marker.pose.position.z = float(node.position[2])\n            marker.scale.x = 0.3\n            marker.scale.y = 0.3\n            marker.scale.z = 0.3\n            marker.color.a = 0.8\n            marker.color.r = 0.0\n            marker.color.g = 1.0\n            marker.color.b = 0.0\n            markers.markers.append(marker)\n\n        self.scene_pub.publish(markers)\n\n\ndef main():\n    rclpy.init()\n    node = PerceptionPipeline()\n    rclpy.spin(node)\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"launch-file",children:"Launch File"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="perception_pipeline.launch.py"',children:"\"\"\"Launch perception pipeline.\"\"\"\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node, ComposableNodeContainer\nfrom launch_ros.descriptions import ComposableNode\n\ndef generate_launch_description():\n    # Isaac ROS detection\n    detection_node = ComposableNode(\n        package='isaac_ros_detectnet',\n        plugin='nvidia::isaac_ros::detectnet::DetectNetDecoderNode',\n        name='detectnet',\n        parameters=[{\n            'model_file_path': '/models/peoplenet.plan',\n        }]\n    )\n\n    # Isaac ROS visual SLAM\n    slam_node = ComposableNode(\n        package='isaac_ros_visual_slam',\n        plugin='nvidia::isaac_ros::visual_slam::VisualSlamNode',\n        name='visual_slam',\n    )\n\n    # GPU container\n    container = ComposableNodeContainer(\n        name='perception_container',\n        namespace='',\n        package='rclcpp_components',\n        executable='component_container_mt',\n        composable_node_descriptions=[detection_node, slam_node],\n        output='screen'\n    )\n\n    # Perception pipeline\n    pipeline = Node(\n        package='perception_pipeline',\n        executable='perception_pipeline',\n        name='perception_pipeline',\n        parameters=[{\n            'detection_threshold': 0.5,\n            'tracking_enabled': True,\n        }],\n        output='screen'\n    )\n\n    return LaunchDescription([container, pipeline])\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,r.jsx)(n.h3,{id:"profiling-tools",children:"Profiling Tools"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",metastring:'title="Profile perception pipeline"',children:"# ROS 2 topic bandwidth\nros2 topic bw /perception/tracks\n\n# ROS 2 topic frequency\nros2 topic hz /perception/tracks\n\n# CPU profiling\nperf record -g ros2 run perception_pipeline perception_pipeline\nperf report\n\n# GPU profiling\nnsys profile ros2 launch perception_pipeline perception.launch.py\n"})}),"\n",(0,r.jsx)(n.h3,{id:"optimization-strategies",children:"Optimization Strategies"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Bottleneck"}),(0,r.jsx)(n.th,{children:"Solution"}),(0,r.jsx)(n.th,{children:"Impact"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Image resize"})}),(0,r.jsx)(n.td,{children:"GPU-accelerated resize"}),(0,r.jsx)(n.td,{children:"5-10x faster"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Detection"})}),(0,r.jsx)(n.td,{children:"TensorRT optimization"}),(0,r.jsx)(n.td,{children:"10x faster"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Point cloud"})}),(0,r.jsx)(n.td,{children:"Voxel downsampling"}),(0,r.jsx)(n.td,{children:"Reduce data 10x"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Tracking"})}),(0,r.jsx)(n.td,{children:"Parallel association"}),(0,r.jsx)(n.td,{children:"2-3x faster"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Memory"})}),(0,r.jsx)(n.td,{children:"Pre-allocated buffers"}),(0,r.jsx)(n.td,{children:"Reduce GC"})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"exercise-1-build-detection-pipeline",children:"Exercise 1: Build Detection Pipeline"}),"\n",(0,r.jsxs)(n.admonition,{title:"Exercise 1: Multi-Sensor Detection",type:"tip",children:[(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Objective"}),": Create a detection pipeline fusing camera and LiDAR."]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Steps"}),":"]}),(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Set up synchronized sensor inputs"}),"\n",(0,r.jsx)(n.li,{children:"Run Isaac ROS DetectNet on camera images"}),"\n",(0,r.jsx)(n.li,{children:"Run PointPillars on LiDAR point clouds"}),"\n",(0,r.jsx)(n.li,{children:"Implement detection fusion"}),"\n",(0,r.jsx)(n.li,{children:"Visualize fused detections in RViz"}),"\n"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected Output"}),":"]}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Fused detections at 20+ Hz"}),"\n",(0,r.jsx)(n.li,{children:"3D bounding boxes with class labels"}),"\n",(0,r.jsx)(n.li,{children:"Latency < 100ms"}),"\n"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Time Estimate"}),": 90 minutes"]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"exercise-2-object-tracking",children:"Exercise 2: Object Tracking"}),"\n",(0,r.jsxs)(n.admonition,{title:"Exercise 2: Multi-Object Tracker",type:"tip",children:[(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Objective"}),": Implement persistent tracking across frames."]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Steps"}),":"]}),(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Implement Kalman filter tracker"}),"\n",(0,r.jsx)(n.li,{children:"Configure track management parameters"}),"\n",(0,r.jsx)(n.li,{children:"Test with moving objects"}),"\n",(0,r.jsx)(n.li,{children:"Evaluate tracking metrics (MOTA, IDF1)"}),"\n",(0,r.jsx)(n.li,{children:"Handle occlusions and reappearances"}),"\n"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Metrics"}),":"]}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Track fragmentation < 10%"}),"\n",(0,r.jsx)(n.li,{children:"ID switches < 5%"}),"\n",(0,r.jsx)(n.li,{children:"Tracking accuracy > 80%"}),"\n"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Time Estimate"}),": 60 minutes"]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"exercise-3-scene-graph-system",children:"Exercise 3: Scene Graph System"}),"\n",(0,r.jsxs)(n.admonition,{title:"Exercise 3: Semantic Scene Understanding",type:"tip",children:[(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Objective"}),": Build a scene graph from perception outputs."]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Steps"}),":"]}),(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Create scene graph data structures"}),"\n",(0,r.jsx)(n.li,{children:"Populate from tracked objects"}),"\n",(0,r.jsx)(n.li,{children:"Compute spatial relations"}),"\n",(0,r.jsx)(n.li,{children:"Implement simple queries"}),"\n",(0,r.jsx)(n.li,{children:"Visualize in RViz"}),"\n"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Example Queries"}),":"]}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'"Find all cups on tables"'}),"\n",(0,r.jsx)(n.li,{children:'"Objects near the robot"'}),"\n",(0,r.jsx)(n.li,{children:'"People in front of the robot"'}),"\n"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Time Estimate"}),": 75 minutes"]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"In this chapter, you learned:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Architecture"}),": Modular pipeline design principles"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Synchronization"}),": Multi-sensor time alignment"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fusion"}),": Combining camera and LiDAR detections"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Tracking"}),": Kalman filter-based multi-object tracking"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Scene Graphs"}),": Semantic scene representation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Optimization"}),": Profiling and performance tuning"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"A well-designed perception pipeline transforms raw sensor data into actionable understanding that robots can use for planning and control. The key is balancing accuracy, latency, and robustness."}),"\n",(0,r.jsxs)(n.p,{children:["This completes Module 3 on NVIDIA Isaac. Next, explore ",(0,r.jsx)(n.a,{href:"/docs/module-4",children:"Module 4: Vision-Language-Action Models"})," to learn how foundation models enable natural language robot control."]}),"\n",(0,r.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/NVIDIA-ISAAC-ROS",children:"Isaac ROS Perception"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://navigation.ros.org/perception/index.html",children:"ROS 2 Perception Pipeline"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2006.16567",children:"Multi-Object Tracking Survey"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://cs.stanford.edu/~danfei/scene-graph.html",children:"Scene Graph Generation"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://filterpy.readthedocs.io/",children:"FilterPy Kalman Filter"})}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var t=i(6540);const r={},s=t.createContext(r);function a(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);