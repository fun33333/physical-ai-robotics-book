"use strict";(globalThis.webpackChunkbook_website=globalThis.webpackChunkbook_website||[]).push([[3267],{454:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4/index","title":"Module 4: Vision-Language-Action (VLA)","description":"Build end-to-end systems that connect vision, language understanding, and robot action generation.","source":"@site/docs/module-4/index.md","sourceDirName":"module-4","slug":"/module-4/","permalink":"/physical-ai-robotics-book/docs/module-4/","draft":false,"unlisted":false,"editUrl":"https://github.com/fun33333/physical-ai-robotics-book/tree/main/book-website/docs/module-4/index.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Module 4: Vision-Language-Action (VLA)","sidebar_position":1,"description":"Build end-to-end systems that connect vision, language understanding, and robot action generation."},"sidebar":"tutorialSidebar","previous":{"title":"Perception Pipelines","permalink":"/physical-ai-robotics-book/docs/module-3/perception-pipelines"},"next":{"title":"VLA Foundations","permalink":"/physical-ai-robotics-book/docs/module-4/vla-foundations"}}');var s=i(4848),r=i(8453);const o={title:"Module 4: Vision-Language-Action (VLA)",sidebar_position:1,description:"Build end-to-end systems that connect vision, language understanding, and robot action generation."},l="Module 4: Vision-Language-Action (VLA)",d={},c=[{value:"Module Overview",id:"module-overview",level:2},{value:"What You&#39;ll Learn",id:"what-youll-learn",level:3},{value:"Prerequisites",id:"prerequisites",level:3},{value:"The VLA Revolution",id:"the-vla-revolution",level:2},{value:"From Separate Models to Unified Systems",id:"from-separate-models-to-unified-systems",level:3},{value:"Why VLA Matters",id:"why-vla-matters",level:3},{value:"The VLA Architecture",id:"the-vla-architecture",level:3},{value:"Learning Path",id:"learning-path",level:2},{value:"Week 1: Foundations",id:"week-1-foundations",level:3},{value:"Week 2: Components",id:"week-2-components",level:3},{value:"Module Project: VLA Demo System",id:"module-project-vla-demo-system",level:2},{value:"Project Overview",id:"project-overview",level:3},{value:"Deliverables",id:"deliverables",level:3},{value:"Key Concepts Preview",id:"key-concepts-preview",level:2},{value:"Concepts You&#39;ll Master",id:"concepts-youll-master",level:3},{value:"Landmark Systems",id:"landmark-systems",level:3},{value:"Hardware Requirements",id:"hardware-requirements",level:2},{value:"For Inference",id:"for-inference",level:3},{value:"For Training/Fine-tuning",id:"for-trainingfine-tuning",level:3},{value:"Getting Started Checklist",id:"getting-started-checklist",level:2},{value:"Quick Environment Setup",id:"quick-environment-setup",level:3},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2}];function a(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,s.jsx)(n.p,{children:"Vision-Language-Action (VLA) models represent the frontier of robot intelligence, enabling systems that understand natural language commands, perceive their environment through vision, and generate appropriate physical actions. This module explores how foundation models are revolutionizing robotics."}),"\n",(0,s.jsx)(n.h2,{id:"module-overview",children:"Module Overview"}),"\n",(0,s.jsx)(n.p,{children:"VLA systems bridge the gap between foundation models trained on internet-scale data and the physical world of robotics. By leveraging pre-trained vision and language models, robots can generalize to new tasks, understand nuanced instructions, and adapt to novel situations with minimal task-specific training."}),"\n",(0,s.jsx)(n.p,{children:"This module takes you from the theoretical foundations through practical implementation of VLA components, preparing you to work on the cutting edge of Physical AI."}),"\n",(0,s.jsx)(n.h3,{id:"what-youll-learn",children:"What You'll Learn"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Chapter"}),(0,s.jsx)(n.th,{children:"Focus"}),(0,s.jsx)(n.th,{children:"Key Skills"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.a,{href:"/docs/module-4/vla-foundations",children:"VLA Foundations"})}),(0,s.jsx)(n.td,{children:"Architecture overview"}),(0,s.jsx)(n.td,{children:"Understanding multimodal transformers"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.a,{href:"/docs/module-4/vision-models",children:"Vision Models"})}),(0,s.jsx)(n.td,{children:"Visual perception"}),(0,s.jsx)(n.td,{children:"ViT, CLIP, feature extraction"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.a,{href:"/docs/module-4/language-integration",children:"Language Integration"})}),(0,s.jsx)(n.td,{children:"LLM for robotics"}),(0,s.jsx)(n.td,{children:"Instruction following, planning"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.a,{href:"/docs/module-4/action-generation",children:"Action Generation"})}),(0,s.jsx)(n.td,{children:"Policy architectures"}),(0,s.jsx)(n.td,{children:"Diffusion policies, action tokenization"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.a,{href:"/docs/module-4/end-to-end-vla",children:"End-to-End VLA"})}),(0,s.jsx)(n.td,{children:"Complete systems"}),(0,s.jsx)(n.td,{children:"RT-2, Octo, OpenVLA"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsx)(n.p,{children:"Before starting this module, ensure you have:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Completed ",(0,s.jsx)(n.a,{href:"/docs/module-1",children:"Module 1: ROS 2"})," (robot control basics)"]}),"\n",(0,s.jsxs)(n.li,{children:["Completed ",(0,s.jsx)(n.a,{href:"/docs/module-3",children:"Module 3: Isaac Platform"})," (GPU computing)"]}),"\n",(0,s.jsx)(n.li,{children:"Understanding of neural networks (CNNs, transformers)"}),"\n",(0,s.jsx)(n.li,{children:"PyTorch proficiency"}),"\n",(0,s.jsx)(n.li,{children:"Basic familiarity with attention mechanisms"}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{title:"Advanced Content",type:"warning",children:(0,s.jsx)(n.p,{children:"This module covers research-level material. Some concepts are from papers published in 2023-2024. Expect to engage with academic literature and experimental code."})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"the-vla-revolution",children:"The VLA Revolution"}),"\n",(0,s.jsx)(n.h3,{id:"from-separate-models-to-unified-systems",children:"From Separate Models to Unified Systems"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Evolution of Robot Intelligence                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                  \u2502\n\u2502   Traditional Robotics (2000-2015)                              \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502\n\u2502   \u2502 Vision  \u2502 \u2192  \u2502 Planning\u2502 \u2192  \u2502 Control \u2502   Separate         \u2502\n\u2502   \u2502 (CV)    \u2502    \u2502 (PDDL)  \u2502    \u2502 (PID)   \u2502   components       \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2502\n\u2502                                                                  \u2502\n\u2502   Learning-Based (2015-2020)                                    \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u2502\n\u2502   \u2502 Vision  \u2502 \u2192  \u2502 End-to-End Policy (RL)  \u2502   Vision-to-      \u2502\n\u2502   \u2502 (CNN)   \u2502    \u2502 (MLP/RNN)               \u2502   action          \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                   \u2502\n\u2502                                                                  \u2502\n\u2502   VLA Era (2022-Present)                                        \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502   \u2502                 VLA Model (Transformer)                   \u2502  \u2502\n\u2502   \u2502  Vision Encoder + Language Model + Action Decoder        \u2502  \u2502\n\u2502   \u2502                                                           \u2502  \u2502\n\u2502   \u2502  "Pick up the red cup" \u2192 [joint angles, gripper state]   \u2502  \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n'})}),"\n",(0,s.jsx)(n.h3,{id:"why-vla-matters",children:"Why VLA Matters"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Capability"}),(0,s.jsx)(n.th,{children:"Traditional"}),(0,s.jsx)(n.th,{children:"VLA"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"New tasks"})}),(0,s.jsx)(n.td,{children:"Requires reprogramming"}),(0,s.jsx)(n.td,{children:"Natural language instruction"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Generalization"})}),(0,s.jsx)(n.td,{children:"Task-specific"}),(0,s.jsx)(n.td,{children:"Broad transfer"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Scene understanding"})}),(0,s.jsx)(n.td,{children:"Hand-crafted features"}),(0,s.jsx)(n.td,{children:"Semantic comprehension"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Reasoning"})}),(0,s.jsx)(n.td,{children:"Fixed rules"}),(0,s.jsx)(n.td,{children:"Common-sense reasoning"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Data efficiency"})}),(0,s.jsx)(n.td,{children:"Millions of robot demos"}),(0,s.jsx)(n.td,{children:"Leverages internet data"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"the-vla-architecture",children:"The VLA Architecture"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    VLA Model Architecture                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                  \u2502\n\u2502   Inputs:                                                        \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502   \u2502   Images     \u2502  \u2502  Language Command    \u2502  \u2502 Robot State  \u2502 \u2502\n\u2502   \u2502  (RGB/Depth) \u2502  \u2502 "Pick up the apple"  \u2502  \u2502  (joints)    \u2502 \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502          \u2502                     \u2502                     \u2502          \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502   \u2502   Vision     \u2502  \u2502     Language         \u2502  \u2502   State      \u2502 \u2502\n\u2502   \u2502   Encoder    \u2502  \u2502     Encoder          \u2502  \u2502   Encoder    \u2502 \u2502\n\u2502   \u2502   (ViT)      \u2502  \u2502   (Transformer)      \u2502  \u2502   (MLP)      \u2502 \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502          \u2502                     \u2502                     \u2502          \u2502\n\u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n\u2502                                \u2502                                 \u2502\n\u2502                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502\n\u2502                    \u2502    Cross-Modal        \u2502                    \u2502\n\u2502                    \u2502    Fusion Layer       \u2502                    \u2502\n\u2502                    \u2502    (Attention)        \u2502                    \u2502\n\u2502                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2502\n\u2502                                \u2502                                 \u2502\n\u2502                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502\n\u2502                    \u2502    Action Decoder     \u2502                    \u2502\n\u2502                    \u2502    (Transformer)      \u2502                    \u2502\n\u2502                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2502\n\u2502                                \u2502                                 \u2502\n\u2502   Output:          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502\n\u2502                    \u2502   Robot Actions       \u2502                    \u2502\n\u2502                    \u2502   [\u0394x, \u0394y, \u0394z, \u0394rpy,  \u2502                    \u2502\n\u2502                    \u2502    gripper]           \u2502                    \u2502\n\u2502                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2502\n\u2502                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"learning-path",children:"Learning Path"}),"\n",(0,s.jsx)(n.h3,{id:"week-1-foundations",children:"Week 1: Foundations"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Day 1-2: VLA Foundations\n\u251c\u2500\u2500 Transformer architecture review\n\u251c\u2500\u2500 Multimodal learning concepts\n\u2514\u2500\u2500 Key papers: RT-2, PaLM-E\n\nDay 3-4: Vision Models\n\u251c\u2500\u2500 Vision Transformer (ViT)\n\u251c\u2500\u2500 CLIP and contrastive learning\n\u2514\u2500\u2500 Feature extraction for robotics\n\nDay 5-7: Integration Lab\n\u251c\u2500\u2500 Set up inference environment\n\u251c\u2500\u2500 Run pre-trained VLA model\n\u2514\u2500\u2500 Analyze model outputs\n"})}),"\n",(0,s.jsx)(n.h3,{id:"week-2-components",children:"Week 2: Components"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Day 1-2: Language Integration\n\u251c\u2500\u2500 LLM architectures for robotics\n\u251c\u2500\u2500 Instruction tokenization\n\u2514\u2500\u2500 Prompt engineering\n\nDay 3-4: Action Generation\n\u251c\u2500\u2500 Action representations\n\u251c\u2500\u2500 Diffusion policies\n\u2514\u2500\u2500 Safety constraints\n\nDay 5-7: End-to-End Systems\n\u251c\u2500\u2500 Complete VLA architectures\n\u251c\u2500\u2500 Training strategies\n\u2514\u2500\u2500 Deployment considerations\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"module-project-vla-demo-system",children:"Module Project: VLA Demo System"}),"\n",(0,s.jsx)(n.p,{children:"The capstone project for this module is building a language-conditioned robot control system:"}),"\n",(0,s.jsx)(n.h3,{id:"project-overview",children:"Project Overview"}),"\n",(0,s.jsx)(n.p,{children:"Build a system that:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Accepts natural language commands"}),"\n",(0,s.jsx)(n.li,{children:"Processes camera images of the scene"}),"\n",(0,s.jsx)(n.li,{children:"Generates robot actions to accomplish the task"}),"\n",(0,s.jsx)(n.li,{children:"Provides feedback on action execution"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"deliverables",children:"Deliverables"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"VLA inference pipeline with pre-trained model"}),"\n",(0,s.jsx)(n.li,{children:"ROS 2 integration for robot control"}),"\n",(0,s.jsx)(n.li,{children:"Natural language interface"}),"\n",(0,s.jsx)(n.li,{children:"Visualization of model attention/decisions"}),"\n",(0,s.jsx)(n.li,{children:"Documentation and demo video"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["See ",(0,s.jsx)(n.a,{href:"/docs/assessments/projects#project-4-vla-demo-system",children:"Project 4: VLA Demo System"})," for full requirements."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"key-concepts-preview",children:"Key Concepts Preview"}),"\n",(0,s.jsx)(n.h3,{id:"concepts-youll-master",children:"Concepts You'll Master"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Concept"}),(0,s.jsx)(n.th,{children:"Description"}),(0,s.jsx)(n.th,{children:"Applied In"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Vision Transformer (ViT)"})}),(0,s.jsx)(n.td,{children:"Patch-based image processing"}),(0,s.jsx)(n.td,{children:"Vision encoding"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Cross-Modal Attention"})}),(0,s.jsx)(n.td,{children:"Fusing vision and language"}),(0,s.jsx)(n.td,{children:"Multimodal fusion"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Action Tokenization"})}),(0,s.jsx)(n.td,{children:"Discretizing continuous actions"}),(0,s.jsx)(n.td,{children:"Action generation"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Diffusion Policy"})}),(0,s.jsx)(n.td,{children:"Denoising for action generation"}),(0,s.jsx)(n.td,{children:"Smooth trajectories"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Behavioral Cloning"})}),(0,s.jsx)(n.td,{children:"Learning from demonstrations"}),(0,s.jsx)(n.td,{children:"Training VLA"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Prompt Engineering"})}),(0,s.jsx)(n.td,{children:"Crafting effective instructions"}),(0,s.jsx)(n.td,{children:"Language interface"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"landmark-systems",children:"Landmark Systems"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"System"}),(0,s.jsx)(n.th,{children:"Organization"}),(0,s.jsx)(n.th,{children:"Key Innovation"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"RT-2"})}),(0,s.jsx)(n.td,{children:"Google DeepMind"}),(0,s.jsx)(n.td,{children:"Vision-language-action transformer"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"PaLM-E"})}),(0,s.jsx)(n.td,{children:"Google"}),(0,s.jsx)(n.td,{children:"Embodied language model"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Octo"})}),(0,s.jsx)(n.td,{children:"Berkeley"}),(0,s.jsx)(n.td,{children:"Open-source generalist policy"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"OpenVLA"})}),(0,s.jsx)(n.td,{children:"Stanford/Berkeley"}),(0,s.jsx)(n.td,{children:"Open weights, fine-tunable"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"RoboFlamingo"})}),(0,s.jsx)(n.td,{children:"ByteDance"}),(0,s.jsx)(n.td,{children:"Efficient adaptation"})]})]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,s.jsx)(n.h3,{id:"for-inference",children:"For Inference"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Component"}),(0,s.jsx)(n.th,{children:"Requirement"}),(0,s.jsx)(n.th,{children:"Notes"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"GPU"}),(0,s.jsx)(n.td,{children:"RTX 3060 / 8GB VRAM"}),(0,s.jsx)(n.td,{children:"Minimum for small models"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"CPU"}),(0,s.jsx)(n.td,{children:"8 cores"}),(0,s.jsx)(n.td,{children:"Data preprocessing"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"RAM"}),(0,s.jsx)(n.td,{children:"32GB"}),(0,s.jsx)(n.td,{children:"Model loading"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Storage"}),(0,s.jsx)(n.td,{children:"50GB"}),(0,s.jsx)(n.td,{children:"Model weights"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"for-trainingfine-tuning",children:"For Training/Fine-tuning"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Component"}),(0,s.jsx)(n.th,{children:"Requirement"}),(0,s.jsx)(n.th,{children:"Notes"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"GPU"}),(0,s.jsx)(n.td,{children:"RTX 4090 / A100"}),(0,s.jsx)(n.td,{children:"24GB+ VRAM recommended"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"CPU"}),(0,s.jsx)(n.td,{children:"16+ cores"}),(0,s.jsx)(n.td,{children:"Data loading"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"RAM"}),(0,s.jsx)(n.td,{children:"64GB+"}),(0,s.jsx)(n.td,{children:"Batch processing"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Storage"}),(0,s.jsx)(n.td,{children:"500GB+ NVMe"}),(0,s.jsx)(n.td,{children:"Datasets and checkpoints"})]})]})]}),"\n",(0,s.jsx)(n.admonition,{title:"Cloud Alternative",type:"tip",children:(0,s.jsx)(n.p,{children:"VLA training is resource-intensive. Consider Lambda Labs or RunPod with A100 GPUs for training experiments. Inference can run on consumer hardware."})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"getting-started-checklist",children:"Getting Started Checklist"}),"\n",(0,s.jsx)(n.p,{children:"Before diving into the chapters:"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Review transformer architecture basics"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Install PyTorch with CUDA support"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Set up Hugging Face account for model access"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Clone example repositories"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Allocate 50GB+ storage for model weights"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Review attention mechanism fundamentals"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"quick-environment-setup",children:"Quick Environment Setup"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Create VLA environment\nconda create -n vla python=3.10\nconda activate vla\n\n# Install PyTorch with CUDA\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n\n# Install transformers and robotics libraries\npip install transformers accelerate\npip install robomimic diffusers\n\n# Verify GPU access\npython -c \"import torch; print(f'CUDA: {torch.cuda.is_available()}')\"\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"Module 4 introduces Vision-Language-Action models\u2014the cutting edge of robot intelligence:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"VLA Foundations"}),": Understanding multimodal transformer architectures"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Vision Models"}),": Leveraging pre-trained vision encoders"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Language Integration"}),": Connecting LLMs to robot control"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action Generation"}),": From perception to physical action"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"End-to-End Systems"}),": Complete VLA implementations"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"By the end of this module, you'll be able to:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Explain how VLA models combine vision, language, and action"}),"\n",(0,s.jsx)(n.li,{children:"Implement inference pipelines using pre-trained VLA models"}),"\n",(0,s.jsx)(n.li,{children:"Design action representations for manipulation tasks"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate VLA system performance"}),"\n",(0,s.jsx)(n.li,{children:"Deploy language-conditioned robot control systems"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Let's begin with ",(0,s.jsx)(n.a,{href:"/docs/module-4/vla-foundations",children:"VLA Foundations"})," to understand the architectural principles."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2307.15818",children:"RT-2 Paper"})," - Vision-Language-Action Models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2303.03378",children:"PaLM-E Paper"})," - Embodied Multimodal Language Model"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://octo-models.github.io/",children:"Octo Paper"})," - Open-Source Generalist Policy"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://openvla.github.io/",children:"OpenVLA"})," - Open VLA for Robot Manipulation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://diffusion-policy.cs.columbia.edu/",children:"Diffusion Policy"})," - Visuomotor Policy Learning"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(a,{...e})}):a(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>l});var t=i(6540);const s={},r=t.createContext(s);function o(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);