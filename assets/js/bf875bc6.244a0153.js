"use strict";(globalThis.webpackChunkbook_website=globalThis.webpackChunkbook_website||[]).push([[8190],{3857:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-4/vla-foundations","title":"VLA Foundations","description":"Theoretical foundations of Vision-Language-Action models: architectures, training, and capabilities.","source":"@site/docs/module-4/vla-foundations.md","sourceDirName":"module-4","slug":"/module-4/vla-foundations","permalink":"/physical-ai-robotics-book/docs/module-4/vla-foundations","draft":false,"unlisted":false,"editUrl":"https://github.com/fun33333/physical-ai-robotics-book/tree/main/book-website/docs/module-4/vla-foundations.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"VLA Foundations","sidebar_position":2,"description":"Theoretical foundations of Vision-Language-Action models: architectures, training, and capabilities."},"sidebar":"tutorialSidebar","previous":{"title":"Module 4: Vision-Language-Action (VLA)","permalink":"/physical-ai-robotics-book/docs/module-4/"},"next":{"title":"Vision Models for Robotics","permalink":"/physical-ai-robotics-book/docs/module-4/vision-models"}}');var s=t(4848),o=t(8453);const r={title:"VLA Foundations",sidebar_position:2,description:"Theoretical foundations of Vision-Language-Action models: architectures, training, and capabilities."},a="VLA Foundations",l={},d=[{value:"Overview",id:"overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"The Transformer Foundation",id:"the-transformer-foundation",level:2},{value:"Attention Is All You Need",id:"attention-is-all-you-need",level:3},{value:"Key Properties for Robotics",id:"key-properties-for-robotics",level:3},{value:"Attention Mechanism",id:"attention-mechanism",level:3},{value:"Multimodal Learning",id:"multimodal-learning",level:2},{value:"From Unimodal to Multimodal",id:"from-unimodal-to-multimodal",level:3},{value:"Cross-Modal Attention",id:"cross-modal-attention",level:3},{value:"The Grounding Problem",id:"the-grounding-problem",level:2},{value:"Language to Physical World",id:"language-to-physical-world",level:3},{value:"Approaches to Grounding",id:"approaches-to-grounding",level:3},{value:"Landmark VLA Systems",id:"landmark-vla-systems",level:2},{value:"RT-2: Robotics Transformer 2",id:"rt-2-robotics-transformer-2",level:3},{value:"PaLM-E: Embodied Language Model",id:"palm-e-embodied-language-model",level:3},{value:"Octo: Open-Source Generalist Policy",id:"octo-open-source-generalist-policy",level:3},{value:"Training Paradigms",id:"training-paradigms",level:2},{value:"Behavioral Cloning",id:"behavioral-cloning",level:3},{value:"Co-Training with Internet Data",id:"co-training-with-internet-data",level:3},{value:"Exercise 1: Implement Self-Attention",id:"exercise-1-implement-self-attention",level:2},{value:"Exercise 2: Multimodal Tokenization",id:"exercise-2-multimodal-tokenization",level:2},{value:"Exercise 3: Analyze VLA Attention",id:"exercise-3-analyze-vla-attention",level:2},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2}];function c(n){const e={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"vla-foundations",children:"VLA Foundations"})}),"\n",(0,s.jsx)(e.p,{children:"Vision-Language-Action models combine insights from computer vision, natural language processing, and robot learning into unified systems capable of understanding commands and generating physical actions. This chapter provides the theoretical foundation for these powerful architectures."}),"\n",(0,s.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(e.p,{children:"In this section, you will:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Understand the transformer architecture underlying VLA models"}),"\n",(0,s.jsx)(e.li,{children:"Learn how multimodal learning combines vision and language"}),"\n",(0,s.jsx)(e.li,{children:"Explore the grounding problem in embodied AI"}),"\n",(0,s.jsx)(e.li,{children:"Study landmark VLA systems and their innovations"}),"\n",(0,s.jsx)(e.li,{children:"Understand training paradigms for VLA models"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Understanding of neural networks (layers, activations, backpropagation)"}),"\n",(0,s.jsx)(e.li,{children:"Basic familiarity with transformers and attention"}),"\n",(0,s.jsx)(e.li,{children:"Experience with PyTorch or similar framework"}),"\n",(0,s.jsx)(e.li,{children:"Linear algebra fundamentals"}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"the-transformer-foundation",children:"The Transformer Foundation"}),"\n",(0,s.jsx)(e.h3,{id:"attention-is-all-you-need",children:"Attention Is All You Need"}),"\n",(0,s.jsx)(e.p,{children:"VLA models build on the transformer architecture, which uses self-attention to process sequences:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Transformer Architecture                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                  \u2502\n\u2502   Input: [token_1, token_2, ..., token_n]                       \u2502\n\u2502                                                                  \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502                Multi-Head Self-Attention                 \u2502   \u2502\n\u2502   \u2502                                                          \u2502   \u2502\n\u2502   \u2502   Q = XW_q    K = XW_k    V = XW_v                      \u2502   \u2502\n\u2502   \u2502                                                          \u2502   \u2502\n\u2502   \u2502   Attention(Q,K,V) = softmax(QK^T / \u221ad_k) V             \u2502   \u2502\n\u2502   \u2502                                                          \u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                           \u2502                                      \u2502\n\u2502                     Add & Norm                                   \u2502\n\u2502                           \u2502                                      \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502              Feed-Forward Network                        \u2502   \u2502\n\u2502   \u2502         FFN(x) = max(0, xW_1 + b_1)W_2 + b_2           \u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                           \u2502                                      \u2502\n\u2502                     Add & Norm                                   \u2502\n\u2502                           \u2502                                      \u2502\n\u2502   Output: [out_1, out_2, ..., out_n]                            \u2502\n\u2502                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(e.h3,{id:"key-properties-for-robotics",children:"Key Properties for Robotics"}),"\n",(0,s.jsxs)(e.table,{children:[(0,s.jsx)(e.thead,{children:(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.th,{children:"Property"}),(0,s.jsx)(e.th,{children:"Benefit for VLA"})]})}),(0,s.jsxs)(e.tbody,{children:[(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"Parallelization"})}),(0,s.jsx)(e.td,{children:"Process full sequences simultaneously"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"Long-range dependencies"})}),(0,s.jsx)(e.td,{children:"Relate distant tokens (e.g., instruction to action)"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"Flexibility"})}),(0,s.jsx)(e.td,{children:"Handle variable-length inputs/outputs"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"Scalability"})}),(0,s.jsx)(e.td,{children:"Performance improves with scale"})]})]})]}),"\n",(0,s.jsx)(e.h3,{id:"attention-mechanism",children:"Attention Mechanism"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",metastring:'title="attention.py"',children:'"""Self-attention implementation."""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass SelfAttention(nn.Module):\n    """Single-head self-attention."""\n\n    def __init__(self, embed_dim: int):\n        super().__init__()\n        self.embed_dim = embed_dim\n\n        # Linear projections for Q, K, V\n        self.W_q = nn.Linear(embed_dim, embed_dim)\n        self.W_k = nn.Linear(embed_dim, embed_dim)\n        self.W_v = nn.Linear(embed_dim, embed_dim)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """\n        Args:\n            x: Input tensor [batch, seq_len, embed_dim]\n\n        Returns:\n            Output tensor [batch, seq_len, embed_dim]\n        """\n        # Project to Q, K, V\n        Q = self.W_q(x)  # [batch, seq_len, embed_dim]\n        K = self.W_k(x)\n        V = self.W_v(x)\n\n        # Compute attention scores\n        scores = torch.matmul(Q, K.transpose(-2, -1))\n        scores = scores / math.sqrt(self.embed_dim)\n\n        # Softmax to get attention weights\n        attn_weights = F.softmax(scores, dim=-1)\n\n        # Apply attention to values\n        output = torch.matmul(attn_weights, V)\n\n        return output, attn_weights\n\n\n# Example usage\nbatch_size, seq_len, embed_dim = 4, 10, 256\nx = torch.randn(batch_size, seq_len, embed_dim)\n\nattention = SelfAttention(embed_dim)\noutput, weights = attention(x)\n\nprint(f"Input shape: {x.shape}")\nprint(f"Output shape: {output.shape}")\nprint(f"Attention weights shape: {weights.shape}")\n'})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"multimodal-learning",children:"Multimodal Learning"}),"\n",(0,s.jsx)(e.h3,{id:"from-unimodal-to-multimodal",children:"From Unimodal to Multimodal"}),"\n",(0,s.jsx)(e.p,{children:"VLA models must process multiple modalities:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:'\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Multimodal Processing                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                  \u2502\n\u2502   Modality 1: Vision                                            \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502  Image \u2192 Patches \u2192 Patch Embedding \u2192 Vision Tokens      \u2502   \u2502\n\u2502   \u2502  [224x224] \u2192 [16x16 patches] \u2192 [196 tokens x 768]      \u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                                  \u2502\n\u2502   Modality 2: Language                                          \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502  Text \u2192 Tokenize \u2192 Token Embedding \u2192 Language Tokens    \u2502   \u2502\n\u2502   \u2502  "Pick up" \u2192 [512, 3894, ...] \u2192 [n tokens x 768]       \u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                                  \u2502\n\u2502   Modality 3: Robot State                                       \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502  State \u2192 Normalize \u2192 MLP \u2192 State Tokens                \u2502   \u2502\n\u2502   \u2502  [q1...q7, gripper] \u2192 [state token x 768]              \u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                                  \u2502\n\u2502   Combined Sequence:                                             \u2502\n\u2502   [CLS] [vision_1...vision_196] [SEP] [lang_1...lang_n] [state] \u2502\n\u2502                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n'})}),"\n",(0,s.jsx)(e.h3,{id:"cross-modal-attention",children:"Cross-Modal Attention"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",metastring:'title="cross_attention.py"',children:'"""Cross-modal attention for VLA."""\nimport torch\nimport torch.nn as nn\n\nclass CrossModalAttention(nn.Module):\n    """Attend from one modality to another."""\n\n    def __init__(self, query_dim: int, key_dim: int, num_heads: int = 8):\n        super().__init__()\n        self.num_heads = num_heads\n        self.head_dim = query_dim // num_heads\n\n        self.q_proj = nn.Linear(query_dim, query_dim)\n        self.k_proj = nn.Linear(key_dim, query_dim)\n        self.v_proj = nn.Linear(key_dim, query_dim)\n        self.out_proj = nn.Linear(query_dim, query_dim)\n\n    def forward(self, query: torch.Tensor, context: torch.Tensor) -> torch.Tensor:\n        """\n        Args:\n            query: Query modality [batch, query_len, query_dim]\n            context: Context modality [batch, ctx_len, key_dim]\n\n        Returns:\n            Attended output [batch, query_len, query_dim]\n        """\n        batch_size = query.size(0)\n\n        # Project\n        Q = self.q_proj(query)\n        K = self.k_proj(context)\n        V = self.v_proj(context)\n\n        # Reshape for multi-head attention\n        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n\n        # Attention\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n        attn = torch.softmax(scores, dim=-1)\n        out = torch.matmul(attn, V)\n\n        # Reshape and project\n        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.head_dim)\n        return self.out_proj(out)\n\n\n# Example: Language attending to vision\nvision_tokens = torch.randn(4, 196, 768)  # 14x14 patches\nlanguage_tokens = torch.randn(4, 20, 768)  # 20 text tokens\n\ncross_attn = CrossModalAttention(768, 768)\nattended = cross_attn(language_tokens, vision_tokens)\nprint(f"Output shape: {attended.shape}")  # [4, 20, 768]\n'})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"the-grounding-problem",children:"The Grounding Problem"}),"\n",(0,s.jsx)(e.h3,{id:"language-to-physical-world",children:"Language to Physical World"}),"\n",(0,s.jsxs)(e.p,{children:["A fundamental challenge in VLA is ",(0,s.jsx)(e.strong,{children:"grounding"}),"\u2014connecting abstract language to physical entities and actions:"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:'\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    The Grounding Problem                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                  \u2502\n\u2502   Language: "Pick up the red cup"                               \u2502\n\u2502                                                                  \u2502\n\u2502   Grounding Requirements:                                        \u2502\n\u2502                                                                  \u2502\n\u2502   1. Object Grounding                                           \u2502\n\u2502      "red cup" \u2192 specific object in scene                       \u2502\n\u2502      Which pixels? Which 3D location?                           \u2502\n\u2502                                                                  \u2502\n\u2502   2. Action Grounding                                           \u2502\n\u2502      "pick up" \u2192 specific motion sequence                       \u2502\n\u2502      Approach trajectory? Grasp pose? Lift height?              \u2502\n\u2502                                                                  \u2502\n\u2502   3. Context Grounding                                          \u2502\n\u2502      Implicit constraints from scene                            \u2502\n\u2502      Avoid obstacles, respect physics, safety                   \u2502\n\u2502                                                                  \u2502\n\u2502   4. Success Grounding                                          \u2502\n\u2502      What does "done" look like?                                \u2502\n\u2502      Cup lifted? At specific height? In hand?                   \u2502\n\u2502                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n'})}),"\n",(0,s.jsx)(e.h3,{id:"approaches-to-grounding",children:"Approaches to Grounding"}),"\n",(0,s.jsxs)(e.table,{children:[(0,s.jsx)(e.thead,{children:(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.th,{children:"Approach"}),(0,s.jsx)(e.th,{children:"Description"}),(0,s.jsx)(e.th,{children:"Examples"})]})}),(0,s.jsxs)(e.tbody,{children:[(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"End-to-End"})}),(0,s.jsx)(e.td,{children:"Learn grounding implicitly"}),(0,s.jsx)(e.td,{children:"RT-2, OpenVLA"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"Modular"})}),(0,s.jsx)(e.td,{children:"Explicit grounding modules"}),(0,s.jsx)(e.td,{children:"SayCan, Code as Policies"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"Hybrid"})}),(0,s.jsx)(e.td,{children:"Foundation model + specialized heads"}),(0,s.jsx)(e.td,{children:"PaLM-E"})]})]})]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"landmark-vla-systems",children:"Landmark VLA Systems"}),"\n",(0,s.jsx)(e.h3,{id:"rt-2-robotics-transformer-2",children:"RT-2: Robotics Transformer 2"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:'\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        RT-2 Architecture                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                  \u2502\n\u2502   Base: PaLI-X (Vision-Language Model, 55B parameters)          \u2502\n\u2502                                                                  \u2502\n\u2502   Input:                                                         \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502   \u2502    Images      \u2502  \u2502  "What action should the      \u2502         \u2502\n\u2502   \u2502    (history)   \u2502  \u2502   robot take to pick up       \u2502         \u2502\n\u2502   \u2502                \u2502  \u2502   the can?"                   \u2502         \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502            \u2502                          \u2502                          \u2502\n\u2502            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                          \u2502\n\u2502                           \u2502                                      \u2502\n\u2502                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510                              \u2502\n\u2502                    \u2502   PaLI-X    \u2502                              \u2502\n\u2502                    \u2502  Backbone   \u2502                              \u2502\n\u2502                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518                              \u2502\n\u2502                           \u2502                                      \u2502\n\u2502   Output:         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502                   \u2502  "1 128 91 241 5 101 127 100"     \u2502         \u2502\n\u2502                   \u2502  (tokenized action)               \u2502         \u2502\n\u2502                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502                                                                  \u2502\n\u2502   Key Innovation: Actions as text tokens in LLM vocabulary      \u2502\n\u2502                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n'})}),"\n",(0,s.jsx)(e.h3,{id:"palm-e-embodied-language-model",children:"PaLM-E: Embodied Language Model"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",metastring:'title="palm_e_concept.py"',children:'"""Conceptual PaLM-E architecture."""\n\nclass PaLME:\n    """\n    PaLM-E: An Embodied Multimodal Language Model\n\n    Key ideas:\n    1. Inject continuous sensor observations into language model\n    2. Use ViT to encode images into "visual tokens"\n    3. Interleave visual tokens with text tokens\n    4. Output can be text (reasoning) or actions\n    """\n\n    def __init__(self):\n        self.vision_encoder = ViT()  # Vision Transformer\n        self.language_model = PaLM()  # 540B parameter LLM\n\n    def forward(self, images, text):\n        # Encode images to visual tokens\n        visual_tokens = self.vision_encoder(images)\n\n        # Interleave with text\n        # "I see <img> on the table. Pick up the <img>."\n        # becomes: [I, see, [vis_tokens], on, the, table, ...]\n\n        combined = self.interleave(text, visual_tokens)\n\n        # Generate output (text or actions)\n        output = self.language_model(combined)\n\n        return output\n'})}),"\n",(0,s.jsx)(e.h3,{id:"octo-open-source-generalist-policy",children:"Octo: Open-Source Generalist Policy"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        Octo Architecture                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                  \u2502\n\u2502   Key Features:                                                  \u2502\n\u2502   \u2022 Open-source and fine-tunable                                \u2502\n\u2502   \u2022 Trained on 800K robot trajectories (Open X-Embodiment)      \u2502\n\u2502   \u2022 Supports multiple robots and tasks                          \u2502\n\u2502                                                                  \u2502\n\u2502   Architecture:                                                  \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502\n\u2502   \u2502   Images    \u2502  \u2502  Language   \u2502  \u2502   State     \u2502            \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502\n\u2502          \u2502                \u2502                \u2502                     \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502\n\u2502   \u2502    ViT     \u2502  \u2502  T5 Encoder \u2502  \u2502    MLP     \u2502             \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502\n\u2502          \u2502                \u2502                \u2502                     \u2502\n\u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2502\n\u2502                           \u2502                                      \u2502\n\u2502                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                             \u2502\n\u2502                   \u2502  Transformer  \u2502                             \u2502\n\u2502                   \u2502   Backbone    \u2502                             \u2502\n\u2502                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                             \u2502\n\u2502                           \u2502                                      \u2502\n\u2502                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                             \u2502\n\u2502                   \u2502 Action Head   \u2502                             \u2502\n\u2502                   \u2502 (Diffusion)   \u2502                             \u2502\n\u2502                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                             \u2502\n\u2502                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"training-paradigms",children:"Training Paradigms"}),"\n",(0,s.jsx)(e.h3,{id:"behavioral-cloning",children:"Behavioral Cloning"}),"\n",(0,s.jsx)(e.p,{children:"Learn directly from demonstrations:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",metastring:'title="behavioral_cloning.py"',children:'"""Behavioral cloning training loop."""\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\ndef train_bc(model, dataloader, optimizer, epochs=100):\n    """\n    Train policy via behavioral cloning.\n\n    Dataset contains (observation, action) pairs from expert demonstrations.\n    """\n    criterion = nn.MSELoss()\n\n    for epoch in range(epochs):\n        total_loss = 0\n\n        for batch in dataloader:\n            obs = batch[\'observation\']  # Images + language + state\n            expert_action = batch[\'action\']  # Expert action\n\n            optimizer.zero_grad()\n\n            # Predict action\n            predicted_action = model(obs)\n\n            # MSE loss against expert\n            loss = criterion(predicted_action, expert_action)\n\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        print(f"Epoch {epoch}: Loss = {total_loss / len(dataloader):.4f}")\n'})}),"\n",(0,s.jsx)(e.h3,{id:"co-training-with-internet-data",children:"Co-Training with Internet Data"}),"\n",(0,s.jsx)(e.p,{children:"Leverage internet-scale data alongside robot data:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Co-Training Strategy                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                  \u2502\n\u2502   Internet Data (billions of examples)                          \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502  Image-text pairs, video captions, QA datasets          \u2502   \u2502\n\u2502   \u2502  \u2192 Learn visual understanding, language, world knowledge\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                              +                                   \u2502\n\u2502   Robot Data (millions of examples)                             \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502  Robot trajectories with language annotations           \u2502   \u2502\n\u2502   \u2502  \u2192 Learn action generation, physical grounding          \u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                              =                                   \u2502\n\u2502   VLA Model                                                      \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502  \u2022 Generalizes to new objects (from internet data)      \u2502   \u2502\n\u2502   \u2502  \u2022 Generates valid robot actions (from robot data)      \u2502   \u2502\n\u2502   \u2502  \u2022 Follows complex instructions (from both)             \u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"exercise-1-implement-self-attention",children:"Exercise 1: Implement Self-Attention"}),"\n",(0,s.jsxs)(e.admonition,{title:"Exercise 1: Build Attention from Scratch",type:"tip",children:[(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Objective"}),": Implement multi-head self-attention."]}),(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Steps"}),":"]}),(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Implement single-head attention"}),"\n",(0,s.jsx)(e.li,{children:"Extend to multi-head attention"}),"\n",(0,s.jsx)(e.li,{children:"Add position encodings"}),"\n",(0,s.jsx)(e.li,{children:"Test on a sequence modeling task"}),"\n"]}),(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Starter Code"}),":"]}),(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"class MultiHeadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        # TODO: Implement\n        pass\n\n    def forward(self, x):\n        # TODO: Implement\n        pass\n"})}),(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Verification"}),":"]}),(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Output shape matches input shape"}),"\n",(0,s.jsx)(e.li,{children:"Attention weights sum to 1 along key dimension"}),"\n",(0,s.jsx)(e.li,{children:"Different heads learn different patterns"}),"\n"]}),(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Time Estimate"}),": 45 minutes"]})]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"exercise-2-multimodal-tokenization",children:"Exercise 2: Multimodal Tokenization"}),"\n",(0,s.jsxs)(e.admonition,{title:"Exercise 2: Tokenize Vision and Language",type:"tip",children:[(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Objective"}),": Create a unified token sequence from images and text."]}),(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Steps"}),":"]}),(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Patch an image into 16x16 patches"}),"\n",(0,s.jsx)(e.li,{children:"Project patches to embedding dimension"}),"\n",(0,s.jsx)(e.li,{children:"Tokenize text with a pretrained tokenizer"}),"\n",(0,s.jsx)(e.li,{children:"Concatenate into single sequence with special tokens"}),"\n"]}),(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Expected Output"}),":"]}),(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"[CLS] [IMG_1] ... [IMG_196] [SEP] [TEXT_1] ... [TEXT_N] [EOS]\n"})}),(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Time Estimate"}),": 30 minutes"]})]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"exercise-3-analyze-vla-attention",children:"Exercise 3: Analyze VLA Attention"}),"\n",(0,s.jsxs)(e.admonition,{title:"Exercise 3: Visualize Cross-Modal Attention",type:"tip",children:[(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Objective"}),": Understand what VLA models attend to."]}),(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Steps"}),":"]}),(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Load a pre-trained VLA model (Octo or OpenVLA)"}),"\n",(0,s.jsx)(e.li,{children:"Run inference and extract attention weights"}),"\n",(0,s.jsx)(e.li,{children:"Visualize which image patches get attention for different instructions"}),"\n",(0,s.jsx)(e.li,{children:"Compare attention for different tasks"}),"\n"]}),(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Analysis Questions"}),":"]}),(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:'Does "pick up the red object" attend to red pixels?'}),"\n",(0,s.jsx)(e.li,{children:"How does attention change over the action sequence?"}),"\n"]}),(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Time Estimate"}),": 60 minutes"]})]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(e.p,{children:"In this chapter, you learned:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Transformers"}),": The architectural foundation of VLA models"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Multimodal Learning"}),": Combining vision and language representations"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Grounding"}),": The challenge of connecting language to physical action"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Landmark Systems"}),": RT-2, PaLM-E, Octo and their innovations"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Training"}),": Behavioral cloning and co-training strategies"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"VLA models represent a paradigm shift from task-specific robots to general-purpose systems that understand language. The transformer architecture enables this by providing a flexible framework for multimodal learning."}),"\n",(0,s.jsxs)(e.p,{children:["Next, explore ",(0,s.jsx)(e.a,{href:"/docs/module-4/vision-models",children:"Vision Models for Robotics"})," to understand how visual perception feeds into VLA systems."]}),"\n",(0,s.jsx)(e.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"https://arxiv.org/abs/1706.03762",children:"Attention Is All You Need"})," - Original transformer paper"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"https://arxiv.org/abs/2307.15818",children:"RT-2"})," - Vision-Language-Action Models"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"https://arxiv.org/abs/2303.03378",children:"PaLM-E"})," - Embodied Multimodal Language Model"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"https://octo-models.github.io/",children:"Octo"})," - Open-Source Generalist Policy"]}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"https://arxiv.org/abs/2402.00046",children:"A Survey on Vision-Language-Action Models"})}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(c,{...n})}):c(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>r,x:()=>a});var i=t(6540);const s={},o=i.createContext(s);function r(n){const e=i.useContext(o);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:r(n.components),i.createElement(o.Provider,{value:e},n.children)}}}]);