"use strict";(globalThis.webpackChunkbook_website=globalThis.webpackChunkbook_website||[]).push([[5958],{6187:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>a,contentTitle:()=>c,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"module-4/action-generation","title":"Action Generation","description":"From perception to action: policy architectures, action spaces, and motion generation techniques.","source":"@site/docs/module-4/action-generation.md","sourceDirName":"module-4","slug":"/module-4/action-generation","permalink":"/physical-ai-robotics-book/docs/module-4/action-generation","draft":false,"unlisted":false,"editUrl":"https://github.com/fun33333/physical-ai-robotics-book/tree/main/book-website/docs/module-4/action-generation.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"Action Generation","sidebar_position":5,"description":"From perception to action: policy architectures, action spaces, and motion generation techniques."},"sidebar":"tutorialSidebar","previous":{"title":"Language Model Integration","permalink":"/physical-ai-robotics-book/docs/module-4/language-integration"},"next":{"title":"End-to-End VLA Systems","permalink":"/physical-ai-robotics-book/docs/module-4/end-to-end-vla"}}');var t=i(4848),s=i(8453);const r={title:"Action Generation",sidebar_position:5,description:"From perception to action: policy architectures, action spaces, and motion generation techniques."},c="Action Generation",a={},l=[{value:"Overview",id:"overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Action Space Design",id:"action-space-design",level:2},{value:"Types of Action Representations",id:"types-of-action-representations",level:3},{value:"Action Space Comparison",id:"action-space-comparison",level:3},{value:"Implementing Action Spaces",id:"implementing-action-spaces",level:3},{value:"Transformer Action Heads",id:"transformer-action-heads",level:2},{value:"Action Prediction Architecture",id:"action-prediction-architecture",level:3},{value:"Implementation",id:"implementation",level:3},{value:"Diffusion Policies",id:"diffusion-policies",level:2},{value:"Diffusion for Trajectory Generation",id:"diffusion-for-trajectory-generation",level:3},{value:"Implementation",id:"implementation-1",level:3},{value:"Flow Matching Policies",id:"flow-matching-policies",level:2},{value:"Flow Matching for Actions",id:"flow-matching-for-actions",level:3},{value:"Action Safety and Constraints",id:"action-safety-and-constraints",level:2},{value:"Constraint Enforcement",id:"constraint-enforcement",level:3},{value:"Exercise 1: Implement Action Transformer",id:"exercise-1-implement-action-transformer",level:2},{value:"Exercise 2: Train Diffusion Policy",id:"exercise-2-train-diffusion-policy",level:2},{value:"Exercise 3: Safe Action Execution",id:"exercise-3-safe-action-execution",level:2},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(n){const e={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"action-generation",children:"Action Generation"})}),"\n",(0,t.jsx)(e.p,{children:"The final step in any VLA pipeline is generating actions that achieve the desired goal. This chapter covers the various approaches to action generation, from discrete skill selection to continuous trajectory prediction. You'll learn about different action representations, policy architectures, and techniques for ensuring smooth, safe robot motion."}),"\n",(0,t.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(e.p,{children:"In this section, you will:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Understand different action space representations"}),"\n",(0,t.jsx)(e.li,{children:"Implement transformer-based action prediction"}),"\n",(0,t.jsx)(e.li,{children:"Build diffusion policies for smooth trajectories"}),"\n",(0,t.jsx)(e.li,{children:"Design action tokenization schemes"}),"\n",(0,t.jsx)(e.li,{children:"Create motion generation pipelines"}),"\n",(0,t.jsx)(e.li,{children:"Handle action safety and constraints"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Understanding of neural network architectures"}),"\n",(0,t.jsx)(e.li,{children:"Familiarity with PyTorch"}),"\n",(0,t.jsx)(e.li,{children:"Knowledge of robot kinematics basics"}),"\n",(0,t.jsxs)(e.li,{children:["Completed ",(0,t.jsx)(e.a,{href:"/docs/module-4/vision-models",children:"Vision Models"})," and ",(0,t.jsx)(e.a,{href:"/docs/module-4/language-integration",children:"Language Integration"})]}),"\n"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"action-space-design",children:"Action Space Design"}),"\n",(0,t.jsx)(e.h3,{id:"types-of-action-representations",children:"Types of Action Representations"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Action Space Taxonomy                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                  \u2502\n\u2502   Discrete Actions                                               \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502  \u2022 Skill selection: pick, place, push, pull             \u2502   \u2502\n\u2502   \u2502  \u2022 Direction: up, down, left, right                     \u2502   \u2502\n\u2502   \u2502  \u2022 Object choice: object_1, object_2, ...               \u2502   \u2502\n\u2502   \u2502  + Simple to learn, interpretable                       \u2502   \u2502\n\u2502   \u2502  - Limited precision, cannot express continuous motion  \u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                                  \u2502\n\u2502   Continuous Actions                                             \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502  \u2022 Joint velocities: [dq1, dq2, ..., dqn]               \u2502   \u2502\n\u2502   \u2502  \u2022 End-effector delta: [dx, dy, dz, droll, dpitch, dyaw]\u2502   \u2502\n\u2502   \u2502  \u2022 Joint positions: [q1, q2, ..., qn]                   \u2502   \u2502\n\u2502   \u2502  + Precise control, smooth motion                       \u2502   \u2502\n\u2502   \u2502  - Higher dimensional, harder to learn                  \u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                                  \u2502\n\u2502   Hybrid Actions                                                 \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502  \u2022 Skill + parameters: pick(x, y, z)                    \u2502   \u2502\n\u2502   \u2502  \u2022 Waypoints: [(x1,y1,z1), (x2,y2,z2), ...]            \u2502   \u2502\n\u2502   \u2502  \u2022 Tokenized continuous: discretized into bins          \u2502   \u2502\n\u2502   \u2502  + Balance of expressiveness and learnability           \u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsx)(e.h3,{id:"action-space-comparison",children:"Action Space Comparison"}),"\n",(0,t.jsxs)(e.table,{children:[(0,t.jsx)(e.thead,{children:(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.th,{children:"Representation"}),(0,t.jsx)(e.th,{children:"Dimensionality"}),(0,t.jsx)(e.th,{children:"Precision"}),(0,t.jsx)(e.th,{children:"Learning Difficulty"}),(0,t.jsx)(e.th,{children:"Use Case"})]})}),(0,t.jsxs)(e.tbody,{children:[(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:(0,t.jsx)(e.strong,{children:"Discrete skills"})}),(0,t.jsx)(e.td,{children:"Low (10-50)"}),(0,t.jsx)(e.td,{children:"Low"}),(0,t.jsx)(e.td,{children:"Easy"}),(0,t.jsx)(e.td,{children:"High-level planning"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:(0,t.jsx)(e.strong,{children:"Joint velocities"})}),(0,t.jsx)(e.td,{children:"Medium (6-12)"}),(0,t.jsx)(e.td,{children:"High"}),(0,t.jsx)(e.td,{children:"Medium"}),(0,t.jsx)(e.td,{children:"Velocity control"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:(0,t.jsx)(e.strong,{children:"EE delta pose"})}),(0,t.jsx)(e.td,{children:"Fixed (6-7)"}),(0,t.jsx)(e.td,{children:"High"}),(0,t.jsx)(e.td,{children:"Medium"}),(0,t.jsx)(e.td,{children:"Cartesian control"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:(0,t.jsx)(e.strong,{children:"Joint positions"})}),(0,t.jsx)(e.td,{children:"Medium (6-12)"}),(0,t.jsx)(e.td,{children:"High"}),(0,t.jsx)(e.td,{children:"Medium"}),(0,t.jsx)(e.td,{children:"Position control"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:(0,t.jsx)(e.strong,{children:"Tokenized"})}),(0,t.jsx)(e.td,{children:"High (100-1000)"}),(0,t.jsx)(e.td,{children:"Medium"}),(0,t.jsx)(e.td,{children:"Hard"}),(0,t.jsx)(e.td,{children:"VLA models"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:(0,t.jsx)(e.strong,{children:"Trajectories"})}),(0,t.jsx)(e.td,{children:"Very High"}),(0,t.jsx)(e.td,{children:"Very High"}),(0,t.jsx)(e.td,{children:"Hard"}),(0,t.jsx)(e.td,{children:"Smooth motion"})]})]})]}),"\n",(0,t.jsx)(e.h3,{id:"implementing-action-spaces",children:"Implementing Action Spaces"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",metastring:'title="action_spaces.py"',children:'"""Action space definitions for robot control."""\nimport numpy as np\nfrom dataclasses import dataclass\nfrom typing import List, Tuple, Optional\nfrom enum import Enum\n\nclass ActionType(Enum):\n    DISCRETE = "discrete"\n    CONTINUOUS = "continuous"\n    HYBRID = "hybrid"\n\n\n@dataclass\nclass ActionSpace:\n    """Base action space definition."""\n    action_type: ActionType\n    dimension: int\n    bounds: Optional[Tuple[np.ndarray, np.ndarray]] = None\n\n    def normalize(self, action: np.ndarray) -> np.ndarray:\n        """Normalize action to [-1, 1]."""\n        if self.bounds is None:\n            return action\n        low, high = self.bounds\n        return 2.0 * (action - low) / (high - low) - 1.0\n\n    def denormalize(self, action: np.ndarray) -> np.ndarray:\n        """Denormalize action from [-1, 1]."""\n        if self.bounds is None:\n            return action\n        low, high = self.bounds\n        return low + (action + 1.0) * (high - low) / 2.0\n\n\nclass EndEffectorDeltaSpace(ActionSpace):\n    """6-DOF end-effector delta action space."""\n\n    def __init__(self, max_delta_pos: float = 0.05, max_delta_rot: float = 0.1):\n        bounds = (\n            np.array([-max_delta_pos] * 3 + [-max_delta_rot] * 3),\n            np.array([max_delta_pos] * 3 + [max_delta_rot] * 3)\n        )\n        super().__init__(\n            action_type=ActionType.CONTINUOUS,\n            dimension=6,\n            bounds=bounds\n        )\n\n    def to_transform(self, action: np.ndarray) -> np.ndarray:\n        """Convert action to 4x4 transformation matrix."""\n        dx, dy, dz, droll, dpitch, dyaw = action\n\n        # Rotation matrix from Euler angles\n        from scipy.spatial.transform import Rotation\n        R = Rotation.from_euler(\'xyz\', [droll, dpitch, dyaw]).as_matrix()\n\n        T = np.eye(4)\n        T[:3, :3] = R\n        T[:3, 3] = [dx, dy, dz]\n        return T\n\n\nclass JointPositionSpace(ActionSpace):\n    """Joint position action space."""\n\n    def __init__(self, joint_limits: List[Tuple[float, float]]):\n        low = np.array([lim[0] for lim in joint_limits])\n        high = np.array([lim[1] for lim in joint_limits])\n        super().__init__(\n            action_type=ActionType.CONTINUOUS,\n            dimension=len(joint_limits),\n            bounds=(low, high)\n        )\n\n\nclass TokenizedActionSpace(ActionSpace):\n    """Discretized continuous action space using bins."""\n\n    def __init__(self, continuous_dim: int, num_bins: int = 256):\n        self.continuous_dim = continuous_dim\n        self.num_bins = num_bins\n\n        super().__init__(\n            action_type=ActionType.HYBRID,\n            dimension=continuous_dim,\n            bounds=(np.zeros(continuous_dim), np.ones(continuous_dim) * (num_bins - 1))\n        )\n\n    def tokenize(self, continuous_action: np.ndarray) -> np.ndarray:\n        """Convert continuous action to tokens."""\n        # Assume input is normalized to [0, 1]\n        tokens = np.clip(\n            (continuous_action * self.num_bins).astype(int),\n            0, self.num_bins - 1\n        )\n        return tokens\n\n    def detokenize(self, tokens: np.ndarray) -> np.ndarray:\n        """Convert tokens back to continuous action."""\n        return (tokens + 0.5) / self.num_bins\n\n\n# Example usage\nee_space = EndEffectorDeltaSpace(max_delta_pos=0.02, max_delta_rot=0.05)\naction = np.array([0.01, 0.0, -0.005, 0.0, 0.0, 0.02])\nnormalized = ee_space.normalize(action)\nprint(f"Normalized action: {normalized}")\n\ntokenized_space = TokenizedActionSpace(continuous_dim=7, num_bins=256)\ntokens = tokenized_space.tokenize(np.array([0.5, 0.3, 0.7, 0.1, 0.9, 0.5, 0.5]))\nprint(f"Tokens: {tokens}")\n'})}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"transformer-action-heads",children:"Transformer Action Heads"}),"\n",(0,t.jsx)(e.h3,{id:"action-prediction-architecture",children:"Action Prediction Architecture"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                Transformer Action Head                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                  \u2502\n\u2502   Vision Features     Language Features     Proprioception      \u2502\n\u2502   [v1, v2, ..., vn]   [l1, l2, ..., lm]    [p1, p2, ..., pk]   \u2502\n\u2502         \u2502                    \u2502                    \u2502              \u2502\n\u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n\u2502                              \u2502                                   \u2502\n\u2502                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                        \u2502\n\u2502                    \u2502  Cross-Attention  \u2502                        \u2502\n\u2502                    \u2502    Transformer    \u2502                        \u2502\n\u2502                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                        \u2502\n\u2502                              \u2502                                   \u2502\n\u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n\u2502              \u2502               \u2502               \u2502                   \u2502\n\u2502       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u2502\n\u2502       \u2502  Position   \u2502 \u2502  Rotation   \u2502 \u2502  Gripper   \u2502           \u2502\n\u2502       \u2502    Head     \u2502 \u2502    Head     \u2502 \u2502   Head     \u2502           \u2502\n\u2502       \u2502  (x,y,z)    \u2502 \u2502 (qx,qy,qz,w)\u2502 \u2502  (open)    \u2502           \u2502\n\u2502       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2502\n\u2502                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsx)(e.h3,{id:"implementation",children:"Implementation"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",metastring:'title="action_transformer.py"',children:'"""Transformer-based action generation."""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange\n\nclass ActionTransformer(nn.Module):\n    """Transformer for action prediction from multimodal inputs."""\n\n    def __init__(\n        self,\n        vision_dim: int = 768,\n        language_dim: int = 768,\n        proprio_dim: int = 12,\n        hidden_dim: int = 512,\n        num_layers: int = 4,\n        num_heads: int = 8,\n        action_dim: int = 7,\n        action_horizon: int = 1,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n\n        self.action_horizon = action_horizon\n\n        # Input projections\n        self.vision_proj = nn.Linear(vision_dim, hidden_dim)\n        self.language_proj = nn.Linear(language_dim, hidden_dim)\n        self.proprio_proj = nn.Linear(proprio_dim, hidden_dim)\n\n        # Learnable action queries\n        self.action_queries = nn.Parameter(\n            torch.randn(action_horizon, hidden_dim)\n        )\n\n        # Transformer decoder\n        decoder_layer = nn.TransformerDecoderLayer(\n            d_model=hidden_dim,\n            nhead=num_heads,\n            dim_feedforward=hidden_dim * 4,\n            dropout=dropout,\n            batch_first=True\n        )\n        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n\n        # Action heads\n        self.position_head = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 3)\n        )\n\n        self.rotation_head = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 4)  # quaternion\n        )\n\n        self.gripper_head = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(\n        self,\n        vision_features: torch.Tensor,\n        language_features: torch.Tensor,\n        proprio: torch.Tensor\n    ) -> dict:\n        """\n        Generate actions from multimodal inputs.\n\n        Args:\n            vision_features: [batch, num_patches, vision_dim]\n            language_features: [batch, seq_len, language_dim]\n            proprio: [batch, proprio_dim]\n\n        Returns:\n            dict with position, rotation, gripper actions\n        """\n        batch_size = vision_features.size(0)\n\n        # Project inputs\n        v = self.vision_proj(vision_features)  # [B, P, H]\n        l = self.language_proj(language_features)  # [B, S, H]\n        p = self.proprio_proj(proprio).unsqueeze(1)  # [B, 1, H]\n\n        # Concatenate context\n        context = torch.cat([v, l, p], dim=1)  # [B, P+S+1, H]\n\n        # Expand action queries for batch\n        queries = self.action_queries.unsqueeze(0).expand(batch_size, -1, -1)\n\n        # Decode actions\n        action_features = self.decoder(queries, context)  # [B, H, hidden]\n\n        # Predict action components\n        position = self.position_head(action_features)  # [B, H, 3]\n        rotation = self.rotation_head(action_features)  # [B, H, 4]\n        rotation = F.normalize(rotation, dim=-1)  # Normalize quaternion\n        gripper = self.gripper_head(action_features)  # [B, H, 1]\n\n        return {\n            \'position\': position,\n            \'rotation\': rotation,\n            \'gripper\': gripper.squeeze(-1),\n            \'features\': action_features\n        }\n\n\nclass ActionTokenPredictor(nn.Module):\n    """Predict discretized action tokens autoregressively."""\n\n    def __init__(\n        self,\n        vocab_size: int = 256,\n        action_dim: int = 7,\n        hidden_dim: int = 512,\n        num_layers: int = 6,\n        num_heads: int = 8\n    ):\n        super().__init__()\n\n        self.action_dim = action_dim\n        self.vocab_size = vocab_size\n\n        # Token embedding\n        self.token_embed = nn.Embedding(vocab_size, hidden_dim)\n        self.pos_embed = nn.Embedding(action_dim, hidden_dim)\n\n        # Transformer\n        decoder_layer = nn.TransformerDecoderLayer(\n            d_model=hidden_dim,\n            nhead=num_heads,\n            dim_feedforward=hidden_dim * 4,\n            batch_first=True\n        )\n        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n\n        # Output projection\n        self.output_proj = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(\n        self,\n        context: torch.Tensor,\n        action_tokens: torch.Tensor = None\n    ) -> torch.Tensor:\n        """\n        Predict action tokens.\n\n        Args:\n            context: [batch, context_len, hidden_dim] - encoded observation\n            action_tokens: [batch, seq_len] - previous tokens (for training)\n\n        Returns:\n            logits: [batch, seq_len, vocab_size]\n        """\n        batch_size = context.size(0)\n        device = context.device\n\n        if action_tokens is None:\n            # Generate from scratch\n            return self._generate(context)\n\n        # Training mode - teacher forcing\n        seq_len = action_tokens.size(1)\n        positions = torch.arange(seq_len, device=device)\n\n        # Embed tokens and positions\n        token_emb = self.token_embed(action_tokens)\n        pos_emb = self.pos_embed(positions)\n        query = token_emb + pos_emb\n\n        # Causal mask\n        causal_mask = torch.triu(\n            torch.ones(seq_len, seq_len, device=device),\n            diagonal=1\n        ).bool()\n\n        # Decode\n        hidden = self.decoder(\n            query, context,\n            tgt_mask=causal_mask\n        )\n\n        return self.output_proj(hidden)\n\n    @torch.no_grad()\n    def _generate(self, context: torch.Tensor) -> torch.Tensor:\n        """Generate action tokens autoregressively."""\n        batch_size = context.size(0)\n        device = context.device\n\n        tokens = []\n        for i in range(self.action_dim):\n            if i == 0:\n                # First token - no previous context\n                pos_emb = self.pos_embed(torch.tensor([0], device=device))\n                query = pos_emb.expand(batch_size, 1, -1)\n            else:\n                # Use previous tokens\n                prev_tokens = torch.stack(tokens, dim=1)\n                positions = torch.arange(i, device=device)\n                token_emb = self.token_embed(prev_tokens)\n                pos_emb = self.pos_embed(positions)\n                query = token_emb + pos_emb\n\n            hidden = self.decoder(query, context)\n            logits = self.output_proj(hidden[:, -1])  # Last position\n            next_token = logits.argmax(dim=-1)\n            tokens.append(next_token)\n\n        return torch.stack(tokens, dim=1)\n'})}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"diffusion-policies",children:"Diffusion Policies"}),"\n",(0,t.jsx)(e.h3,{id:"diffusion-for-trajectory-generation",children:"Diffusion for Trajectory Generation"}),"\n",(0,t.jsx)(e.p,{children:"Diffusion models generate smooth, multi-modal action distributions:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Diffusion Policy                              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                  \u2502\n\u2502   Forward Process (Training):                                   \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502   \u2502 Action   \u2502\u2500\u2500\u2500\u25b6\u2502 + Noise  \u2502\u2500\u2500\u2500\u25b6\u2502 + Noise  \u2502\u2500\u2500\u2500\u25b6\u2502  Pure    \u2502 \u2502\n\u2502   \u2502 \u03c4\u2080       \u2502    \u2502 \u03c4\u2081       \u2502    \u2502 \u03c4\u2082       \u2502    \u2502  Noise   \u2502 \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                  \u2502\n\u2502   Reverse Process (Inference):                                  \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502   \u2502  Pure    \u2502\u2500\u2500\u2500\u25b6\u2502 Denoise  \u2502\u2500\u2500\u2500\u25b6\u2502 Denoise  \u2502\u2500\u2500\u2500\u25b6\u2502 Action   \u2502 \u2502\n\u2502   \u2502  Noise   \u2502    \u2502 (UNet)   \u2502    \u2502 (UNet)   \u2502    \u2502 \u03c4\u2080       \u2502 \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                        \u25b2               \u25b2               \u25b2        \u2502\n\u2502                        \u2502               \u2502               \u2502        \u2502\n\u2502                   Observation     Observation     Observation   \u2502\n\u2502                   Conditioning   Conditioning    Conditioning   \u2502\n\u2502                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsx)(e.h3,{id:"implementation-1",children:"Implementation"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",metastring:'title="diffusion_policy.py"',children:'"""Diffusion policy for action generation."""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nclass SinusoidalPosEmb(nn.Module):\n    """Sinusoidal positional embedding for diffusion timestep."""\n\n    def __init__(self, dim: int):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, t: torch.Tensor) -> torch.Tensor:\n        device = t.device\n        half_dim = self.dim // 2\n        emb = np.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n        emb = t[:, None] * emb[None, :]\n        return torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n\n\nclass ConditionalUNet1D(nn.Module):\n    """1D U-Net for denoising action trajectories."""\n\n    def __init__(\n        self,\n        action_dim: int = 7,\n        action_horizon: int = 16,\n        obs_dim: int = 512,\n        hidden_dim: int = 256,\n        num_layers: int = 4\n    ):\n        super().__init__()\n\n        self.action_horizon = action_horizon\n        self.action_dim = action_dim\n\n        # Time embedding\n        self.time_embed = nn.Sequential(\n            SinusoidalPosEmb(hidden_dim),\n            nn.Linear(hidden_dim, hidden_dim * 2),\n            nn.Mish(),\n            nn.Linear(hidden_dim * 2, hidden_dim)\n        )\n\n        # Observation conditioning\n        self.obs_encoder = nn.Sequential(\n            nn.Linear(obs_dim, hidden_dim),\n            nn.Mish(),\n            nn.Linear(hidden_dim, hidden_dim)\n        )\n\n        # Downsampling path\n        self.down_blocks = nn.ModuleList()\n        self.down_convs = nn.ModuleList()\n\n        in_channels = action_dim\n        channels = [hidden_dim, hidden_dim * 2, hidden_dim * 4]\n\n        for out_channels in channels:\n            self.down_blocks.append(\n                ResidualBlock1D(in_channels, out_channels, hidden_dim)\n            )\n            self.down_convs.append(\n                nn.Conv1d(out_channels, out_channels, 3, stride=2, padding=1)\n            )\n            in_channels = out_channels\n\n        # Middle block\n        self.mid_block = ResidualBlock1D(channels[-1], channels[-1], hidden_dim)\n\n        # Upsampling path\n        self.up_blocks = nn.ModuleList()\n        self.up_convs = nn.ModuleList()\n\n        for i, out_channels in enumerate(reversed(channels[:-1])):\n            in_ch = channels[-(i+1)]\n            skip_ch = channels[-(i+2)]\n            self.up_convs.append(\n                nn.ConvTranspose1d(in_ch, in_ch, 4, stride=2, padding=1)\n            )\n            self.up_blocks.append(\n                ResidualBlock1D(in_ch + skip_ch, out_channels, hidden_dim)\n            )\n\n        # Output projection\n        self.out_conv = nn.Conv1d(channels[0], action_dim, 1)\n\n    def forward(\n        self,\n        noisy_action: torch.Tensor,\n        timestep: torch.Tensor,\n        obs: torch.Tensor\n    ) -> torch.Tensor:\n        """\n        Predict noise in action trajectory.\n\n        Args:\n            noisy_action: [batch, horizon, action_dim]\n            timestep: [batch]\n            obs: [batch, obs_dim]\n\n        Returns:\n            predicted_noise: [batch, horizon, action_dim]\n        """\n        # Embed time and observation\n        t_emb = self.time_embed(timestep)  # [B, H]\n        o_emb = self.obs_encoder(obs)  # [B, H]\n        cond = t_emb + o_emb  # [B, H]\n\n        # Reshape for 1D conv: [B, C, T]\n        x = noisy_action.transpose(1, 2)\n\n        # Downsampling\n        skip_connections = []\n        for block, conv in zip(self.down_blocks, self.down_convs):\n            x = block(x, cond)\n            skip_connections.append(x)\n            x = conv(x)\n\n        # Middle\n        x = self.mid_block(x, cond)\n\n        # Upsampling\n        for conv, block, skip in zip(\n            self.up_convs, self.up_blocks, reversed(skip_connections[:-1])\n        ):\n            x = conv(x)\n            x = torch.cat([x, skip], dim=1)\n            x = block(x, cond)\n\n        # Output\n        x = self.out_conv(x)\n\n        return x.transpose(1, 2)  # [B, T, C]\n\n\nclass ResidualBlock1D(nn.Module):\n    """Residual block with conditional normalization."""\n\n    def __init__(self, in_channels: int, out_channels: int, cond_dim: int):\n        super().__init__()\n\n        self.conv1 = nn.Conv1d(in_channels, out_channels, 3, padding=1)\n        self.conv2 = nn.Conv1d(out_channels, out_channels, 3, padding=1)\n        self.norm1 = nn.GroupNorm(8, out_channels)\n        self.norm2 = nn.GroupNorm(8, out_channels)\n\n        self.cond_proj = nn.Linear(cond_dim, out_channels * 2)\n\n        if in_channels != out_channels:\n            self.skip = nn.Conv1d(in_channels, out_channels, 1)\n        else:\n            self.skip = nn.Identity()\n\n    def forward(self, x: torch.Tensor, cond: torch.Tensor) -> torch.Tensor:\n        h = self.conv1(x)\n        h = self.norm1(h)\n\n        # Condition via scale and shift\n        scale, shift = self.cond_proj(cond).chunk(2, dim=-1)\n        h = h * (1 + scale.unsqueeze(-1)) + shift.unsqueeze(-1)\n\n        h = F.mish(h)\n        h = self.conv2(h)\n        h = self.norm2(h)\n        h = F.mish(h)\n\n        return h + self.skip(x)\n\n\nclass DiffusionPolicy:\n    """Complete diffusion policy for robot control."""\n\n    def __init__(\n        self,\n        model: ConditionalUNet1D,\n        action_dim: int = 7,\n        action_horizon: int = 16,\n        num_diffusion_steps: int = 100,\n        beta_start: float = 0.0001,\n        beta_end: float = 0.02\n    ):\n        self.model = model\n        self.action_dim = action_dim\n        self.action_horizon = action_horizon\n        self.num_steps = num_diffusion_steps\n\n        # Noise schedule\n        self.betas = torch.linspace(beta_start, beta_end, num_diffusion_steps)\n        self.alphas = 1 - self.betas\n        self.alpha_cumprod = torch.cumprod(self.alphas, dim=0)\n\n    def add_noise(\n        self,\n        action: torch.Tensor,\n        t: torch.Tensor\n    ) -> tuple[torch.Tensor, torch.Tensor]:\n        """Add noise to action at timestep t."""\n        device = action.device\n        alpha_cumprod_t = self.alpha_cumprod[t].view(-1, 1, 1).to(device)\n\n        noise = torch.randn_like(action)\n        noisy_action = (\n            torch.sqrt(alpha_cumprod_t) * action +\n            torch.sqrt(1 - alpha_cumprod_t) * noise\n        )\n\n        return noisy_action, noise\n\n    def training_loss(\n        self,\n        action: torch.Tensor,\n        obs: torch.Tensor\n    ) -> torch.Tensor:\n        """Compute training loss."""\n        batch_size = action.size(0)\n        device = action.device\n\n        # Sample random timesteps\n        t = torch.randint(0, self.num_steps, (batch_size,), device=device)\n\n        # Add noise\n        noisy_action, noise = self.add_noise(action, t)\n\n        # Predict noise\n        predicted_noise = self.model(noisy_action, t.float(), obs)\n\n        # MSE loss\n        return F.mse_loss(predicted_noise, noise)\n\n    @torch.no_grad()\n    def sample(self, obs: torch.Tensor) -> torch.Tensor:\n        """Generate action trajectory via reverse diffusion."""\n        batch_size = obs.size(0)\n        device = obs.device\n\n        # Start from pure noise\n        x = torch.randn(batch_size, self.action_horizon, self.action_dim, device=device)\n\n        # Reverse diffusion\n        for t in reversed(range(self.num_steps)):\n            t_batch = torch.full((batch_size,), t, device=device, dtype=torch.float)\n\n            # Predict noise\n            predicted_noise = self.model(x, t_batch, obs)\n\n            # Compute denoised action\n            alpha = self.alphas[t]\n            alpha_cumprod = self.alpha_cumprod[t]\n            beta = self.betas[t]\n\n            if t > 0:\n                noise = torch.randn_like(x)\n            else:\n                noise = 0\n\n            x = (\n                1 / torch.sqrt(alpha) *\n                (x - beta / torch.sqrt(1 - alpha_cumprod) * predicted_noise) +\n                torch.sqrt(beta) * noise\n            )\n\n        return x\n\n\n# Example usage\nmodel = ConditionalUNet1D(action_dim=7, action_horizon=16, obs_dim=512)\npolicy = DiffusionPolicy(model)\n\n# Training\nobs = torch.randn(32, 512)  # Batch of observations\naction = torch.randn(32, 16, 7)  # Ground truth trajectories\nloss = policy.training_loss(action, obs)\n\n# Inference\nobs = torch.randn(1, 512)\ntrajectory = policy.sample(obs)\nprint(f"Generated trajectory: {trajectory.shape}")  # [1, 16, 7]\n'})}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"flow-matching-policies",children:"Flow Matching Policies"}),"\n",(0,t.jsx)(e.h3,{id:"flow-matching-for-actions",children:"Flow Matching for Actions"}),"\n",(0,t.jsx)(e.p,{children:"Flow matching offers a simpler alternative to diffusion:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",metastring:'title="flow_matching_policy.py"',children:'"""Flow matching policy for action generation."""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass FlowMatchingPolicy(nn.Module):\n    """Flow matching for trajectory generation."""\n\n    def __init__(\n        self,\n        action_dim: int = 7,\n        action_horizon: int = 16,\n        obs_dim: int = 512,\n        hidden_dim: int = 256\n    ):\n        super().__init__()\n\n        self.action_dim = action_dim\n        self.action_horizon = action_horizon\n\n        # Velocity network\n        self.velocity_net = nn.Sequential(\n            nn.Linear(action_dim * action_horizon + obs_dim + 1, hidden_dim),\n            nn.Mish(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.Mish(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.Mish(),\n            nn.Linear(hidden_dim, action_dim * action_horizon)\n        )\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        t: torch.Tensor,\n        obs: torch.Tensor\n    ) -> torch.Tensor:\n        """\n        Predict velocity field.\n\n        Args:\n            x: [batch, horizon * action_dim] - current position\n            t: [batch, 1] - time\n            obs: [batch, obs_dim] - observation\n\n        Returns:\n            velocity: [batch, horizon * action_dim]\n        """\n        inp = torch.cat([x, t, obs], dim=-1)\n        return self.velocity_net(inp)\n\n    def training_loss(\n        self,\n        action: torch.Tensor,\n        obs: torch.Tensor\n    ) -> torch.Tensor:\n        """Compute flow matching loss."""\n        batch_size = action.size(0)\n        device = action.device\n\n        # Flatten action\n        action_flat = action.view(batch_size, -1)\n\n        # Sample time uniformly\n        t = torch.rand(batch_size, 1, device=device)\n\n        # Sample noise (source distribution)\n        noise = torch.randn_like(action_flat)\n\n        # Interpolate between noise and action\n        x_t = (1 - t) * noise + t * action_flat\n\n        # Target velocity (optimal transport)\n        target_velocity = action_flat - noise\n\n        # Predict velocity\n        predicted_velocity = self(x_t, t, obs)\n\n        return F.mse_loss(predicted_velocity, target_velocity)\n\n    @torch.no_grad()\n    def sample(\n        self,\n        obs: torch.Tensor,\n        num_steps: int = 10\n    ) -> torch.Tensor:\n        """Generate action via ODE integration."""\n        batch_size = obs.size(0)\n        device = obs.device\n\n        # Start from noise\n        x = torch.randn(\n            batch_size,\n            self.action_horizon * self.action_dim,\n            device=device\n        )\n\n        # Euler integration\n        dt = 1.0 / num_steps\n        for i in range(num_steps):\n            t = torch.full((batch_size, 1), i * dt, device=device)\n            velocity = self(x, t, obs)\n            x = x + velocity * dt\n\n        return x.view(batch_size, self.action_horizon, self.action_dim)\n'})}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"action-safety-and-constraints",children:"Action Safety and Constraints"}),"\n",(0,t.jsx)(e.h3,{id:"constraint-enforcement",children:"Constraint Enforcement"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",metastring:'title="action_constraints.py"',children:'"""Enforce safety constraints on generated actions."""\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\nclass ActionConstraintEnforcer:\n    """Enforce physical and safety constraints on actions."""\n\n    def __init__(\n        self,\n        joint_limits: list,\n        velocity_limits: list,\n        workspace_bounds: dict,\n        collision_checker=None\n    ):\n        self.joint_limits = torch.tensor(joint_limits)\n        self.velocity_limits = torch.tensor(velocity_limits)\n        self.workspace = workspace_bounds\n        self.collision_checker = collision_checker\n\n    def enforce_joint_limits(self, action: torch.Tensor) -> torch.Tensor:\n        """Clamp actions to joint limits."""\n        low = self.joint_limits[:, 0]\n        high = self.joint_limits[:, 1]\n        return torch.clamp(action, low, high)\n\n    def enforce_velocity_limits(\n        self,\n        action: torch.Tensor,\n        current_position: torch.Tensor,\n        dt: float = 0.1\n    ) -> torch.Tensor:\n        """Limit action velocity."""\n        delta = action - current_position\n        max_delta = self.velocity_limits * dt\n\n        # Clamp delta to velocity limits\n        delta = torch.clamp(delta, -max_delta, max_delta)\n\n        return current_position + delta\n\n    def enforce_workspace(\n        self,\n        ee_position: torch.Tensor\n    ) -> tuple[torch.Tensor, bool]:\n        """Enforce workspace bounds on end-effector."""\n        clamped = torch.zeros_like(ee_position)\n        clamped[0] = torch.clamp(\n            ee_position[0],\n            self.workspace[\'x_min\'],\n            self.workspace[\'x_max\']\n        )\n        clamped[1] = torch.clamp(\n            ee_position[1],\n            self.workspace[\'y_min\'],\n            self.workspace[\'y_max\']\n        )\n        clamped[2] = torch.clamp(\n            ee_position[2],\n            self.workspace[\'z_min\'],\n            self.workspace[\'z_max\']\n        )\n\n        modified = not torch.allclose(clamped, ee_position)\n        return clamped, modified\n\n    def smooth_trajectory(\n        self,\n        trajectory: torch.Tensor,\n        smoothing_factor: float = 0.5\n    ) -> torch.Tensor:\n        """Apply smoothing to trajectory."""\n        # Simple exponential moving average\n        smoothed = trajectory.clone()\n        for i in range(1, trajectory.size(0)):\n            smoothed[i] = (\n                smoothing_factor * smoothed[i-1] +\n                (1 - smoothing_factor) * trajectory[i]\n            )\n        return smoothed\n\n    def check_collision(self, action: torch.Tensor) -> bool:\n        """Check if action would cause collision."""\n        if self.collision_checker is None:\n            return False\n        return self.collision_checker(action.numpy())\n\n    def apply_all_constraints(\n        self,\n        action: torch.Tensor,\n        current_state: torch.Tensor\n    ) -> tuple[torch.Tensor, dict]:\n        """Apply all constraints and return info."""\n        info = {\n            \'original\': action.clone(),\n            \'modifications\': []\n        }\n\n        # Joint limits\n        action = self.enforce_joint_limits(action)\n        if not torch.allclose(action, info[\'original\']):\n            info[\'modifications\'].append(\'joint_limits\')\n\n        # Velocity limits\n        action = self.enforce_velocity_limits(action, current_state)\n        info[\'modifications\'].append(\'velocity_limits\')\n\n        # Collision check\n        if self.check_collision(action):\n            info[\'modifications\'].append(\'collision_stopped\')\n            action = current_state  # Stay in place\n\n        info[\'final\'] = action\n        return action, info\n\n\nclass SafeActionWrapper(nn.Module):\n    """Wrapper that enforces safety on any action generator."""\n\n    def __init__(self, policy: nn.Module, constraint_enforcer: ActionConstraintEnforcer):\n        super().__init__()\n        self.policy = policy\n        self.enforcer = constraint_enforcer\n\n    def forward(self, *args, current_state: torch.Tensor = None, **kwargs):\n        """Generate action and enforce constraints."""\n        # Get raw action from policy\n        raw_action = self.policy(*args, **kwargs)\n\n        # Handle dict output\n        if isinstance(raw_action, dict):\n            action = raw_action.get(\'action\', raw_action.get(\'position\'))\n        else:\n            action = raw_action\n\n        # Enforce constraints\n        if current_state is not None:\n            action, info = self.enforcer.apply_all_constraints(action, current_state)\n        else:\n            action = self.enforcer.enforce_joint_limits(action)\n            info = {}\n\n        if isinstance(raw_action, dict):\n            raw_action[\'action\'] = action\n            raw_action[\'constraint_info\'] = info\n            return raw_action\n        return action\n'})}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"exercise-1-implement-action-transformer",children:"Exercise 1: Implement Action Transformer"}),"\n",(0,t.jsxs)(e.admonition,{title:"Exercise 1: Action Prediction Head",type:"tip",children:[(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Objective"}),": Build a transformer-based action prediction module."]}),(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Steps"}),":"]}),(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Create input projections for vision, language, proprioception"}),"\n",(0,t.jsx)(e.li,{children:"Implement cross-attention between action queries and context"}),"\n",(0,t.jsx)(e.li,{children:"Add separate heads for position, rotation, gripper"}),"\n",(0,t.jsx)(e.li,{children:"Train on simulated pick-and-place trajectories"}),"\n",(0,t.jsx)(e.li,{children:"Evaluate action prediction accuracy"}),"\n"]}),(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Verification"}),":"]}),(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"model = ActionTransformer()\naction = model(vision_feat, lang_feat, proprio)\nassert action['position'].shape == (batch, horizon, 3)\nassert action['rotation'].shape == (batch, horizon, 4)\n"})}),(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Time Estimate"}),": 60 minutes"]})]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"exercise-2-train-diffusion-policy",children:"Exercise 2: Train Diffusion Policy"}),"\n",(0,t.jsxs)(e.admonition,{title:"Exercise 2: Diffusion for Manipulation",type:"tip",children:[(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Objective"}),": Train a diffusion policy on manipulation demonstrations."]}),(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Steps"}),":"]}),(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Implement the noise schedule and forward process"}),"\n",(0,t.jsx)(e.li,{children:"Build the conditional U-Net denoiser"}),"\n",(0,t.jsx)(e.li,{children:"Implement the training loop with MSE loss"}),"\n",(0,t.jsx)(e.li,{children:"Add DDPM sampling for inference"}),"\n",(0,t.jsx)(e.li,{children:"Visualize generated trajectories"}),"\n"]}),(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Training Tips"}),":"]}),(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Use learning rate 1e-4"}),"\n",(0,t.jsx)(e.li,{children:"Train for 100k steps minimum"}),"\n",(0,t.jsx)(e.li,{children:"Start with action_horizon=8"}),"\n"]}),(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Time Estimate"}),": 90 minutes"]})]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"exercise-3-safe-action-execution",children:"Exercise 3: Safe Action Execution"}),"\n",(0,t.jsxs)(e.admonition,{title:"Exercise 3: Constraint-Aware Actions",type:"tip",children:[(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Objective"}),": Implement safety constraints for action execution."]}),(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Steps"}),":"]}),(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Define joint limits and velocity bounds"}),"\n",(0,t.jsx)(e.li,{children:"Implement workspace enforcement"}),"\n",(0,t.jsx)(e.li,{children:"Add trajectory smoothing"}),"\n",(0,t.jsx)(e.li,{children:"Create collision checking interface"}),"\n",(0,t.jsx)(e.li,{children:"Test with intentionally unsafe predictions"}),"\n"]}),(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Safety Requirements"}),":"]}),(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Never exceed joint velocity by more than 10%"}),"\n",(0,t.jsx)(e.li,{children:"Stop immediately if collision predicted"}),"\n",(0,t.jsx)(e.li,{children:"Log all constraint violations"}),"\n"]}),(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Time Estimate"}),": 45 minutes"]})]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(e.p,{children:"In this chapter, you learned:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Action Spaces"}),": Discrete, continuous, and hybrid representations"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Transformer Actions"}),": Cross-attention for action prediction from context"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Diffusion Policies"}),": Generating smooth, multi-modal trajectories"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Flow Matching"}),": Simpler alternative to diffusion for action generation"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Safety Constraints"}),": Enforcing limits, workspace bounds, and collision avoidance"]}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"Action generation is where VLA models produce their output - the robot motions that accomplish tasks. The choice of action representation, policy architecture, and constraint enforcement all significantly impact what behaviors the robot can express and how safely it operates."}),"\n",(0,t.jsxs)(e.p,{children:["Next, explore ",(0,t.jsx)(e.a,{href:"/docs/module-4/end-to-end-vla",children:"End-to-End VLA Systems"})," to see how all components come together into complete, deployable systems."]}),"\n",(0,t.jsx)(e.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.a,{href:"https://arxiv.org/abs/2303.04137",children:"Diffusion Policy Paper"})," - Chi et al."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.a,{href:"https://arxiv.org/abs/2304.13705",children:"Action Chunking with Transformers"})," - ACT Policy"]}),"\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"https://arxiv.org/abs/2310.07842",children:"Flow Matching for Robotics"})}),"\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"https://arxiv.org/abs/2212.06817",children:"RT-1: Robotics Transformer"})}),"\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"https://arxiv.org/abs/2109.00137",children:"Implicit Behavioral Cloning"})}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>r,x:()=>c});var o=i(6540);const t={},s=o.createContext(t);function r(n){const e=o.useContext(s);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function c(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:r(n.components),o.createElement(s.Provider,{value:e},n.children)}}}]);