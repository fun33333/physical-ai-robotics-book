"use strict";(globalThis.webpackChunkbook_website=globalThis.webpackChunkbook_website||[]).push([[7188],{8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var i=t(6540);const s={},o=i.createContext(s);function r(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),i.createElement(o.Provider,{value:n},e.children)}},9887:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-4/language-integration","title":"Language Model Integration","description":"Connecting large language models to robotics: instruction following, planning, and reasoning.","source":"@site/docs/module-4/language-integration.md","sourceDirName":"module-4","slug":"/module-4/language-integration","permalink":"/physical-ai-robotics-book/docs/module-4/language-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/fun33333/physical-ai-robotics-book/tree/main/book-website/docs/module-4/language-integration.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Language Model Integration","sidebar_position":4,"description":"Connecting large language models to robotics: instruction following, planning, and reasoning."},"sidebar":"tutorialSidebar","previous":{"title":"Vision Models for Robotics","permalink":"/physical-ai-robotics-book/docs/module-4/vision-models"},"next":{"title":"Action Generation","permalink":"/physical-ai-robotics-book/docs/module-4/action-generation"}}');var s=t(4848),o=t(8453);const r={title:"Language Model Integration",sidebar_position:4,description:"Connecting large language models to robotics: instruction following, planning, and reasoning."},a="Language Model Integration",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Integration Patterns",id:"integration-patterns",level:2},{value:"Overview of Approaches",id:"overview-of-approaches",level:3},{value:"Pattern Comparison",id:"pattern-comparison",level:3},{value:"High-Level Planning",id:"high-level-planning",level:2},{value:"Task Decomposition",id:"task-decomposition",level:3},{value:"Scene-Aware Planning",id:"scene-aware-planning",level:3},{value:"Code Generation",id:"code-generation",level:2},{value:"Robot Code Synthesis",id:"robot-code-synthesis",level:3},{value:"Prompt Engineering for Robotics",id:"prompt-engineering-for-robotics",level:2},{value:"Effective Prompt Design",id:"effective-prompt-design",level:3},{value:"Few-Shot Prompting",id:"few-shot-prompting",level:3},{value:"Safety and Uncertainty Handling",id:"safety-and-uncertainty-handling",level:2},{value:"Confidence Estimation",id:"confidence-estimation",level:3},{value:"Human-in-the-Loop Verification",id:"human-in-the-loop-verification",level:3},{value:"ROS 2 Integration",id:"ros-2-integration",level:2},{value:"LLM Service Node",id:"llm-service-node",level:3},{value:"Action Client Integration",id:"action-client-integration",level:3},{value:"Exercise 1: Build a Task Planner",id:"exercise-1-build-a-task-planner",level:2},{value:"Exercise 2: Object Grounding",id:"exercise-2-object-grounding",level:2},{value:"Exercise 3: Safe Execution Pipeline",id:"exercise-3-safe-execution-pipeline",level:2},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"language-model-integration",children:"Language Model Integration"})}),"\n",(0,s.jsx)(n.p,{children:"Large language models (LLMs) offer capabilities that dramatically enhance robot intelligence - from understanding complex instructions to reasoning about plans and explaining decisions. This chapter explores how to effectively integrate LLMs into robotics systems, covering different integration patterns, prompt engineering techniques, and strategies for handling uncertainties in safety-critical applications."}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"In this section, you will:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understand different LLM integration patterns for robotics"}),"\n",(0,s.jsx)(n.li,{children:"Implement instruction parsing and grounding"}),"\n",(0,s.jsx)(n.li,{children:"Build LLM-based task planners"}),"\n",(0,s.jsx)(n.li,{children:"Design effective prompts for robot control"}),"\n",(0,s.jsx)(n.li,{children:"Handle LLM uncertainties and failures safely"}),"\n",(0,s.jsx)(n.li,{children:"Create conversational robot interfaces"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Familiarity with transformer architecture concepts"}),"\n",(0,s.jsx)(n.li,{children:"Python programming experience"}),"\n",(0,s.jsx)(n.li,{children:"Understanding of ROS 2 basics"}),"\n",(0,s.jsx)(n.li,{children:"API access to an LLM (OpenAI, Anthropic, or local models)"}),"\n",(0,s.jsxs)(n.li,{children:["Completed ",(0,s.jsx)(n.a,{href:"/docs/module-4/vla-foundations",children:"VLA Foundations"})]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"integration-patterns",children:"Integration Patterns"}),"\n",(0,s.jsx)(n.h3,{id:"overview-of-approaches",children:"Overview of Approaches"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                LLM Integration Patterns                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                  \u2502\n\u2502   Pattern 1: High-Level Planner                                 \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502   \u2502   Natural   \u2502\u2500\u2500\u2500\u2500\u25b6\u2502    LLM      \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Skill     \u2502      \u2502\n\u2502   \u2502   Language  \u2502     \u2502   Planner   \u2502     \u2502   Sequence  \u2502      \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502                                                                  \u2502\n\u2502   Pattern 2: Code Generation                                    \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502   \u2502   Task      \u2502\u2500\u2500\u2500\u2500\u25b6\u2502    LLM      \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Python    \u2502      \u2502\n\u2502   \u2502   Description\u2502    \u2502   Coder     \u2502     \u2502   Code      \u2502      \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502                                                                  \u2502\n\u2502   Pattern 3: Reasoning Engine                                   \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502   \u2502   Scene +   \u2502\u2500\u2500\u2500\u2500\u25b6\u2502    LLM      \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Decision  \u2502      \u2502\n\u2502   \u2502   Query     \u2502     \u2502   Reasoner  \u2502     \u2502   + Explain \u2502      \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502                                                                  \u2502\n\u2502   Pattern 4: End-to-End VLA                                     \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502   \u2502   Image +   \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  Multimodal \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Actions   \u2502      \u2502\n\u2502   \u2502   Language  \u2502     \u2502    VLM      \u2502     \u2502   Directly  \u2502      \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.h3,{id:"pattern-comparison",children:"Pattern Comparison"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Pattern"}),(0,s.jsx)(n.th,{children:"Latency"}),(0,s.jsx)(n.th,{children:"Flexibility"}),(0,s.jsx)(n.th,{children:"Safety"}),(0,s.jsx)(n.th,{children:"Use Case"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"High-Level Planner"})}),(0,s.jsx)(n.td,{children:"High (1-5s)"}),(0,s.jsx)(n.td,{children:"High"}),(0,s.jsx)(n.td,{children:"Good"}),(0,s.jsx)(n.td,{children:"Complex multi-step tasks"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Code Generation"})}),(0,s.jsx)(n.td,{children:"High (2-10s)"}),(0,s.jsx)(n.td,{children:"Very High"}),(0,s.jsx)(n.td,{children:"Requires review"}),(0,s.jsx)(n.td,{children:"Novel tasks, scripting"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Reasoning Engine"})}),(0,s.jsx)(n.td,{children:"Medium (0.5-2s)"}),(0,s.jsx)(n.td,{children:"Medium"}),(0,s.jsx)(n.td,{children:"Good"}),(0,s.jsx)(n.td,{children:"Decision support, QA"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"End-to-End VLA"})}),(0,s.jsx)(n.td,{children:"Low (50-200ms)"}),(0,s.jsx)(n.td,{children:"Low"}),(0,s.jsx)(n.td,{children:"Learned"}),(0,s.jsx)(n.td,{children:"Real-time control"})]})]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"high-level-planning",children:"High-Level Planning"}),"\n",(0,s.jsx)(n.h3,{id:"task-decomposition",children:"Task Decomposition"}),"\n",(0,s.jsx)(n.p,{children:"LLMs excel at breaking complex tasks into manageable steps:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:'title="task_planner.py"',children:'"""LLM-based task planner for robotics."""\nimport json\nfrom dataclasses import dataclass\nfrom typing import List, Optional\nfrom openai import OpenAI\n\n@dataclass\nclass RobotSkill:\n    """Available robot primitive skill."""\n    name: str\n    description: str\n    parameters: dict\n    preconditions: List[str]\n    effects: List[str]\n\n@dataclass\nclass PlanStep:\n    """Single step in task plan."""\n    skill: str\n    parameters: dict\n    description: str\n\nclass TaskPlanner:\n    """LLM-based task planner."""\n\n    def __init__(self, model: str = "gpt-4"):\n        self.client = OpenAI()\n        self.model = model\n        self.skills = self._define_skills()\n\n    def _define_skills(self) -> List[RobotSkill]:\n        """Define available robot skills."""\n        return [\n            RobotSkill(\n                name="navigate_to",\n                description="Move robot base to a location",\n                parameters={"location": "string - named location or coordinates"},\n                preconditions=["robot is not carrying fragile item"],\n                effects=["robot is at location"]\n            ),\n            RobotSkill(\n                name="pick_object",\n                description="Pick up an object with the gripper",\n                parameters={"object": "string - object name"},\n                preconditions=["robot is near object", "gripper is empty"],\n                effects=["robot is holding object", "object is not on surface"]\n            ),\n            RobotSkill(\n                name="place_object",\n                description="Place held object at a location",\n                parameters={"location": "string - where to place"},\n                preconditions=["robot is holding object", "robot is near location"],\n                effects=["object is at location", "gripper is empty"]\n            ),\n            RobotSkill(\n                name="open_gripper",\n                description="Open the robot gripper",\n                parameters={},\n                preconditions=[],\n                effects=["gripper is open"]\n            ),\n            RobotSkill(\n                name="close_gripper",\n                description="Close the robot gripper",\n                parameters={},\n                preconditions=["gripper is open"],\n                effects=["gripper is closed"]\n            ),\n            RobotSkill(\n                name="look_at",\n                description="Point camera at target",\n                parameters={"target": "string - what to look at"},\n                preconditions=[],\n                effects=["camera is viewing target"]\n            ),\n        ]\n\n    def _build_system_prompt(self) -> str:\n        """Build system prompt with skill definitions."""\n        skills_desc = "\\n".join([\n            f"- {s.name}: {s.description}\\n"\n            f"  Parameters: {json.dumps(s.parameters)}\\n"\n            f"  Preconditions: {s.preconditions}\\n"\n            f"  Effects: {s.effects}"\n            for s in self.skills\n        ])\n\n        return f"""You are a robot task planner. Given a natural language instruction,\ndecompose it into a sequence of robot skills.\n\nAvailable skills:\n{skills_desc}\n\nOutput a JSON array of steps, each with:\n- "skill": skill name\n- "parameters": parameter values\n- "description": human-readable description\n\nOnly use the skills listed above. Ensure preconditions are met before each step.\nIf the task is impossible with available skills, explain why."""\n\n    def plan(self, instruction: str, scene_context: str = "") -> List[PlanStep]:\n        """Generate plan from natural language instruction."""\n        user_prompt = f"Instruction: {instruction}"\n        if scene_context:\n            user_prompt += f"\\n\\nCurrent scene: {scene_context}"\n\n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=[\n                {"role": "system", "content": self._build_system_prompt()},\n                {"role": "user", "content": user_prompt}\n            ],\n            response_format={"type": "json_object"},\n            temperature=0.1  # Low temperature for consistent planning\n        )\n\n        result = json.loads(response.choices[0].message.content)\n\n        steps = []\n        for step_data in result.get("steps", []):\n            steps.append(PlanStep(\n                skill=step_data["skill"],\n                parameters=step_data.get("parameters", {}),\n                description=step_data.get("description", "")\n            ))\n\n        return steps\n\n\n# Example usage\nplanner = TaskPlanner()\n\ninstruction = "Put the red cup on the kitchen table"\nscene = "Robot is at charging station. Red cup is on the counter. Kitchen table is 3m away."\n\nplan = planner.plan(instruction, scene)\nfor i, step in enumerate(plan):\n    print(f"{i+1}. {step.skill}({step.parameters}) - {step.description}")\n\n# Output:\n# 1. navigate_to({\'location\': \'counter\'}) - Move to the counter where the cup is\n# 2. look_at({\'target\': \'red cup\'}) - Locate the red cup visually\n# 3. pick_object({\'object\': \'red cup\'}) - Pick up the red cup\n# 4. navigate_to({\'location\': \'kitchen table\'}) - Move to the kitchen table\n# 5. place_object({\'location\': \'kitchen table\'}) - Place the cup on the table\n'})}),"\n",(0,s.jsx)(n.h3,{id:"scene-aware-planning",children:"Scene-Aware Planning"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:'title="scene_aware_planner.py"',children:'"""Scene-aware planning with visual context."""\nimport base64\nfrom pathlib import Path\n\nclass SceneAwarePlanner(TaskPlanner):\n    """Planner that incorporates visual scene understanding."""\n\n    def __init__(self, model: str = "gpt-4-vision-preview"):\n        super().__init__(model)\n\n    def _encode_image(self, image_path: str) -> str:\n        """Encode image as base64."""\n        with open(image_path, "rb") as f:\n            return base64.b64encode(f.read()).decode("utf-8")\n\n    def plan_with_image(self, instruction: str,\n                        image_path: str) -> List[PlanStep]:\n        """Generate plan using image context."""\n        image_b64 = self._encode_image(image_path)\n\n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=[\n                {"role": "system", "content": self._build_system_prompt()},\n                {\n                    "role": "user",\n                    "content": [\n                        {\n                            "type": "text",\n                            "text": f"Instruction: {instruction}\\n\\n"\n                                   "The image shows the current scene. "\n                                   "Identify relevant objects and plan accordingly."\n                        },\n                        {\n                            "type": "image_url",\n                            "image_url": {\n                                "url": f"data:image/jpeg;base64,{image_b64}"\n                            }\n                        }\n                    ]\n                }\n            ],\n            max_tokens=1000\n        )\n\n        # Parse response and return plan steps\n        # (Implementation similar to base class)\n        pass\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"code-generation",children:"Code Generation"}),"\n",(0,s.jsx)(n.h3,{id:"robot-code-synthesis",children:"Robot Code Synthesis"}),"\n",(0,s.jsx)(n.p,{children:"LLMs can generate executable robot code for novel tasks:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:'title="code_generator.py"',children:'"""LLM-based code generation for robot control."""\nimport ast\nimport textwrap\nfrom typing import Callable, Dict\n\nclass RobotCodeGenerator:\n    """Generate and execute robot control code."""\n\n    def __init__(self, robot_api: object):\n        self.client = OpenAI()\n        self.robot = robot_api\n        self.safe_globals = self._build_safe_globals()\n\n    def _build_safe_globals(self) -> Dict:\n        """Build sandboxed execution environment."""\n        return {\n            # Math operations\n            "abs": abs,\n            "min": min,\n            "max": max,\n            "sum": sum,\n            "round": round,\n\n            # Robot API (read-only introspection)\n            "robot": self.robot,\n            "get_position": self.robot.get_position,\n            "get_joint_states": self.robot.get_joint_states,\n            "get_gripper_state": self.robot.get_gripper_state,\n\n            # Safe robot commands (with limits)\n            "move_to": self._safe_move_to,\n            "pick": self._safe_pick,\n            "place": self._safe_place,\n            "wait": self._safe_wait,\n\n            # Perception\n            "detect_objects": self.robot.detect_objects,\n            "get_object_pose": self.robot.get_object_pose,\n        }\n\n    def _safe_move_to(self, x: float, y: float, z: float):\n        """Move with workspace limits."""\n        # Enforce workspace bounds\n        x = max(-1.0, min(1.0, x))\n        y = max(-1.0, min(1.0, y))\n        z = max(0.0, min(1.5, z))\n        return self.robot.move_to(x, y, z)\n\n    def _safe_pick(self, object_name: str):\n        """Pick with validation."""\n        if not self.robot.is_object_reachable(object_name):\n            raise ValueError(f"Object {object_name} not reachable")\n        return self.robot.pick(object_name)\n\n    def _safe_place(self, location):\n        """Place with validation."""\n        if not self.robot.is_holding_object():\n            raise ValueError("Not holding any object")\n        return self.robot.place(location)\n\n    def _safe_wait(self, seconds: float):\n        """Wait with maximum limit."""\n        seconds = min(seconds, 30.0)  # Max 30 second wait\n        return self.robot.wait(seconds)\n\n    def generate_code(self, task: str, scene_description: str) -> str:\n        """Generate Python code for task."""\n        system_prompt = """You are a robot programming assistant. Generate Python code\nto accomplish the given task using the available robot API.\n\nAvailable functions:\n- get_position() -> (x, y, z): Get current end-effector position\n- get_joint_states() -> dict: Get all joint positions\n- detect_objects() -> list: Get detected objects with positions\n- get_object_pose(name) -> (x, y, z, qx, qy, qz, qw): Get object pose\n- move_to(x, y, z): Move end-effector to position\n- pick(object_name): Pick up named object\n- place(location): Place held object at location\n- wait(seconds): Wait for specified time\n\nGenerate clean, safe Python code. Include error handling.\nOnly use the functions listed above. Do not import any modules.\nWrap your code in ```python``` blocks."""\n\n        response = self.client.chat.completions.create(\n            model="gpt-4",\n            messages=[\n                {"role": "system", "content": system_prompt},\n                {"role": "user", "content": f"Task: {task}\\n\\nScene: {scene_description}"}\n            ],\n            temperature=0.1\n        )\n\n        # Extract code from response\n        content = response.choices[0].message.content\n        code_blocks = content.split("```python")\n        if len(code_blocks) > 1:\n            code = code_blocks[1].split("```")[0]\n            return code.strip()\n        return ""\n\n    def validate_code(self, code: str) -> tuple[bool, str]:\n        """Validate generated code for safety."""\n        try:\n            tree = ast.parse(code)\n        except SyntaxError as e:\n            return False, f"Syntax error: {e}"\n\n        # Check for forbidden operations\n        forbidden = {"import", "exec", "eval", "open", "__"}\n        for node in ast.walk(tree):\n            if isinstance(node, ast.Import) or isinstance(node, ast.ImportFrom):\n                return False, "Import statements not allowed"\n            if isinstance(node, ast.Name) and any(f in node.id for f in forbidden):\n                return False, f"Forbidden identifier: {node.id}"\n            if isinstance(node, ast.Call):\n                if isinstance(node.func, ast.Name):\n                    if node.func.id in ["exec", "eval", "compile"]:\n                        return False, f"Forbidden function: {node.func.id}"\n\n        return True, "Code validated successfully"\n\n    def execute_code(self, code: str, dry_run: bool = False) -> dict:\n        """Execute validated code."""\n        is_valid, message = self.validate_code(code)\n        if not is_valid:\n            return {"success": False, "error": message}\n\n        if dry_run:\n            return {"success": True, "message": "Dry run - code validated", "code": code}\n\n        try:\n            exec(code, self.safe_globals)\n            return {"success": True, "message": "Execution completed"}\n        except Exception as e:\n            return {"success": False, "error": str(e)}\n\n\n# Example usage\ngenerator = RobotCodeGenerator(robot_api)\n\ntask = "Stack the blocks by size, largest on bottom"\nscene = "Three blocks detected: small_block at (0.3, 0.1, 0.05), medium_block at (0.4, 0.2, 0.05), large_block at (0.2, 0.3, 0.05)"\n\ncode = generator.generate_code(task, scene)\nprint("Generated code:")\nprint(code)\n\n# Validate before execution\nresult = generator.execute_code(code, dry_run=True)\nif result["success"]:\n    result = generator.execute_code(code)\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"prompt-engineering-for-robotics",children:"Prompt Engineering for Robotics"}),"\n",(0,s.jsx)(n.h3,{id:"effective-prompt-design",children:"Effective Prompt Design"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:'title="prompt_templates.py"',children:'"""Prompt templates for robotics applications."""\n\n# Task planning prompt\nPLANNING_PROMPT = """You are planning actions for a robot assistant.\n\n## Robot Capabilities\n{capabilities}\n\n## Current State\n- Location: {robot_location}\n- Gripper: {gripper_state}\n- Holding: {held_object}\n\n## Scene Objects\n{scene_objects}\n\n## Task\n{task}\n\n## Instructions\n1. Break the task into steps using only available capabilities\n2. Ensure each step\'s preconditions are met\n3. Consider failure cases and recovery\n4. Output as JSON with format: {{"steps": [...]}}\n\nGenerate a safe, efficient plan:"""\n\n# Object grounding prompt\nGROUNDING_PROMPT = """Given a natural language description and a list of detected objects,\nidentify which object(s) the description refers to.\n\n## Detected Objects\n{objects}\n\n## Description\n"{description}"\n\n## Instructions\nReturn a JSON object with:\n- "matches": list of object IDs that match the description\n- "confidence": float 0-1 indicating certainty\n- "reasoning": brief explanation\n\nIf no objects match, return empty matches list with explanation."""\n\n# Error recovery prompt\nRECOVERY_PROMPT = """A robot action has failed. Help diagnose and recover.\n\n## Attempted Action\n{action}\n\n## Error\n{error}\n\n## Current State\n{state}\n\n## Available Recovery Options\n{recovery_options}\n\nAnalyze the failure and suggest recovery steps. Consider:\n1. What likely caused the failure?\n2. Can the action be retried?\n3. Are alternative approaches available?\n4. Should the task be aborted?\n\nRespond with JSON: {{"diagnosis": "...", "recovery_plan": [...], "should_abort": bool}}"""\n\n\nclass PromptBuilder:\n    """Build prompts for robot LLM queries."""\n\n    @staticmethod\n    def build_planning_prompt(\n        capabilities: list,\n        robot_state: dict,\n        scene: dict,\n        task: str\n    ) -> str:\n        """Build task planning prompt."""\n        cap_str = "\\n".join(f"- {c[\'name\']}: {c[\'description\']}" for c in capabilities)\n        obj_str = "\\n".join(f"- {o[\'name\']} at {o[\'position\']}" for o in scene.get("objects", []))\n\n        return PLANNING_PROMPT.format(\n            capabilities=cap_str,\n            robot_location=robot_state.get("location", "unknown"),\n            gripper_state=robot_state.get("gripper", "unknown"),\n            held_object=robot_state.get("holding", "nothing"),\n            scene_objects=obj_str,\n            task=task\n        )\n\n    @staticmethod\n    def build_grounding_prompt(objects: list, description: str) -> str:\n        """Build object grounding prompt."""\n        obj_str = "\\n".join(\n            f"- ID: {o[\'id\']}, Type: {o[\'type\']}, "\n            f"Color: {o.get(\'color\', \'unknown\')}, "\n            f"Position: {o[\'position\']}"\n            for o in objects\n        )\n\n        return GROUNDING_PROMPT.format(\n            objects=obj_str,\n            description=description\n        )\n'})}),"\n",(0,s.jsx)(n.h3,{id:"few-shot-prompting",children:"Few-Shot Prompting"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:'title="few_shot_examples.py"',children:'"""Few-shot examples for robotics tasks."""\n\nPICK_PLACE_EXAMPLES = [\n    {\n        "instruction": "Put the apple in the bowl",\n        "scene": "apple at (0.3, 0.1, 0.05), bowl at (0.5, 0.2, 0.02)",\n        "plan": [\n            {"skill": "navigate_to", "params": {"location": "apple"}},\n            {"skill": "pick_object", "params": {"object": "apple"}},\n            {"skill": "navigate_to", "params": {"location": "bowl"}},\n            {"skill": "place_object", "params": {"location": "bowl"}}\n        ]\n    },\n    {\n        "instruction": "Move all the cups to the tray",\n        "scene": "cup_1 at (0.2, 0.1, 0.05), cup_2 at (0.4, 0.1, 0.05), tray at (0.6, 0.3, 0.01)",\n        "plan": [\n            {"skill": "navigate_to", "params": {"location": "cup_1"}},\n            {"skill": "pick_object", "params": {"object": "cup_1"}},\n            {"skill": "navigate_to", "params": {"location": "tray"}},\n            {"skill": "place_object", "params": {"location": "tray"}},\n            {"skill": "navigate_to", "params": {"location": "cup_2"}},\n            {"skill": "pick_object", "params": {"object": "cup_2"}},\n            {"skill": "navigate_to", "params": {"location": "tray"}},\n            {"skill": "place_object", "params": {"location": "tray"}}\n        ]\n    }\n]\n\ndef build_few_shot_prompt(examples: list, new_instruction: str, new_scene: str) -> str:\n    """Build few-shot prompt with examples."""\n    prompt = "Here are examples of robot task planning:\\n\\n"\n\n    for i, ex in enumerate(examples):\n        prompt += f"Example {i+1}:\\n"\n        prompt += f"Instruction: {ex[\'instruction\']}\\n"\n        prompt += f"Scene: {ex[\'scene\']}\\n"\n        prompt += f"Plan: {json.dumps(ex[\'plan\'], indent=2)}\\n\\n"\n\n    prompt += "Now plan for this task:\\n"\n    prompt += f"Instruction: {new_instruction}\\n"\n    prompt += f"Scene: {new_scene}\\n"\n    prompt += "Plan:"\n\n    return prompt\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"safety-and-uncertainty-handling",children:"Safety and Uncertainty Handling"}),"\n",(0,s.jsx)(n.h3,{id:"confidence-estimation",children:"Confidence Estimation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:'title="confidence_estimation.py"',children:'"""Estimate and handle LLM uncertainty."""\nimport numpy as np\nfrom typing import List, Tuple\n\nclass ConfidenceEstimator:\n    """Estimate confidence in LLM outputs."""\n\n    def __init__(self, client, model: str = "gpt-4"):\n        self.client = client\n        self.model = model\n\n    def get_multiple_responses(self, prompt: str, n: int = 5) -> List[str]:\n        """Get multiple responses for consistency check."""\n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=[{"role": "user", "content": prompt}],\n            n=n,\n            temperature=0.7\n        )\n        return [choice.message.content for choice in response.choices]\n\n    def estimate_plan_confidence(self, prompt: str) -> Tuple[dict, float]:\n        """Estimate confidence through self-consistency."""\n        responses = self.get_multiple_responses(prompt, n=5)\n\n        # Parse all responses as plans\n        plans = []\n        for resp in responses:\n            try:\n                plan = json.loads(resp)\n                plans.append(plan)\n            except json.JSONDecodeError:\n                continue\n\n        if not plans:\n            return None, 0.0\n\n        # Check consistency of first steps\n        first_steps = [p.get("steps", [{}])[0].get("skill") for p in plans]\n        most_common = max(set(first_steps), key=first_steps.count)\n        consistency = first_steps.count(most_common) / len(first_steps)\n\n        # Return most consistent plan\n        for plan in plans:\n            if plan.get("steps", [{}])[0].get("skill") == most_common:\n                return plan, consistency\n\n        return plans[0], consistency\n\n    def get_explicit_confidence(self, plan: dict) -> dict:\n        """Ask LLM to rate its own confidence."""\n        prompt = f"""Rate your confidence in each step of this robot plan.\n\nPlan: {json.dumps(plan, indent=2)}\n\nFor each step, rate:\n- feasibility (0-1): Can the robot physically do this?\n- safety (0-1): Is this step safe?\n- likelihood (0-1): Will this achieve the intended effect?\n\nOutput JSON with confidence ratings for each step."""\n\n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=[{"role": "user", "content": prompt}],\n            response_format={"type": "json_object"}\n        )\n\n        return json.loads(response.choices[0].message.content)\n\n\nclass SafetyChecker:\n    """Verify safety of LLM-generated plans."""\n\n    def __init__(self, workspace_bounds: dict, forbidden_regions: list):\n        self.bounds = workspace_bounds\n        self.forbidden = forbidden_regions\n\n    def check_plan_safety(self, plan: List[dict]) -> Tuple[bool, List[str]]:\n        """Check if plan is safe to execute."""\n        issues = []\n\n        for i, step in enumerate(plan):\n            skill = step.get("skill")\n            params = step.get("parameters", {})\n\n            # Check workspace bounds\n            if skill == "move_to":\n                pos = params.get("position", [0, 0, 0])\n                if not self._in_workspace(pos):\n                    issues.append(f"Step {i}: Position {pos} outside workspace")\n\n            # Check forbidden regions\n            if skill in ["navigate_to", "move_to"]:\n                location = params.get("location") or params.get("position")\n                if self._in_forbidden_region(location):\n                    issues.append(f"Step {i}: Location in forbidden region")\n\n            # Check for dangerous operations\n            if skill == "pick_object":\n                obj = params.get("object", "")\n                if "human" in obj.lower() or "person" in obj.lower():\n                    issues.append(f"Step {i}: Cannot pick up humans")\n\n        return len(issues) == 0, issues\n\n    def _in_workspace(self, position: list) -> bool:\n        """Check if position is within workspace."""\n        x, y, z = position\n        return (self.bounds["x_min"] <= x <= self.bounds["x_max"] and\n                self.bounds["y_min"] <= y <= self.bounds["y_max"] and\n                self.bounds["z_min"] <= z <= self.bounds["z_max"])\n\n    def _in_forbidden_region(self, location) -> bool:\n        """Check if location is in forbidden region."""\n        # Simplified check - implement actual geometry\n        return False\n'})}),"\n",(0,s.jsx)(n.h3,{id:"human-in-the-loop-verification",children:"Human-in-the-Loop Verification"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:'title="human_verification.py"',children:"\"\"\"Human-in-the-loop verification for critical actions.\"\"\"\nimport rclpy\nfrom rclpy.node import Node\nfrom std_srvs.srv import Trigger\nfrom std_msgs.msg import String\n\nclass HumanVerificationNode(Node):\n    \"\"\"Request human verification for uncertain actions.\"\"\"\n\n    def __init__(self):\n        super().__init__('human_verification')\n\n        # Parameters\n        self.declare_parameter('confidence_threshold', 0.8)\n        self.declare_parameter('always_verify_skills', ['pick_object', 'place_object'])\n\n        # Service for verification requests\n        self.verify_srv = self.create_service(\n            Trigger, '/robot/request_verification',\n            self.verification_callback\n        )\n\n        # Publisher for verification requests to UI\n        self.request_pub = self.create_publisher(\n            String, '/ui/verification_request', 10\n        )\n\n        # Subscriber for verification responses\n        self.response_sub = self.create_subscription(\n            String, '/ui/verification_response',\n            self.response_callback, 10\n        )\n\n        self.pending_verification = None\n        self.verification_result = None\n\n    def request_verification(self, action: dict, confidence: float) -> bool:\n        \"\"\"Request human verification for an action.\"\"\"\n        threshold = self.get_parameter('confidence_threshold').value\n        always_verify = self.get_parameter('always_verify_skills').value\n\n        needs_verification = (\n            confidence < threshold or\n            action.get('skill') in always_verify\n        )\n\n        if not needs_verification:\n            return True  # Auto-approve\n\n        # Send verification request\n        request_msg = String()\n        request_msg.data = json.dumps({\n            'action': action,\n            'confidence': confidence,\n            'reason': 'Low confidence' if confidence < threshold else 'Requires verification'\n        })\n        self.request_pub.publish(request_msg)\n\n        # Wait for response (with timeout)\n        self.pending_verification = action\n        self.verification_result = None\n\n        timeout = 30.0  # seconds\n        start = self.get_clock().now()\n        while self.verification_result is None:\n            rclpy.spin_once(self, timeout_sec=0.1)\n            if (self.get_clock().now() - start).nanoseconds > timeout * 1e9:\n                self.get_logger().warn('Verification timeout - rejecting action')\n                return False\n\n        return self.verification_result\n\n    def response_callback(self, msg):\n        \"\"\"Handle verification response from human.\"\"\"\n        response = json.loads(msg.data)\n        self.verification_result = response.get('approved', False)\n\n        if not self.verification_result:\n            self.get_logger().info(\n                f\"Action rejected: {response.get('reason', 'No reason given')}\"\n            )\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,s.jsx)(n.h3,{id:"llm-service-node",children:"LLM Service Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:'title="llm_service_node.py"',children:'"""ROS 2 service node for LLM queries."""\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.callback_groups import ReentrantCallbackGroup\nfrom std_msgs.msg import String\nfrom example_interfaces.srv import SetBool\n\nclass LLMServiceNode(Node):\n    """ROS 2 node providing LLM services."""\n\n    def __init__(self):\n        super().__init__(\'llm_service\')\n\n        self.callback_group = ReentrantCallbackGroup()\n\n        # Initialize planner\n        self.planner = TaskPlanner()\n        self.confidence_estimator = ConfidenceEstimator(\n            self.planner.client, self.planner.model\n        )\n\n        # Planning service\n        self.plan_srv = self.create_service(\n            PlanTask, \'/llm/plan_task\',\n            self.plan_callback,\n            callback_group=self.callback_group\n        )\n\n        # Grounding service\n        self.ground_srv = self.create_service(\n            GroundObject, \'/llm/ground_object\',\n            self.ground_callback,\n            callback_group=self.callback_group\n        )\n\n        # Scene context subscription\n        self.scene_sub = self.create_subscription(\n            SceneGraph, \'/perception/scene_graph\',\n            self.scene_callback, 10\n        )\n\n        self.current_scene = None\n        self.get_logger().info(\'LLM service node ready\')\n\n    def scene_callback(self, msg):\n        """Update current scene context."""\n        self.current_scene = msg\n\n    async def plan_callback(self, request, response):\n        """Handle planning request."""\n        try:\n            # Build scene context string\n            scene_ctx = self._scene_to_string(self.current_scene)\n\n            # Generate plan with confidence\n            plan, confidence = self.confidence_estimator.estimate_plan_confidence(\n                PromptBuilder.build_planning_prompt(\n                    capabilities=self.planner.skills,\n                    robot_state=request.robot_state,\n                    scene={"objects": self._parse_scene(self.current_scene)},\n                    task=request.instruction\n                )\n            )\n\n            response.success = plan is not None\n            response.plan = json.dumps(plan) if plan else ""\n            response.confidence = confidence\n\n        except Exception as e:\n            self.get_logger().error(f\'Planning failed: {e}\')\n            response.success = False\n            response.error = str(e)\n\n        return response\n\n    def _scene_to_string(self, scene) -> str:\n        """Convert scene graph to text description."""\n        if scene is None:\n            return "No scene information available"\n\n        lines = []\n        for obj in scene.objects:\n            lines.append(f"{obj.label} at ({obj.pose.position.x:.2f}, "\n                        f"{obj.pose.position.y:.2f}, {obj.pose.position.z:.2f})")\n        return "\\n".join(lines)\n\n\ndef main():\n    rclpy.init()\n    node = LLMServiceNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"action-client-integration",children:"Action Client Integration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:'title="llm_action_client.py"',children:'"""Integration with ROS 2 action system."""\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionClient\nfrom nav2_msgs.action import NavigateToPose\nfrom control_msgs.action import GripperCommand\n\nclass LLMActionExecutor(Node):\n    """Execute LLM-generated plans using ROS 2 actions."""\n\n    def __init__(self):\n        super().__init__(\'llm_executor\')\n\n        # Action clients\n        self.nav_client = ActionClient(self, NavigateToPose, \'/navigate_to_pose\')\n        self.gripper_client = ActionClient(self, GripperCommand, \'/gripper_controller/gripper_cmd\')\n\n        # Skill to action mapping\n        self.skill_executors = {\n            \'navigate_to\': self._execute_navigate,\n            \'pick_object\': self._execute_pick,\n            \'place_object\': self._execute_place,\n            \'open_gripper\': self._execute_open_gripper,\n            \'close_gripper\': self._execute_close_gripper,\n        }\n\n    async def execute_plan(self, plan: List[dict]) -> bool:\n        """Execute a complete plan."""\n        for i, step in enumerate(plan):\n            self.get_logger().info(f"Executing step {i+1}: {step[\'skill\']}")\n\n            executor = self.skill_executors.get(step[\'skill\'])\n            if executor is None:\n                self.get_logger().error(f"Unknown skill: {step[\'skill\']}")\n                return False\n\n            success = await executor(step.get(\'parameters\', {}))\n            if not success:\n                self.get_logger().error(f"Step {i+1} failed")\n                return False\n\n        return True\n\n    async def _execute_navigate(self, params: dict) -> bool:\n        """Execute navigation action."""\n        goal = NavigateToPose.Goal()\n        goal.pose.header.frame_id = \'map\'\n\n        # Get pose from location name or coordinates\n        location = params.get(\'location\')\n        if isinstance(location, str):\n            pose = self._location_to_pose(location)\n        else:\n            pose = location\n\n        goal.pose.pose.position.x = pose[0]\n        goal.pose.pose.position.y = pose[1]\n        goal.pose.pose.orientation.w = 1.0\n\n        self.nav_client.wait_for_server()\n        result = await self.nav_client.send_goal_async(goal)\n\n        return result.status == 4  # SUCCEEDED\n\n    def _location_to_pose(self, location_name: str) -> tuple:\n        """Convert location name to coordinates."""\n        # Would typically look up from semantic map\n        locations = {\n            \'kitchen\': (1.0, 2.0),\n            \'living_room\': (3.0, 1.0),\n            \'charging_station\': (0.0, 0.0),\n        }\n        return locations.get(location_name, (0.0, 0.0))\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"exercise-1-build-a-task-planner",children:"Exercise 1: Build a Task Planner"}),"\n",(0,s.jsxs)(n.admonition,{title:"Exercise 1: LLM Task Planner",type:"tip",children:[(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),": Implement an LLM-based task planner for a robot."]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Steps"}),":"]}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Define 5+ robot skills with preconditions/effects"}),"\n",(0,s.jsx)(n.li,{children:"Create a planning prompt with skill descriptions"}),"\n",(0,s.jsx)(n.li,{children:"Implement plan generation with JSON output"}),"\n",(0,s.jsx)(n.li,{children:"Add confidence estimation through sampling"}),"\n",(0,s.jsx)(n.li,{children:"Test with 5 different task instructions"}),"\n"]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Verification"}),":"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'planner = TaskPlanner()\nplan = planner.plan("Clean up the table")\nassert len(plan) > 0\nassert all(step.skill in valid_skills for step in plan)\n'})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Time Estimate"}),": 60 minutes"]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"exercise-2-object-grounding",children:"Exercise 2: Object Grounding"}),"\n",(0,s.jsxs)(n.admonition,{title:"Exercise 2: Language-to-Object Grounding",type:"tip",children:[(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),": Ground natural language descriptions to detected objects."]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Steps"}),":"]}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Create object grounding prompt template"}),"\n",(0,s.jsx)(n.li,{children:'Handle ambiguous descriptions ("the cup" vs "the red cup")'}),"\n",(0,s.jsx)(n.li,{children:"Return confidence scores for matches"}),"\n",(0,s.jsx)(n.li,{children:"Handle cases with no matches"}),"\n",(0,s.jsx)(n.li,{children:"Test with various description styles"}),"\n"]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Test Cases"}),":"]}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'"the red ball" \u2192 should match red_ball object'}),"\n",(0,s.jsx)(n.li,{children:'"something to drink from" \u2192 should match cup objects'}),"\n",(0,s.jsx)(n.li,{children:'"the unicorn" \u2192 should return no matches'}),"\n"]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Time Estimate"}),": 45 minutes"]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"exercise-3-safe-execution-pipeline",children:"Exercise 3: Safe Execution Pipeline"}),"\n",(0,s.jsxs)(n.admonition,{title:"Exercise 3: Safety-First Execution",type:"tip",children:[(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),": Build a safe LLM-to-robot execution pipeline."]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Steps"}),":"]}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Implement plan validation (workspace bounds, forbidden regions)"}),"\n",(0,s.jsx)(n.li,{children:"Add confidence thresholding for auto-approval"}),"\n",(0,s.jsx)(n.li,{children:"Create human verification flow for uncertain actions"}),"\n",(0,s.jsx)(n.li,{children:"Test with intentionally ambiguous commands"}),"\n",(0,s.jsx)(n.li,{children:"Log all decisions for audit"}),"\n"]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Safety Requirements"}),":"]}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Never execute plans with confidence < 0.6 without verification"}),"\n",(0,s.jsx)(n.li,{children:"Reject any plan touching forbidden regions"}),"\n",(0,s.jsx)(n.li,{children:"Log all LLM queries and responses"}),"\n"]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Time Estimate"}),": 90 minutes"]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"In this chapter, you learned:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Integration Patterns"}),": High-level planning, code generation, reasoning, and end-to-end VLA"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Task Planning"}),": Decomposing instructions into robot skill sequences"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Code Generation"}),": Synthesizing executable robot code from descriptions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Prompt Engineering"}),": Designing effective prompts for robotics tasks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety"}),": Confidence estimation, validation, and human-in-the-loop verification"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ROS 2 Integration"}),": Building LLM services and action execution"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"LLMs provide powerful reasoning and language understanding that complements learned robot policies. The key is choosing the right integration pattern for your latency and reliability requirements, while maintaining safety through validation and human oversight."}),"\n",(0,s.jsxs)(n.p,{children:["Next, explore ",(0,s.jsx)(n.a,{href:"/docs/module-4/action-generation",children:"Action Generation"})," to understand how to convert high-level plans into low-level robot motions."]}),"\n",(0,s.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2204.01691",children:"SayCan Paper"})," - Grounding Language in Robotic Affordances"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2209.07753",children:"Code as Policies"})," - LLM Code Generation for Robotics"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2307.15818",children:"RT-2 Paper"})," - Vision-Language-Action Models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2306.17842",children:"Language Models Meet Robotics"})," - Survey of LLM+Robotics"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2303.03378",children:"PaLM-E"})," - Embodied Multimodal Language Model"]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);